diff --git a/projects/configs/far3d.py b/projects/configs/far3d.py
index 3bcf271..61dd8f5 100644
--- a/projects/configs/far3d.py
+++ b/projects/configs/far3d.py
@@ -40,6 +40,7 @@ model = dict(
     use_grid_mask=True,
     stride=[8, 16, 32, 64],
     position_level=[0, 1, 2, 3],
+    **img_norm_cfg,
     img_backbone=dict(
         type='VoVNet', ###use checkpoint to save memory
         spec_name='V-99-eSE',
@@ -67,9 +68,9 @@ model = dict(
         reg_depth_level='p3',
         pred_depth_var=False,    # note 2d depth uncertainty
         loss_depth2d=dict(type='L1Loss', loss_weight=1.0),
-        sample_with_score=True,  # note threshold
+        sample_with_score=False,  # note threshold
         threshold_score=0.1,
-        topk_proposal=None,
+        topk_proposal=50,
         return_context_feat=True,
     ),
     pts_bbox_head=dict(
@@ -189,7 +190,6 @@ train_pipeline = [
 test_pipeline = [
     dict(type='AV2LoadMultiViewImageFromFiles', to_float32=True),
     dict(type='AV2ResizeCropFlipRotImageV2', data_aug_conf=ida_aug_conf),
-    dict(type='NormalizeMultiviewImage', **img_norm_cfg),
     dict(type='AV2PadMultiViewImage', size='same2max'),
     dict(
         type='MultiScaleFlipAug3D',
diff --git a/projects/mmdet3d_plugin/core/bbox/coders/nms_free_coder.py b/projects/mmdet3d_plugin/core/bbox/coders/nms_free_coder.py
index 3accf02..5229230 100644
--- a/projects/mmdet3d_plugin/core/bbox/coders/nms_free_coder.py
+++ b/projects/mmdet3d_plugin/core/bbox/coders/nms_free_coder.py
@@ -66,22 +66,22 @@ class NMSFreeCoder(BaseBBoxCoder):
         if self.post_center_range is not None:
             self.post_center_range = torch.tensor(self.post_center_range, device=scores.device)
             
-            mask = (final_box_preds[..., :3] >=
-                    self.post_center_range[:3]).all(1)
-            mask &= (final_box_preds[..., :3] <=
-                     self.post_center_range[3:]).all(1)
+            #mask = (final_box_preds[..., :3] >=
+                    #self.post_center_range[:3]).all(1)
+            #mask &= (final_box_preds[..., :3] <=
+                     #self.post_center_range[3:]).all(1)
 
-            if self.score_threshold:
-                mask &= thresh_mask
+            #if self.score_threshold:
+                #mask &= thresh_mask
 
-            boxes3d = final_box_preds[mask]
-            scores = final_scores[mask]
-            labels = final_preds[mask]
+            #boxes3d = final_box_preds[mask]
+            #scores = final_scores[mask]
+            #labels = final_preds[mask]
 
             predictions_dict = {
-                'bboxes': boxes3d,
-                'scores': scores,
-                'labels': labels,
+                'bboxes': final_box_preds,
+                'scores': final_scores,
+                'labels': final_preds,
             }
 
         else:
diff --git a/projects/mmdet3d_plugin/core/bbox/util.py b/projects/mmdet3d_plugin/core/bbox/util.py
index 2281fa5..d8551a8 100644
--- a/projects/mmdet3d_plugin/core/bbox/util.py
+++ b/projects/mmdet3d_plugin/core/bbox/util.py
@@ -27,7 +27,8 @@ def denormalize_bbox(normalized_bboxes, pc_range):
     rot_sine = normalized_bboxes[..., 6:7]
 
     rot_cosine = normalized_bboxes[..., 7:8]
-    rot = torch.atan2(rot_sine, rot_cosine)
+    rot = torch.atan2(rot_sine.float(), rot_cosine.float())
+    rot = rot.to(normalized_bboxes.dtype)
 
     # center in the bev
     cx = normalized_bboxes[..., 0:1]
diff --git a/projects/mmdet3d_plugin/datasets/argoverse2_dataset.py b/projects/mmdet3d_plugin/datasets/argoverse2_dataset.py
index d62b0f3..b2b025e 100644
--- a/projects/mmdet3d_plugin/datasets/argoverse2_dataset.py
+++ b/projects/mmdet3d_plugin/datasets/argoverse2_dataset.py
@@ -287,7 +287,7 @@ class Argoverse2Dataset(Custom3DDataset):
                 for saving json files when jsonfile_prefix is not specified.
         """
         import pandas as pd
-
+        self.data_infos = self.data_infos[:len(outputs)]
         assert len(self.data_infos) == len(outputs)
         num_samples = len(outputs)
         print('\nGot {} samples'.format(num_samples))
diff --git a/projects/mmdet3d_plugin/models/dense_heads/farhead.py b/projects/mmdet3d_plugin/models/dense_heads/farhead.py
index 48e8ee5..697f5d4 100644
--- a/projects/mmdet3d_plugin/models/dense_heads/farhead.py
+++ b/projects/mmdet3d_plugin/models/dense_heads/farhead.py
@@ -13,10 +13,12 @@ from mmdet3d.core.bbox.coders import build_bbox_coder
 from projects.mmdet3d_plugin.core.bbox.util import normalize_bbox
 from projects.mmdet3d_plugin.models.utils.positional_encoding import pos2posemb3d, pos2posemb1d, nerf_positional_encoding
 from projects.mmdet3d_plugin.models.utils.misc import MLN, topk_gather, transform_reference_points, memory_refresh, SELayer_Linear
+from mmdet3d.core.bbox.structures.lidar_box3d import LiDARInstance3DBoxes
 import copy
 from mmdet.models.utils import NormedLinear
 from scipy.optimize import linear_sum_assignment
 import numpy as np
+
 @HEADS.register_module()
 class FarHead(AnchorFreeHead):
     """Implements the DETR transformer head.
@@ -187,7 +189,7 @@ class FarHead(AnchorFreeHead):
         self.train_use_gt_depth = train_use_gt_depth
         self.flag_disable_gt_depth = False
         self.val_use_gt_depth = val_use_gt_depth
-        self.add_multi_depth_proposal = add_multi_depth_proposal
+        self.add_multi_depth_proposal = add_multi_depth_proposal and multi_depth_config['topk'] > 1
         self.multi_depth_config = multi_depth_config
         self.return_context_feat = return_context_feat
         self.return_bbox2d_scores = return_bbox2d_scores
@@ -223,7 +225,9 @@ class FarHead(AnchorFreeHead):
 
 
         self._init_layers()
-        self.reset_memory()
+        self.reset_memory(1)
+        range_min = self.multi_depth_config.get('range_min', -1)
+        self.range_min_bin = self._convert_bin_depth_to_specific(torch.tensor([range_min]), inverse=True).item()
 
     def _init_layers(self):
         """Initialize layers of the transformer head."""
@@ -281,12 +285,12 @@ class FarHead(AnchorFreeHead):
             self.ego_pose_pe = MLN(180)
             self.ego_pose_memory = MLN(180)
 
-    def temporal_alignment(self, query_pos, tgt, reference_points):
+    def temporal_alignment(self, query_pos, tgt, reference_points, memory_reference_point, memory_embedding, memory_velo, memory_timestamp, memory_egopose):
         B = query_pos.size(0)
 
-        temp_reference_point = (self.memory_reference_point - self.pc_range[:3]) / (self.pc_range[3:6] - self.pc_range[0:3])
+        temp_reference_point = (memory_reference_point - self.pc_range[:3]) / (self.pc_range[3:6] - self.pc_range[0:3])
         temp_pos = self.query_embedding(pos2posemb3d(temp_reference_point)) 
-        temp_memory = self.memory_embedding
+        temp_memory = memory_embedding
         rec_ego_pose = torch.eye(4, device=query_pos.device).unsqueeze(0).unsqueeze(0).repeat(B, query_pos.size(1), 1, 1)
         
         if self.with_ego_pos:
@@ -294,13 +298,13 @@ class FarHead(AnchorFreeHead):
             rec_ego_motion = nerf_positional_encoding(rec_ego_motion)
             tgt = self.ego_pose_memory(tgt, rec_ego_motion)
             query_pos = self.ego_pose_pe(query_pos, rec_ego_motion)
-            memory_ego_motion = torch.cat([self.memory_velo, self.memory_timestamp, self.memory_egopose[..., :3, :].flatten(-2)], dim=-1).float()
+            memory_ego_motion = torch.cat([memory_velo, memory_timestamp, memory_egopose[..., :3, :].flatten(-2)], dim=-1).float()
             memory_ego_motion = nerf_positional_encoding(memory_ego_motion)
             temp_pos = self.ego_pose_pe(temp_pos, memory_ego_motion)
             temp_memory = self.ego_pose_memory(temp_memory, memory_ego_motion)
 
         query_pos += self.time_embedding(pos2posemb1d(torch.zeros_like(reference_points[...,:1])))
-        temp_pos += self.time_embedding(pos2posemb1d(self.memory_timestamp).float())
+        temp_pos += self.time_embedding(pos2posemb1d(memory_timestamp).float())
 
         if self.num_propagated > 0:
             tgt = torch.cat([tgt, temp_memory[:, :self.num_propagated]], dim=1)
@@ -443,40 +447,74 @@ class FarHead(AnchorFreeHead):
                 nn.init.constant_(m[-1].bias, bias_init)
 
 
-    def reset_memory(self):
-        self.memory_embedding = None
-        self.memory_reference_point = None
-        self.memory_timestamp = None
-        self.memory_egopose = None
-        self.memory_velo = None
+    def reset_memory(self, batch_size, device="cuda:0"):
+        self.state = self.get_memory(batch_size, device)
+
+    def get_memory(self, batch_size, device="cuda:0"):
+        memory_embedding = torch.zeros(batch_size, self.memory_len, self.embed_dims, device=device)
+        memory_reference_point = torch.zeros(batch_size, self.memory_len, 3, device=device)
+        memory_timestamp = torch.zeros(batch_size, self.memory_len, 1, device=device)
+        memory_egopose = torch.zeros(batch_size, self.memory_len, 4, 4, device=device)
+        memory_velo = torch.zeros(batch_size, self.memory_len, 2, device=device)
+        return dict(memory_embedding=memory_embedding, memory_reference_point=memory_reference_point,
+                    memory_timestamp=memory_timestamp, memory_egopose=memory_egopose,
+                    memory_velo=memory_velo)
+
 
-    def pre_update_memory(self, data):
+    def pre_update_memory(self, data, memory_embedding, memory_reference_point, memory_timestamp, memory_egopose, memory_velo):
         x = data['prev_exists']
         B = x.size(0)
         # refresh the memory when the scene changes
-        if self.memory_embedding is None:
-            self.memory_embedding = x.new_zeros(B, self.memory_len, self.embed_dims)
-            self.memory_reference_point = x.new_zeros(B, self.memory_len, 3)
-            self.memory_timestamp = x.new_zeros(B, self.memory_len, 1)
-            self.memory_egopose = x.new_zeros(B, self.memory_len, 4, 4)
-            self.memory_velo = x.new_zeros(B, self.memory_len, 2)
-        else:
-            self.memory_timestamp += data['timestamp'].unsqueeze(-1).unsqueeze(-1)
-            self.memory_egopose = data['ego_pose_inv'].unsqueeze(1) @ self.memory_egopose
-            self.memory_reference_point = transform_reference_points(self.memory_reference_point, data['ego_pose_inv'], reverse=False)
-            self.memory_timestamp = memory_refresh(self.memory_timestamp[:, :self.memory_len], x)
-            self.memory_reference_point = memory_refresh(self.memory_reference_point[:, :self.memory_len], x)
-            self.memory_embedding = memory_refresh(self.memory_embedding[:, :self.memory_len], x)
-            self.memory_egopose = memory_refresh(self.memory_egopose[:, :self.memory_len], x)
-            self.memory_velo = memory_refresh(self.memory_velo[:, :self.memory_len], x)
+        
+        memory_timestamp += data['timestamp'].unsqueeze(-1).unsqueeze(-1)
+        memory_egopose = data['ego_pose_inv'].unsqueeze(1) @ memory_egopose
+        memory_reference_point = transform_reference_points(memory_reference_point, data['ego_pose_inv'], reverse=False)
+        memory_timestamp = memory_refresh(memory_timestamp[:, :self.memory_len], x)
+        memory_reference_point = memory_refresh(memory_reference_point[:, :self.memory_len], x)
+        memory_embedding = memory_refresh(memory_embedding[:, :self.memory_len], x)
+        memory_egopose = memory_refresh(memory_egopose[:, :self.memory_len], x)
+        memory_velo = memory_refresh(memory_velo[:, :self.memory_len], x)
         
         # for the first frame, padding pseudo_reference_points (non-learnable)
         if self.num_propagated > 0:
             pseudo_reference_points = self.pseudo_reference_points.weight * (self.pc_range[3:6] - self.pc_range[0:3]) + self.pc_range[0:3]
-            self.memory_reference_point[:, :self.num_propagated]  = self.memory_reference_point[:, :self.num_propagated] + (1 - x).view(B, 1, 1) * pseudo_reference_points
-            self.memory_egopose[:, :self.num_propagated]  = self.memory_egopose[:, :self.num_propagated] + (1 - x).view(B, 1, 1, 1) * torch.eye(4, device=x.device)
-
-    def post_update_memory(self, data, rec_ego_pose, all_cls_scores, all_bbox_preds, outs_dec, mask_dict):
+            memory_reference_point[:, :self.num_propagated]  = memory_reference_point[:, :self.num_propagated] + (1 - x).view(B, 1, 1) * pseudo_reference_points
+            memory_egopose[:, :self.num_propagated]  = memory_egopose[:, :self.num_propagated] + (1 - x).view(B, 1, 1, 1) * torch.eye(4, device=x.device)
+        return dict(memory_embedding=memory_embedding, 
+                    memory_reference_point=memory_reference_point, 
+                    memory_timestamp=memory_timestamp, 
+                    memory_egopose=memory_egopose, memory_velo=memory_velo)
+
+
+    def _post_update_memory(self, ego_pose, timestamp, 
+                           memory_embedding, 
+                           memory_reference_point, 
+                           memory_timestamp, 
+                           memory_egopose, 
+                           memory_velo, 
+                           topk_indexes,
+                           rec_timestamp, 
+                           rec_reference_points,
+                           rec_memory,
+                           rec_ego_pose,
+                           rec_velo):
+        rec_timestamp = topk_gather(rec_timestamp, topk_indexes)
+        rec_reference_points = topk_gather(rec_reference_points, topk_indexes).detach()
+        rec_memory = topk_gather(rec_memory, topk_indexes).detach()
+        rec_ego_pose = topk_gather(rec_ego_pose, topk_indexes)
+        rec_velo = topk_gather(rec_velo, topk_indexes).detach()
+        memory_embedding = torch.cat([rec_memory, memory_embedding], dim=1)
+        memory_timestamp = torch.cat([rec_timestamp, memory_timestamp], dim=1)
+        memory_egopose= torch.cat([rec_ego_pose, memory_egopose], dim=1)
+        memory_reference_point = torch.cat([rec_reference_points, memory_reference_point], dim=1)
+        memory_velo = torch.cat([rec_velo, memory_velo], dim=1)
+        memory_reference_point = transform_reference_points(memory_reference_point, ego_pose, reverse=False)
+        memory_timestamp -= timestamp.unsqueeze(-1).unsqueeze(-1)
+        memory_egopose = ego_pose.unsqueeze(1) @ memory_egopose
+        return memory_embedding, memory_timestamp, memory_egopose, memory_reference_point, memory_velo
+
+    def post_update_memory(self, ego_pose, timestamp, rec_ego_pose, all_cls_scores, all_bbox_preds, outs_dec, mask_dict, 
+                           memory_embedding, memory_reference_point, memory_timestamp, memory_egopose, memory_velo):
         if self.training and mask_dict and mask_dict['pad_size'] > 0:
             rec_reference_points = all_bbox_preds[:, :, mask_dict['pad_size']:, :3][-1]
             rec_velo = all_bbox_preds[:, :, mask_dict['pad_size']:, -2:][-1]
@@ -492,20 +530,30 @@ class FarHead(AnchorFreeHead):
         
         # topk proposals
         _, topk_indexes = torch.topk(rec_score, self.topk_proposals, dim=1)
-        rec_timestamp = topk_gather(rec_timestamp, topk_indexes)
-        rec_reference_points = topk_gather(rec_reference_points, topk_indexes).detach()
-        rec_memory = topk_gather(rec_memory, topk_indexes).detach()
-        rec_ego_pose = topk_gather(rec_ego_pose, topk_indexes)
-        rec_velo = topk_gather(rec_velo, topk_indexes).detach()
 
-        self.memory_embedding = torch.cat([rec_memory, self.memory_embedding], dim=1)
-        self.memory_timestamp = torch.cat([rec_timestamp, self.memory_timestamp], dim=1)
-        self.memory_egopose= torch.cat([rec_ego_pose, self.memory_egopose], dim=1)
-        self.memory_reference_point = torch.cat([rec_reference_points, self.memory_reference_point], dim=1)
-        self.memory_velo = torch.cat([rec_velo, self.memory_velo], dim=1)
-        self.memory_reference_point = transform_reference_points(self.memory_reference_point, data['ego_pose'], reverse=False)
-        self.memory_timestamp -= data['timestamp'].unsqueeze(-1).unsqueeze(-1)
-        self.memory_egopose = data['ego_pose'].unsqueeze(1) @ self.memory_egopose
+        memory = self._post_update_memory(
+                                        ego_pose=ego_pose, 
+                                        timestamp=timestamp, 
+                                        memory_embedding=memory_embedding, 
+                                        memory_reference_point=memory_reference_point, 
+                                        memory_timestamp=memory_timestamp, 
+                                        memory_egopose=memory_egopose,
+                                        memory_velo=memory_velo, 
+                                        topk_indexes=topk_indexes,
+                                        rec_timestamp=rec_timestamp,
+                                        rec_reference_points=rec_reference_points,
+                                        rec_memory=rec_memory,
+                                        rec_ego_pose=rec_ego_pose,
+                                        rec_velo=rec_velo)
+        memory_embedding, memory_timestamp, memory_egopose, memory_reference_point, memory_velo = memory
+        return dict(memory_embedding=memory_embedding, 
+                    memory_reference_point=memory_reference_point, 
+                    memory_timestamp=memory_timestamp, 
+                    memory_egopose=memory_egopose, 
+                    memory_velo=memory_velo)
+
+    
+        
     
     def _get_gt_depth(self, img_metas, device, BNHW):
         B, N, H, W = BNHW
@@ -530,7 +578,7 @@ class FarHead(AnchorFreeHead):
                 indices = indices.type(torch.int64)
                 return indices
     
-    def forward(self, img_metas, outs_roi=None, **data):
+    def forward(self, img_metas, outs_roi=None, state=None, **data):
         """Forward function.
         Args:
             mlvl_feats (tuple[Tensor]): Features from the upstream
@@ -544,12 +592,15 @@ class FarHead(AnchorFreeHead):
                 head with normalized coordinate format (cx, cy, w, l, cz, h, theta, vx, vy). \
                 Shape [nb_dec, bs, num_query, 9].
         """
-        self.pre_update_memory(data)
+        if state is None:
+            state = self.state
+        state = self.pre_update_memory(data, **state)
+
         mlvl_feats = data['img_feats']
         B, N, _, _, _ = mlvl_feats[0].shape
 
         reference_points = self.reference_points.weight
-        dtype = reference_points.dtype
+        dtype = mlvl_feats[0].dtype
         intrinsics = data['intrinsics'] / 1e3
         extrinsics = data['extrinsics'][..., :3, :]
         mln_input = torch.cat([intrinsics[..., 0,0:1], intrinsics[..., 1,1:2], extrinsics.flatten(-2)], dim=-1)
@@ -560,7 +611,7 @@ class FarHead(AnchorFreeHead):
             B, N, C, H, W = mlvl_feats[i].shape
             mlvl_feat = mlvl_feats[i].reshape(B * N, C, -1).transpose(1, 2)
             mlvl_feat = self.spatial_alignment(mlvl_feat, mln_input)
-            feat_flatten.append(mlvl_feat.to(dtype))
+            feat_flatten.append(mlvl_feat)
             spatial_flatten.append((H, W))
         feat_flatten = torch.cat(feat_flatten, dim=1)
         spatial_flatten = torch.as_tensor(spatial_flatten, dtype=torch.long, device=mlvl_feats[0].device)
@@ -576,7 +627,8 @@ class FarHead(AnchorFreeHead):
             if self.return_context_feat:
                 _dim = feat_flatten.shape[-1]
                 valid_indices = outs_roi['valid_indices']
-                context_feat = feat_flatten[valid_indices.repeat(1, 1, _dim)].reshape(-1, _dim)
+                
+                context_feat = torch.gather(feat_flatten, dim=1, index=valid_indices.repeat(1,1,_dim)).reshape(-1, _dim)
 
                 context2d_feat = context_feat.detach()
             else:
@@ -605,9 +657,15 @@ class FarHead(AnchorFreeHead):
             # depth input: specific bins or depth logits
             input_depth_logits = self.multi_depth_config.get('topk', -1) != -1 and not flag_use_gt_depth
             depth_input = pred_depth_ if not input_depth_logits else pred_depth.permute(0, 2, 3, 1)
-            reference_points2d, context_feat = self.build_query2d_proposal(pred_bbox_list, depth_input, data, (B, N),
-                padHW, pred_depth_var=_pred_depth_var, input_depth_logits=input_depth_logits,
-                context2d_feat=context2d_feat, bbox2d_scores=bbox2d_scores)
+            if self.add_multi_depth_proposal:
+                reference_points2d, context_feat = self.build_query2d_proposal_multi(pred_bbox_list, depth_input, data, (B, N),
+                    padHW, pred_depth_var=_pred_depth_var, input_depth_logits=input_depth_logits,
+                    context2d_feat=context2d_feat, bbox2d_scores=bbox2d_scores)
+            else:
+                reference_points2d, context_feat = self.build_query2d_proposal_single(pred_bbox_list, depth_input, data, (B, N),
+                    padHW, input_depth_logits=input_depth_logits,
+                    context2d_feat=context2d_feat, bbox2d_scores=bbox2d_scores)
+
 
            
             pred_depth_var = None   # [Deprecated function]
@@ -633,14 +691,15 @@ class FarHead(AnchorFreeHead):
                     attn_mask = None
 
         tgt = torch.zeros_like(query_pos)
-        if 'context_feat' in locals() and context_feat is not None:    # add context to Q_feat
+        if context_feat is not None:    # add context to Q_feat
             context_feat = self.context_embed(context_feat)  # newly add
             tgt[:, -pro2d_num:, :] = context_feat
 
         # prepare for the tgt and query_pos using mln.
-        tgt, query_pos, reference_points, temp_memory, temp_pos, rec_ego_pose = self.temporal_alignment(query_pos, tgt, reference_points)
+        
+        tgt, query_pos, reference_points, temp_memory, temp_pos, rec_ego_pose = self.temporal_alignment(query_pos, tgt, reference_points, **state)
 
-        outs_dec = self.transformer(tgt, query_pos, feat_flatten, spatial_flatten, level_start_index, temp_memory, 
+        outs_dec = self.transformer(tgt.to(dtype), query_pos.to(dtype), feat_flatten.to(dtype), spatial_flatten, level_start_index, temp_memory, 
                                     temp_pos, attn_mask, reference_points, self.pc_range, data, img_metas)
 
         outs_dec = torch.nan_to_num(outs_dec)
@@ -664,7 +723,7 @@ class FarHead(AnchorFreeHead):
         all_bbox_preds[..., 0:3] = (all_bbox_preds[..., 0:3] * (self.pc_range[3:6] - self.pc_range[0:3]) + self.pc_range[0:3])
         
         # update the memory bank
-        self.post_update_memory(data, rec_ego_pose, all_cls_scores, all_bbox_preds, outs_dec, mask_dict)
+        state = self.post_update_memory(data['ego_pose'], data['timestamp'], rec_ego_pose, all_cls_scores, all_bbox_preds, outs_dec, mask_dict, **state)
 
         # cls_score_numpy = outputs_class.cpu().numpy()
         # path = img_metas[0]['scene_token']
@@ -689,8 +748,8 @@ class FarHead(AnchorFreeHead):
                 'dn_mask_dict':None,
                 'reference_points2d': reference_points2d,
             }
-
-        return outs
+        self.state = state
+        return outs, state
 
     def split_offline_pred2d(self, pred_bbox_list, device):
         pred_bbox_list = [torch.from_numpy(img_bbox).to(device) if len(img_bbox) > 0 else torch.zeros(0, 6).to(device)
@@ -708,7 +767,82 @@ class FarHead(AnchorFreeHead):
         return new_pred_bbox_list, bbox2d_scores
     
     @torch.no_grad()
-    def build_query2d_proposal(self, pred_bbox_list, pred_depth, data, bn, padHW,
+    def extract_objectwise_depth(self, pred_bbox_list, pred_depth, depth_downsample, pad_w, input_depth_logits):
+        depth_list = []
+        bbox_nums = [len(bbox) for bbox in pred_bbox_list]  # BN values
+        h_max, w_max = pred_depth.shape[1:3]
+        for ith, pred_bbox in enumerate(pred_bbox_list):
+            if bbox_nums[ith] != 0:
+                cur_depthmap = pred_depth[ith].flatten(0, 1)     # shape (HW, 1) or (HW, D)
+                cur_center2d = (pred_bbox[:, :2] / depth_downsample).round().long()      # first w then h
+                centers_x = cur_center2d[:,0]
+                centers_y = cur_center2d[:,1]
+                centers_x.clamp_(0, w_max - 1)
+                centers_y.clamp_(0, h_max - 1)
+
+                cur_center2d_ = centers_y * (pad_w//depth_downsample) + centers_x
+                if input_depth_logits:
+                    cur_depth = torch.gather(cur_depthmap, 0, cur_center2d_.long().unsqueeze(1)
+                                             .repeat(1, cur_depthmap.shape[1]))  # (Mi, D)
+                else:
+                    cur_depth = torch.gather(cur_depthmap, 0, cur_center2d_.long().unsqueeze(1))  # (Mi, 1)
+                depth_list.append(cur_depth)
+        depths = torch.cat(depth_list, dim=0)  # (M, 1) or (M, D)
+        return depths
+
+    @torch.no_grad()
+    def project_reference_points_to_image_single(self, bboxes, depths, img2lidars, bbox_nums, B, N):
+        # (u,v), d -> (ud,vd,d,1)
+        eps = 1e-5
+        coords = torch.cat([bboxes[:, :2], depths], dim=1)     # (M, 3), order is (w, h, d)
+        coords = torch.cat((coords, torch.ones_like(coords[..., :1])), -1)      # (M, 4)
+        coords[..., :2] = coords[..., :2] * torch.maximum(coords[..., 2:3], torch.ones_like(coords[..., 2:3]) * eps)
+        coords = coords.unsqueeze(-1)  # (M, 4, 1)
+        
+        # img2lidar array build
+        img2lidars = img2lidars.view(B*N, 1, 4, 4) # (BN, 1, 4, 4)
+        img2lidars_ = torch.cat([img2lidars[kth].repeat(num, 1, 1) for kth, num in enumerate(bbox_nums)], dim=0)    # (M, 4, 4)
+
+        # matmul and normalize 3d coords
+        coords3d = torch.matmul(img2lidars_, coords)[:,:3,0]    # (M, 3)
+        
+        coords3d[..., 0:1] = (coords3d[..., 0:1] - self.pc_range[0]) / (self.pc_range[3] - self.pc_range[0])
+        coords3d[..., 1:2] = (coords3d[..., 1:2] - self.pc_range[1]) / (self.pc_range[4] - self.pc_range[1])
+        coords3d[..., 2:3] = (coords3d[..., 2:3] - self.pc_range[2]) / (self.pc_range[5] - self.pc_range[2])
+        #coords_mask = (coords3d > 1.0) | (coords3d < 0.0)
+        new_reference_points = coords3d.unsqueeze(0)
+        return new_reference_points
+    
+    @torch.no_grad()
+    def project_reference_points_to_image_multi(self, bboxes, depths, img2lidars, input_depth_logits, topk, valid_indices, bbox_nums, B, N):
+        # (u,v), d -> (ud,vd,d,1)
+        eps = 1e-5
+        coords = torch.cat([bboxes[:, :2], depths], dim=1)     # (M, 3), order is (w, h, d)
+        coords = torch.cat((coords, torch.ones_like(coords[..., :1])), -1)      # (M, 4)
+        coords[..., :2] = coords[..., :2] * torch.maximum(coords[..., 2:3], torch.ones_like(coords[..., 2:3]) * eps)
+        coords = coords.unsqueeze(-1)  # (M, 4, 1)
+        
+        # img2lidar array build
+        img2lidars = img2lidars.view(B*N, 1, 4, 4) # (BN, 1, 4, 4)
+        img2lidars_ = torch.cat([img2lidars[kth].repeat(num, 1, 1) for kth, num in enumerate(bbox_nums)], dim=0)    # (M, 4, 4)
+        if input_depth_logits and self.multi_depth_config.get('topk', -1) != -1:
+            img2lidars_extra = img2lidars_.repeat(topk - 1, 1, 1)
+            img2lidars_extra = img2lidars_extra[valid_indices.repeat(topk - 1)]
+            img2lidars_ = torch.cat([img2lidars_, img2lidars_extra], dim=0)
+
+        # matmul and normalize 3d coords
+        coords3d = torch.matmul(img2lidars_, coords)[:,:3,0]    # (M, 3)
+        
+        coords3d[..., 0:1] = (coords3d[..., 0:1] - self.pc_range[0]) / (self.pc_range[3] - self.pc_range[0])
+        coords3d[..., 1:2] = (coords3d[..., 1:2] - self.pc_range[1]) / (self.pc_range[4] - self.pc_range[1])
+        coords3d[..., 2:3] = (coords3d[..., 2:3] - self.pc_range[2]) / (self.pc_range[5] - self.pc_range[2])
+        #coords_mask = (coords3d > 1.0) | (coords3d < 0.0)
+        new_reference_points = coords3d.unsqueeze(0)
+        return new_reference_points
+
+
+    @torch.no_grad()
+    def build_query2d_proposal_multi(self, pred_bbox_list, pred_depth, data, bn, padHW,
                                valid_pred_depth=None, pred_depth_var=None, input_depth_logits=False,
                                context2d_feat=None, bbox2d_scores=None):
         '''
@@ -718,7 +852,7 @@ class FarHead(AnchorFreeHead):
         '''
         B, N = bn
         pad_h, pad_w = padHW
-        eps = 1e-5
+        
         depth_downsample = int(pad_h / pred_depth.shape[1])
 
         # bbox list to (sum(Mi), 2)
@@ -727,38 +861,23 @@ class FarHead(AnchorFreeHead):
         if sum(bbox_nums) == 0:  # no effective 2d proposal
             return None, None
         # obtain corresponding depth
-        depth_list = []
-        depth_var_list = []
-        h_max, w_max = pred_depth.shape[1:3]
-        for ith, pred_bbox in enumerate(pred_bbox_list):
-            if bbox_nums[ith] != 0:
-                cur_depthmap = pred_depth[ith].flatten(0, 1)     # shape (HW, 1) or (HW, D)
-                cur_center2d = (pred_bbox[:, :2] / depth_downsample).round().long()      # first w then h
-                cur_center2d[cur_center2d < 0] = 0
-                cur_center2d[:, 0][cur_center2d[:, 0] >= w_max] = w_max - 1
-                cur_center2d[:, 1][cur_center2d[:, 1] >= h_max] = h_max - 1
-
-                cur_center2d = cur_center2d.flip(dims=(-1, ))   # to obtain depth, obtain (h, w)
-                cur_center2d_ = cur_center2d[:, 0] * (pad_w/depth_downsample) + cur_center2d[:, 1]
-                if input_depth_logits:
-                    cur_depth = torch.gather(cur_depthmap, 0, cur_center2d_.long().unsqueeze(1)
-                                             .repeat(1, cur_depthmap.shape[1]))  # (Mi, D)
-                else:
-                    cur_depth = torch.gather(cur_depthmap, 0, cur_center2d_.long().unsqueeze(1))  # (Mi, 1)
-                depth_list.append(cur_depth)
-
-                if pred_depth_var is not None:
-                    cur_depth_var = torch.gather(pred_depth_var[ith], 0, cur_center2d_.long())    # (Mi)
-                    depth_var_list.append(cur_depth_var)
-        depths = torch.cat(depth_list, dim=0)  # (M, 1) or (M, D)
+        depths = self.extract_objectwise_depth(pred_bbox_list=pred_bbox_list,
+                                               pred_depth=pred_depth,
+                                               depth_downsample=depth_downsample,
+                                               pad_w=pad_w,
+                                               input_depth_logits=input_depth_logits)
+        topk = self.multi_depth_config.get('topk')
+        valid_indices = None
         if self.add_multi_depth_proposal:
-            range_min = self.multi_depth_config.get('range_min', -1)
+            # this pathway does some weird stuff during export when topk == 1
             if input_depth_logits and self.multi_depth_config.get('topk', -1) != -1:
                 # generate multi proposals by topk depth logits
-                topk = self.multi_depth_config.get('topk')
-                range_min_bin = self._convert_bin_depth_to_specific(torch.tensor([range_min]), inverse=True).item()
-                topk_values, topk_indices = torch.topk(depths, topk, dim=1)  # (M, K)
-                valid_indices = topk_indices[:, 0] >= range_min_bin          # (M)
+                topk_values, topk_indices = torch.topk(depths, topk, dim=1)  # (M, K) # export works until here
+                valid_indices = topk_indices
+                # this causes errors in onnx export, 
+                valid_indices = topk_indices[:, 0] >= self.range_min_bin          # (M)
+                #valid_indices = topk_indices.squeeze(1) # for some reason this materializes a resolv_conj operation
+                
                 bboxes_extra = bboxes.repeat(topk-1, 1)
                 bboxes = torch.cat([bboxes, bboxes_extra[valid_indices.repeat(topk-1)]], dim=0) # (M', 4)
                 depths_extra = topk_indices[:, 1:][valid_indices]   # (M, topk-1)
@@ -769,60 +888,87 @@ class FarHead(AnchorFreeHead):
                 if context2d_feat is not None:
                     context2d_feat_extra = context2d_feat.repeat(topk-1, 1)
                     context2d_feat = torch.cat([context2d_feat, context2d_feat_extra[valid_indices.repeat(topk-1)]], dim=0)
-
-            if bbox2d_scores is not None:   # currently use context_2d by default
-                thr = torch.tensor([0.1]).to(bbox2d_scores.device)   # score threshold
-                log_odds = torch.log(bbox2d_scores / (1 - bbox2d_scores)) - torch.log(thr / (1 - thr))  # (M, 1)
-                if input_depth_logits and self.multi_depth_config.get('topk', -1) != -1:
-                    # softmax depth logits, select topk, and rescale their weight
-                    topk_values = topk_values / topk_values[:, 0:1]   # rescale, (M, topk)
-                    dscores_extra = topk_values[:, 1:][valid_indices].transpose(1, 0).flatten().unsqueeze(-1) # (M*(topk-1), 1)
-                    dscores = torch.cat([topk_values[:, 0:1], dscores_extra], dim=0)    # (M', 1)
-                    log_odds = torch.cat([log_odds, log_odds[valid_indices].repeat(topk-1, 1)], dim=0)
-                    log_odds = log_odds * dscores
-                if context2d_feat is not None:
-                    context2d_feat = torch.cat([context2d_feat, log_odds], dim=-1)  # check dim cat
-                else:
-                    context2d_feat = log_odds.repeat(1, self.in_channels)
+        else:
+            _, depths = torch.topk(depths, 1, dim=1)
+        if bbox2d_scores is not None:   # currently use context_2d by default
+            thr = torch.tensor([0.1]).to(bbox2d_scores.device)   # score threshold
+            log_odds = torch.log(bbox2d_scores / (1 - bbox2d_scores)) - torch.log(thr / (1 - thr))  # (M, 1)
+            if self.add_multi_depth_proposal:
+                # softmax depth logits, select topk, and rescale their weight
+                topk_values = topk_values / topk_values[:, 0:1]   # rescale, (M, topk)
+                dscores_extra = topk_values[:, 1:][valid_indices].transpose(1, 0).flatten().unsqueeze(-1) # (M*(topk-1), 1)
+                dscores = torch.cat([topk_values[:, 0:1], dscores_extra], dim=0)    # (M', 1)
+                log_odds = torch.cat([log_odds, log_odds[valid_indices].repeat(topk-1, 1)], dim=0)
+                log_odds = log_odds * dscores
+            if context2d_feat is not None:
+                context2d_feat = torch.cat([context2d_feat, log_odds], dim=-1)  # check dim cat
+            else:
+                context2d_feat = log_odds.repeat(1, self.in_channels)
+        
 
         # convert bin to float depth
         depths = self._convert_bin_depth_to_specific(depths)
+        new_reference_points = self.project_reference_points_to_image_single(
+            bboxes=bboxes,
+            depths=depths, 
+            img2lidars=data['img2lidar'], 
+            input_depth_logits=input_depth_logits,
+            topk=topk,
+            bbox_nums=bbox_nums,
+            B=B, N=N,
+            valid_indices=valid_indices
+        )
 
-        # (u,v), d -> (ud,vd,d,1)
-        coords = torch.cat([bboxes[:, :2], depths], dim=1)     # (M, 3), order is (w, h, d)
-        coords = torch.cat((coords, torch.ones_like(coords[..., :1])), -1)      # (M, 4)
-        coords[..., :2] = coords[..., :2] * torch.maximum(coords[..., 2:3], torch.ones_like(coords[..., 2:3]) * eps)
-        coords = coords.unsqueeze(-1)  # (M, 4, 1)
-
-        # img2lidar array build
-        img2lidars = data['lidar2img'].inverse()  # (B, N, 4, 4)
-        img2lidars = img2lidars.view(B*N, 1, 4, 4) # (BN, 1, 4, 4)
-        img2lidars_ = torch.cat([img2lidars[kth].repeat(num, 1, 1) for kth, num in enumerate(bbox_nums)], dim=0)    # (M, 4, 4)
-        if self.add_multi_depth_proposal:
-            if input_depth_logits and self.multi_depth_config.get('topk', -1) != -1:
-                img2lidars_extra = img2lidars_.repeat(topk - 1, 1, 1)
-                img2lidars_extra = img2lidars_extra[valid_indices.repeat(topk - 1)]
-                img2lidars_ = torch.cat([img2lidars_, img2lidars_extra], dim=0)
-
-        # matmul and normalize 3d coords
-        coords3d = torch.matmul(img2lidars_, coords).squeeze(-1)[..., :3]    # (M, 3)
-        coords3d[..., 0:1] = (coords3d[..., 0:1] - self.pc_range[0]) / (self.pc_range[3] - self.pc_range[0])
-        coords3d[..., 1:2] = (coords3d[..., 1:2] - self.pc_range[1]) / (self.pc_range[4] - self.pc_range[1])
-        coords3d[..., 2:3] = (coords3d[..., 2:3] - self.pc_range[2]) / (self.pc_range[5] - self.pc_range[2])
-        coords_mask = (coords3d > 1.0) | (coords3d < 0.0)
-        if B == 1:
-            new_reference_points = coords3d.unsqueeze(0)    # (B, M, 3)
-        else:
-            raise NotImplementedError
+        context2d_feat = context2d_feat.unsqueeze(0)
 
+        return new_reference_points, context2d_feat
+    
+    @torch.no_grad()
+    def build_query2d_proposal_single(self, pred_bbox_list, pred_depth, data, bn, padHW,
+                               input_depth_logits=False,
+                               context2d_feat=None, 
+                               bbox2d_scores=None):
         '''
-        if pred_depth_var is not None:
-            depth_var = torch.cat(depth_var_list, dim=0).unsqueeze(0).unsqueeze(-1)    # (B, M)
-        else:
-            depth_var = None
+        pred_centers2d: ~~(B*N H*W 2)~~, now is a list, BN*(Mi, 4)
+        pred_depth: (B*N, H, W, 1) if not use topk depth proposals else (BN, H, W, D)
+        pred_depth_var: (B*N, 1, H, W)
         '''
+        B, N = bn
+        pad_h, pad_w = padHW
+        
+        depth_downsample = int(pad_h / pred_depth.shape[1])
+
+        # bbox list to (sum(Mi), 2)
+        bbox_nums = [len(bbox) for bbox in pred_bbox_list]  # BN values
+        bboxes = torch.cat(pred_bbox_list, dim=0).float()   # gather boxes together
+        if sum(bbox_nums) == 0:  # no effective 2d proposal
+            return None, None
+        # obtain corresponding depth
+        depths = self.extract_objectwise_depth(pred_bbox_list=pred_bbox_list,
+                                               pred_depth=pred_depth,
+                                               depth_downsample=depth_downsample,
+                                               pad_w=pad_w,
+                                               input_depth_logits=input_depth_logits)
+        _, depths = torch.topk(depths, 1, dim=1)
+        if bbox2d_scores is not None:   # currently use context_2d by default
+            thr = torch.tensor([0.1]).to(bbox2d_scores.device)   # score threshold
+            log_odds = torch.log(bbox2d_scores / (1 - bbox2d_scores)) - torch.log(thr / (1 - thr))  # (M, 1)
+            if context2d_feat is not None:
+                context2d_feat = torch.cat([context2d_feat, log_odds], dim=-1)  # check dim cat
+            else:
+                context2d_feat = log_odds.repeat(1, self.in_channels)
+
+        # convert bin to float depth
+        depths = self._convert_bin_depth_to_specific(depths)
+        new_reference_points = self.project_reference_points_to_image_single(
+            bboxes=bboxes,
+            depths=depths, 
+            img2lidars=data['img2lidar'], 
+            bbox_nums=bbox_nums,
+            B=B, N=N,
+        )
 
-        context2d_feat = context2d_feat.unsqueeze(0) if B == 1 and context2d_feat is not None else None
+        context2d_feat = context2d_feat.unsqueeze(0)
 
         return new_reference_points, context2d_feat
 
@@ -1238,8 +1384,27 @@ class FarHead(AnchorFreeHead):
             preds = preds_dicts[i]
             bboxes = preds['bboxes']
             bboxes[:, 2] = bboxes[:, 2] - bboxes[:, 5] * 0.5
-            bboxes = img_metas[i]['box_type_3d'](bboxes, bboxes.size(-1))
+            #bboxes = img_metas[i]['box_type_3d'](bboxes, bboxes.size(-1))
+            #bboxes = LiDARInstance3DBoxes(bboxes, bboxes.size(-1))
             scores = preds['scores']
             labels = preds['labels']
             ret_list.append([bboxes, scores, labels])
         return ret_list
+    
+    def half(self):
+        for branch in self.cls_branches:
+            branch.half()
+        for branch in self.reg_branches:
+            branch.half()
+        self.transformer.half()
+        #self.spatial_alignment.half()
+        
+    def float(self):
+        for branch in self.cls_branches:
+            branch.float()
+        for branch in self.reg_branches:
+            branch.float()
+        self.transformer.float()
+        #self.spatial_alignment.float()
+
+
diff --git a/projects/mmdet3d_plugin/models/dense_heads/yolox_head.py b/projects/mmdet3d_plugin/models/dense_heads/yolox_head.py
index 6884a76..f26f629 100644
--- a/projects/mmdet3d_plugin/models/dense_heads/yolox_head.py
+++ b/projects/mmdet3d_plugin/models/dense_heads/yolox_head.py
@@ -426,18 +426,19 @@ class YOLOXHeadCustom(BaseDenseHead, BBoxTestMixin):
         for i in range(len(objectnesses)):
         # for cls_score in cls_scores:
             # sample_weight = cls_scores[i].topk(1, dim=1).values.sigmoid()       # (BN, 1, Hi, Wi)
-            sample_weight = objectnesses[i].sigmoid() * cls_scores[i].topk(1, dim=1).values.sigmoid()
+            sample_weight = objectnesses[i].sigmoid() * torch.topk(cls_scores[i], 1, dim=1).values.sigmoid()
             sample_weight_nms = nn.functional.max_pool2d(sample_weight, (3, 3), stride=1, padding=1)
             sample_weight_nms = sample_weight_nms.permute(0, 2, 3, 1).reshape(num_imgs, -1, 1)  # (BN, Hi*Wi, 1)
             sample_weight_ = sample_weight.permute(0, 2, 3, 1).reshape(num_imgs, -1, 1)
             sample_weight = sample_weight_ * (sample_weight_ == sample_weight_nms).float()  # (BN, Hi*Wi, 1)
             valid_indices_list.append(sample_weight)
-        valid_indices = torch.cat(valid_indices_list, dim=1)
+        valid_indices = torch.cat(valid_indices_list, dim=1) # concat across our multihead outputs
         flatten_sample_weight = valid_indices.clone()       # (BN, sum(Hi*Wi), 1)
         if self.sample_with_score:
             valid_indices = valid_indices > threshold_score
         else:
             _, topk_indexes = torch.topk(valid_indices, topk_proposal, dim=1)
+            valid_indices = topk_indexes
 
         flatten_cls_scores = torch.cat(flatten_cls_scores, dim=1).sigmoid()     # (BN, sum(Hi*Wi), num_cls)
         flatten_bbox_preds = torch.cat(flatten_bbox_preds, dim=1)               # (BN, sum(Hi*Wi), 4)
@@ -463,8 +464,10 @@ class YOLOXHeadCustom(BaseDenseHead, BBoxTestMixin):
         #     if bbox is not None:
         #         bbox = bbox_xyxy_to_cxcywh(bbox[..., :4])
         #     result_list.append(bbox)
-
-        bbox2d_scores = flatten_sample_weight[valid_indices].reshape(-1, 1)  # (M, 1)
+        if self.sample_with_score:
+            bbox2d_scores = flatten_sample_weight[valid_indices].reshape(-1, 1)  # (M, 1)
+        else:
+            bbox2d_scores = torch.gather(flatten_sample_weight, dim=1, index=valid_indices).reshape(-1, 1)  # (M, 1)
         outs = {
             'bbox_list': result_list,
             'bbox2d_scores': bbox2d_scores,
diff --git a/projects/mmdet3d_plugin/models/detectors/far3d.py b/projects/mmdet3d_plugin/models/detectors/far3d.py
index 12489d2..6025ce7 100644
--- a/projects/mmdet3d_plugin/models/detectors/far3d.py
+++ b/projects/mmdet3d_plugin/models/detectors/far3d.py
@@ -42,7 +42,10 @@ class Far3D(MVXTwoStageDetector):
                  position_level=[0],
                  aux_2d_only=True,
                  single_test=False,
-                 pretrained=None):
+                 pretrained=None,
+                 mean=[103.530, 116.280, 123.675], 
+                 std=[57.375, 57.120, 58.395],
+                 to_rgb=False):
         super(Far3D, self).__init__(pts_voxel_layer, pts_voxel_encoder,
                              pts_middle_encoder, pts_fusion_layer,
                              img_backbone, pts_backbone, img_neck, pts_neck,
@@ -55,6 +58,8 @@ class Far3D(MVXTwoStageDetector):
         self.stride = stride
         self.position_level = position_level
         self.aux_2d_only = aux_2d_only
+        self.mean = torch.Tensor(mean)
+        self.std = torch.Tensor(std)
         if depth_branch is not None:
             self.depth_branch = build_from_cfg(depth_branch, PLUGIN_LAYERS)
         else:
@@ -75,6 +80,8 @@ class Far3D(MVXTwoStageDetector):
                 img = img.reshape(B * N, C, H, W)
             if self.use_grid_mask:
                 img = self.grid_mask(img)
+            img -= torch.tensor(self.mean, device=img.device).reshape(1,3,1,1)
+            img /= torch.tensor(self.std, device=img.device).reshape(1,3,1,1)
 
             img_feats = self.img_backbone(img)
             if isinstance(img_feats, dict):
@@ -120,9 +127,9 @@ class Far3D(MVXTwoStageDetector):
         return location_r
 
     def forward_roi_head(self, location, **data):
-        outs_roi = self.img_roi_head(location, **data)
+        outs_roi = self.img_roi_head(locations=location, **data)
         
-        return outs_roi
+        return outs_roi, data['img_feats']
 
 
     def forward_pts_train(self,
@@ -149,7 +156,7 @@ class Far3D(MVXTwoStageDetector):
         """
         location = self.prepare_location(img_metas, **data)
 
-        outs_roi = self.forward_roi_head(location, **data)
+        outs_roi, _ = self.forward_roi_head(location, **data)
         bbox_dict = self.img_roi_head.get_bboxes(outs_roi, **data)  # {'bbox_list': BN x (Mi, 4), 'bbox_score_list': BN x (Mi, 1)}
         outs_roi.update(bbox_dict)
         outs = self.pts_bbox_head(img_metas, outs_roi, **data)
@@ -229,51 +236,59 @@ class Far3D(MVXTwoStageDetector):
 
         return losses
   
-    def forward_test(self, img_metas, rescale, **data):
+    def forward_test(self, img_metas, rescale, state=None,**data):
         for var, name in [(img_metas, 'img_metas')]:
             if not isinstance(var, list):
                 raise TypeError('{} must be a list, but got {}'.format(
                     name, type(var)))
         for key in data:
-            if key not in ['img', 'gt_bboxes_3d', 'gt_bboxes', 'centers2d']:
+            if key not in ['img', 'gt_bboxes_3d', 'gt_bboxes', 'centers2d', 'state', "prev_exists"]:
                 data[key] = data[key][0][0].unsqueeze(0)
             else:
                 data[key] = data[key][0]
-        return self.simple_test(img_metas[0], **data)
+        data['img2lidar'] = data['lidar2img'].inverse()
+        return self.simple_test(img_metas[0], state=state, **data)
+    
+    def simple_test_bboxes(self, img_metas, outs_roi, state, **data):
+        outs, state = self.pts_bbox_head(img_metas, outs_roi, state=state, **data)
 
-    def simple_test_pts(self, img_metas, **data):
-        """Test function of point cloud branch."""
+        bbox_list = self.pts_bbox_head.get_bboxes(outs, img_metas)
+        return bbox_list, state
+
+    def get_bboxes(self, img_metas, **data):
         location = self.prepare_location(img_metas, **data)
-        outs_roi = self.forward_roi_head(location, **data)
+        outs_roi, img_feats = self.forward_roi_head(location, **data)
         bbox_dict = self.img_roi_head.get_bboxes(outs_roi)  # {'bbox_list': BN x (Mi, 4), 'bbox_score_list': BN x (Mi, 1)}
-        bbox_roi = bbox_dict['bbox_list']
         outs_roi.update(bbox_dict)
+        return outs_roi, img_feats
 
-        if img_metas[0]['scene_token'] != self.prev_scene_token:
-            self.prev_scene_token = img_metas[0]['scene_token']
-            data['prev_exists'] = data['img'].new_zeros(1)
-            self.pts_bbox_head.reset_memory()
-        else:
-            data['prev_exists'] = data['img'].new_ones(1)
-
-        outs = self.pts_bbox_head(img_metas, outs_roi, **data)
-        bbox_list = self.pts_bbox_head.get_bboxes(
-            outs, img_metas)
+    def simple_test_pts(self, img_metas, state, **data):
+        """Test function of point cloud branch."""
+        outs_roi, _ = self.get_bboxes(img_metas, **data)
+        if state is None:
+            if img_metas[0]['scene_token'] != self.prev_scene_token:
+                self.prev_scene_token = img_metas[0]['scene_token']
+                data['prev_exists'] = data['img'].new_zeros(1)
+                self.pts_bbox_head.reset_memory(1)
+            else:
+                data['prev_exists'] = data['img'].new_ones(1)
+        
+        bbox_list, state = self.simple_test_bboxes(img_metas, outs_roi, state, **data)
         bbox_results = [
             bbox3d2result(bboxes, scores, labels)
             for bboxes, scores, labels in bbox_list
         ]
-        return bbox_results, bbox_roi
+        return bbox_results, outs_roi['bbox_list'], state
     
-    def simple_test(self, img_metas, **data):
+    def simple_test(self, img_metas, state=None,**data):
         """Test function without augmentaiton."""
         data['img_feats'] = self.extract_img_feat(data['img'])
 
         bbox_list = [dict() for i in range(len(img_metas))]
-        bbox_pts, bbox_roi = self.simple_test_pts(img_metas, **data)
+        bbox_pts, bbox_roi, state = self.simple_test_pts(img_metas, state=state, **data)
         for result_dict, pts_bbox in zip(bbox_list, bbox_pts):
             result_dict['pts_bbox'] = pts_bbox
             # result_dict['bbox_roi'] = bbox_roi
-        return bbox_list
+        return bbox_list, state
 
     
\ No newline at end of file
diff --git a/projects/mmdet3d_plugin/models/utils/detr3d_transformer.py b/projects/mmdet3d_plugin/models/utils/detr3d_transformer.py
index 37384a1..e4bfeec 100644
--- a/projects/mmdet3d_plugin/models/utils/detr3d_transformer.py
+++ b/projects/mmdet3d_plugin/models/utils/detr3d_transformer.py
@@ -122,6 +122,12 @@ class Detr3DTransformer(BaseModule):
             attn_masks=attn_masks)
 
         return inter_states
+    
+    def half(self):
+        self.decoder.half()
+
+    def float(self):
+        self.decoder.float()
 
 @TRANSFORMER_LAYER_SEQUENCE.register_module()
 class Detr3DTransformerDecoder(TransformerLayerSequence):
@@ -188,6 +194,14 @@ class Detr3DTransformerDecoder(TransformerLayerSequence):
         # np.save('/data/PETR-stream/out_weights/{}.npy'.format(path), all_out_weights)
 
         return torch.stack(intermediate)
+    
+    def half(self):
+        for layer in self.layers:
+            layer.half()
+
+    def float(self):
+        for layer in self.layers:
+            layer.float()
 
 @TRANSFORMER_LAYER.register_module()
 class Detr3DTemporalDecoderLayer(BaseModule):
@@ -372,7 +386,7 @@ class Detr3DTemporalDecoderLayer(BaseModule):
                         f'attn_masks {len(attn_masks)} must be equal ' \
                         f'to the number of attention in ' \
                         f'operation_order {self.num_attn}'
-
+        dtype = query.dtype
 
         for layer in self.operation_order:
             if layer == 'self_attn':
@@ -384,11 +398,11 @@ class Detr3DTemporalDecoderLayer(BaseModule):
                     temp_pos = query_pos
                 query = self.attentions[attn_index](
                     query,
-                    temp_key,
-                    temp_value,
+                    temp_key.to(dtype),
+                    temp_value.to(dtype),
                     identity if self.pre_norm else None,
                     query_pos=query_pos,
-                    key_pos=temp_pos,
+                    key_pos=temp_pos.to(dtype),
                     attn_mask=attn_masks[attn_index],
                     key_padding_mask=query_key_padding_mask,
                     **kwargs)
@@ -396,7 +410,7 @@ class Detr3DTemporalDecoderLayer(BaseModule):
                 identity = query
 
             elif layer == 'norm':
-                query = self.norms[norm_index](query)
+                query = self.norms[norm_index](query.float()).to(dtype)
                 norm_index += 1
 
             elif layer == 'cross_attn':
@@ -478,6 +492,32 @@ class Detr3DTemporalDecoderLayer(BaseModule):
             key_padding_mask,
         )
         return x
+    
+    def half(self):
+        for layer in self.attentions:
+            layer.half()
+        self.ffns.half()
+
+    def float(self):
+        for layer in self.attentions:
+            layer.float()
+        self.ffns.float()
+
+
+def _normalize_points(points_2d, pad_shape):
+    points_2d = points_2d[:,:,:,:,:,0]
+    points_2d = points_2d[:,:,:,:, 0:2] / torch.clamp(points_2d[:,:,:,:, 2:3], min=1e-4)
+    points_2d[..., 0:1] = points_2d[..., 0:1] / pad_shape[1]
+    points_2d[..., 1:2] = points_2d[..., 1:2] / pad_shape[0]
+    return points_2d
+
+def projectPoints(lidar2img_mat, key_points, pad_shape, num_groups, num_levels):
+    pts_extand = torch.cat([key_points, torch.ones_like(key_points[..., :1])], dim=-1)
+    points_2d = torch.matmul(lidar2img_mat[:, :, None, None], pts_extand[:, None, ..., None])
+    points_2d = _normalize_points(points_2d, pad_shape)
+    points_2d = points_2d.flatten(end_dim=1) #[b*7, 900, 13, 2]
+    points_2d = points_2d[:, :, None, None, :, :].repeat(1, 1, num_groups, num_levels, 1, 1)
+    return points_2d
 
 
 @ATTENTION.register_module()
@@ -522,48 +562,56 @@ class DeformableFeatureAggregationCuda(BaseModule):
     def forward(self, instance_feature, query_pos,feat_flatten, reference_points, spatial_flatten, level_start_index, pc_range, lidar2img_mat, img_metas):
         bs, num_anchor = reference_points.shape[:2]
         reference_points = get_global_pos(reference_points, pc_range)
-        key_points = reference_points.unsqueeze(-2) + self.learnable_fc(instance_feature).reshape(bs, num_anchor, -1, 3)
+        key_points = reference_points.unsqueeze(-2) + self.learnable_fc(instance_feature.to(reference_points.dtype)).reshape(bs, num_anchor, -1, 3)
         # key_points_ = key_points.cpu().numpy()
-        weights = self._get_weights(instance_feature, query_pos, lidar2img_mat)
+        weights = self._get_weights(instance_feature, query_pos, lidar2img_mat).to(feat_flatten.dtype)
 
-        features = self.feature_sampling(feat_flatten, spatial_flatten, level_start_index, key_points, weights, lidar2img_mat, img_metas)
+        features = self.feature_sampling(feat_flatten, spatial_flatten, level_start_index, key_points, weights, lidar2img_mat, img_metas).to(instance_feature.dtype)
 
         output = self.output_proj(features)
         output = self.drop(output) + instance_feature
         return output
+    
+    def _embed_camera(self, lidar2img):
+        cam_embed = self.cam_embed(lidar2img) # B, N, C
+        return cam_embed
+
 
     def _get_weights(self, instance_feature, anchor_embed, lidar2img_mat):
         bs, num_anchor = instance_feature.shape[:2]
         lidar2img = lidar2img_mat[..., :3, :].flatten(-2)
-        cam_embed = self.cam_embed(lidar2img) # B, N, C
+        cam_embed = self._embed_camera(lidar2img)
         feat_pos = (instance_feature + anchor_embed).unsqueeze(2) + cam_embed.unsqueeze(1)
         weights = self.weights_fc(feat_pos).reshape(bs, num_anchor, -1, self.num_groups).softmax(dim=-2)
         weights = weights.reshape(bs, num_anchor, self.num_cams, -1, self.num_groups).permute(0, 2, 1, 4, 3).contiguous()
         return weights.flatten(end_dim=1)
+    
 
-    def feature_sampling(self, feat_flatten, spatial_flatten, level_start_index, key_points, weights, lidar2img_mat, img_metas):
-        bs, num_anchor, _ = key_points.shape[:3]
+    def projectPoints(self, lidar2img_mat, key_points, pad_shape):
+        points_2d = projectPoints(lidar2img_mat, key_points, pad_shape, self.num_groups, self.num_levels)
+        return points_2d
 
-        pts_extand = torch.cat([key_points, torch.ones_like(key_points[..., :1])], dim=-1)
-        points_2d = torch.matmul(lidar2img_mat[:, :, None, None], pts_extand[:, None, ..., None]).squeeze(-1)
 
-        points_2d = points_2d[..., :2] / torch.clamp(points_2d[..., 2:3], min=1e-5)
-        points_2d[..., 0:1] = points_2d[..., 0:1] / img_metas[0]['pad_shape'][0][1]
-        points_2d[..., 1:2] = points_2d[..., 1:2] / img_metas[0]['pad_shape'][0][0]
+    def feature_sampling(self, feat_flatten, spatial_flatten, level_start_index, key_points, weights, lidar2img_mat, img_metas):
+        bs, num_anchor, _ = key_points.shape[:3]
+        num_features = feat_flatten.shape[2]
 
-        points_2d = points_2d.flatten(end_dim=1) #[b*7, 900, 13, 2]
-        points_2d = points_2d[:, :, None, None, :, :].repeat(1, 1, self.num_groups, self.num_levels, 1, 1)
-        # print(points_2d)
+        points_2d = self.projectPoints(lidar2img_mat=lidar2img_mat.float(), key_points=key_points.float(), pad_shape=torch.Tensor(img_metas[0]['pad_shape'][0]).float())
 
         bn, num_value, _ = feat_flatten.size()
         feat_flatten = feat_flatten.reshape(bn, num_value, self.num_groups, -1)
         # attention_weights = weights * mask
+        points_2d = points_2d.to(feat_flatten.dtype)
         output = MultiScaleDeformableAttnFunction.apply(
                 feat_flatten, spatial_flatten, level_start_index, points_2d,
                 weights, self.im2col_step)
         
-        output = output.reshape(bs, self.num_cams, num_anchor, -1)
-        # print(output)
+        output = output.reshape(bs, self.num_cams, num_anchor, num_features)
 
+        return output.sum(1)
+    
+    def half(self):
+        self.output_proj.half()
 
-        return output.sum(1)
\ No newline at end of file
+    def float(self):
+        self.output_proj.float()
diff --git a/projects/mmdet3d_plugin/models/utils/misc.py b/projects/mmdet3d_plugin/models/utils/misc.py
index 08d29b0..984f814 100644
--- a/projects/mmdet3d_plugin/models/utils/misc.py
+++ b/projects/mmdet3d_plugin/models/utils/misc.py
@@ -185,9 +185,10 @@ class MLN(nn.Module):
         c = self.reduce(c)
         gamma = self.gamma(c)
         beta = self.beta(c)
-        out = gamma * x + beta
+        out = gamma.to(x.dtype) * x + beta.to(x.dtype)
 
         return out
+         
 
 
 def transform_reference_points(reference_points, egopose, reverse=False, translation=True):
diff --git a/tools/create_infos_av2/create_av2_infos.py b/tools/create_infos_av2/create_av2_infos.py
index 9f2d80f..64e2d9b 100644
--- a/tools/create_infos_av2/create_av2_infos.py
+++ b/tools/create_infos_av2/create_av2_infos.py
@@ -48,6 +48,7 @@ def create_av2_infos(dataset_dir, split, out_dir):
         # if i % 5000 != 0: # to create mini pkl for debug
         #     continue
         infos = {}
+        skip = False
         record = sensor_caches.iloc[i].name
         log_id, _, lidar_timestamp_ns = record
         log_dir = src_dir / log_id
@@ -66,6 +67,7 @@ def create_av2_infos(dataset_dir, split, out_dir):
                 print("No corresponding ring image")
                 cam_infos[cam_name] = None
                 camera_models[cam_name] = None
+                skip = True
                 continue
             
             fpath = Path(split) / log_id / "sensors" / "cameras" / cam_name / f"{cam_timestamp_ns}.jpg" #img path
@@ -106,7 +108,8 @@ def create_av2_infos(dataset_dir, split, out_dir):
             infos['gt2d_infos'] = gt2d_infos
         
         #samplesinfo
-        av2_split_infos.append(infos)
+        if not skip:
+            av2_split_infos.append(infos)
     print('{}_sample:{}'.format(split, len(av2_split_infos)))
     data = dict(infos=av2_split_infos, split=split)
     info_path = osp.join(out_dir, 'av2_{}_infos_mini.pkl'.format(split))
@@ -278,6 +281,7 @@ def post_process_coords(corner_coords, imsize = (2048, 1550)):
 if __name__ == '__main__':
     splits = ["val", "train"]
     for split in splits:
+        print(split)
         create_av2_infos(
             dataset_dir=dataset_dir,
             split=split,
