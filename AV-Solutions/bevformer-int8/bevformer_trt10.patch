diff --git a/TensorRT/plugin/grid_sampler/gridSamplerKernel.cu b/TensorRT/plugin/grid_sampler/gridSamplerKernel.cu
index 4cd713f..f3c7be8 100644
--- a/TensorRT/plugin/grid_sampler/gridSamplerKernel.cu
+++ b/TensorRT/plugin/grid_sampler/gridSamplerKernel.cu
@@ -1922,7 +1922,7 @@ __global__ void grid_sampler_3d_kernel(
   }
 }
 
-void create_desc(const int *dims, int nb_dims, TensorDesc &desc) {
+void create_desc(const TRT_INT_TYPE *dims, int nb_dims, TensorDesc &desc) {
   memcpy(&desc.shape[0], dims, sizeof(int) * nb_dims);
   desc.stride[nb_dims - 1] = 1;
   for (int i = nb_dims - 2; i >= 0; --i) {
@@ -1931,8 +1931,8 @@ void create_desc(const int *dims, int nb_dims, TensorDesc &desc) {
 }
 
 template <typename T>
-void grid_sample(T *output, const T *input, const T *grid, int *output_dims,
-                 int *input_dims, int *grid_dims, int nb_dims,
+void grid_sample(T *output, const T *input, const T *grid, TRT_INT_TYPE *output_dims,
+                 TRT_INT_TYPE *input_dims, TRT_INT_TYPE *grid_dims, int nb_dims,
                  GridSamplerInterpolation interp, GridSamplerPadding padding,
                  bool align_corners, cudaStream_t stream) {
   TensorDesc input_desc;
@@ -1969,7 +1969,7 @@ void grid_sample(T *output, const T *input, const T *grid, int *output_dims,
 
 template <>
 void grid_sample(__half2 *output, const __half2 *input, const __half2 *grid,
-                 int *output_dims, int *input_dims, int *grid_dims, int nb_dims,
+                 TRT_INT_TYPE *output_dims, TRT_INT_TYPE *input_dims, TRT_INT_TYPE *grid_dims, int nb_dims,
                  GridSamplerInterpolation interp, GridSamplerPadding padding,
                  bool align_corners, cudaStream_t stream) {
   TensorDesc input_desc;
@@ -2009,8 +2009,8 @@ void grid_sample(__half2 *output, const __half2 *input, const __half2 *grid,
 
 void grid_sample_int8(int8_4 *output, const float &scale_o, const int8_4 *input,
                       const float &scale_i, const int8_4 *grid,
-                      const float &scale_g, int *output_dims, int *input_dims,
-                      int *grid_dims, int nb_dims,
+                      const float &scale_g, TRT_INT_TYPE *output_dims, TRT_INT_TYPE *input_dims,
+                      TRT_INT_TYPE *grid_dims, int nb_dims,
                       GridSamplerInterpolation interp,
                       GridSamplerPadding padding, bool align_corners,
                       cudaStream_t stream) {
@@ -2043,22 +2043,22 @@ void grid_sample_int8(int8_4 *output, const float &scale_o, const int8_4 *input,
 }
 
 template void grid_sample<float>(float *output, const float *input,
-                                 const float *grid, int *output_dims,
-                                 int *input_dims, int *grid_dims, int nb_dims,
+                                 const float *grid, TRT_INT_TYPE *output_dims,
+                                 TRT_INT_TYPE *input_dims, TRT_INT_TYPE *grid_dims, int nb_dims,
                                  GridSamplerInterpolation interp,
                                  GridSamplerPadding padding, bool align_corners,
                                  cudaStream_t stream);
 
 template void grid_sample<__half>(__half *output, const __half *input,
-                                  const __half *grid, int *output_dims,
-                                  int *input_dims, int *grid_dims, int nb_dims,
+                                  const __half *grid, TRT_INT_TYPE *output_dims,
+                                  TRT_INT_TYPE *input_dims, TRT_INT_TYPE *grid_dims, int nb_dims,
                                   GridSamplerInterpolation interp,
                                   GridSamplerPadding padding,
                                   bool align_corners, cudaStream_t stream);
 
 template void grid_sample<__half2>(__half2 *output, const __half2 *input,
-                                   const __half2 *grid, int *output_dims,
-                                   int *input_dims, int *grid_dims, int nb_dims,
+                                   const __half2 *grid, TRT_INT_TYPE *output_dims,
+                                   TRT_INT_TYPE *input_dims, TRT_INT_TYPE *grid_dims, int nb_dims,
                                    GridSamplerInterpolation interp,
                                    GridSamplerPadding padding,
                                    bool align_corners, cudaStream_t stream);
diff --git a/TensorRT/plugin/grid_sampler/gridSamplerKernel.h b/TensorRT/plugin/grid_sampler/gridSamplerKernel.h
index 0a913c1..321e167 100644
--- a/TensorRT/plugin/grid_sampler/gridSamplerKernel.h
+++ b/TensorRT/plugin/grid_sampler/gridSamplerKernel.h
@@ -7,20 +7,25 @@
 
 #include "cuda_int8.h"
 #include <cuda_runtime.h>
+#include <cuda_runtime_api.h>
+#include <type_traits>
+#include <NvInfer.h>
+
+typedef std::conditional<NV_TENSORRT_MAJOR<10, int, int64_t>::type TRT_INT_TYPE;
 
 enum class GridSamplerInterpolation { Bilinear, Nearest, Bicubic };
 enum class GridSamplerPadding { Zeros, Border, Reflection };
 
 template <typename T>
-void grid_sample(T *output, const T *input, const T *grid, int *output_dims,
-                 int *input_dims, int *grid_dims, int nb_dims,
+void grid_sample(T *output, const T *input, const T *grid, TRT_INT_TYPE *output_dims,
+                 TRT_INT_TYPE *input_dims, TRT_INT_TYPE *grid_dims, int nb_dims,
                  GridSamplerInterpolation interp, GridSamplerPadding padding,
                  bool align_corners, cudaStream_t stream);
 
 void grid_sample_int8(int8_4 *output, const float &scale_o, const int8_4 *input,
                       const float &scale_i, const int8_4 *grid,
-                      const float &scale_g, int *output_dims, int *input_dims,
-                      int *grid_dims, int nb_dims,
+                      const float &scale_g, TRT_INT_TYPE *output_dims, TRT_INT_TYPE *input_dims,
+                      TRT_INT_TYPE *grid_dims, int nb_dims,
                       GridSamplerInterpolation interp,
                       GridSamplerPadding padding, bool align_corners,
                       cudaStream_t stream);
diff --git a/TensorRT/plugin/rotate/rotateKernel.cu b/TensorRT/plugin/rotate/rotateKernel.cu
index 379b7cc..71e1f9f 100644
--- a/TensorRT/plugin/rotate/rotateKernel.cu
+++ b/TensorRT/plugin/rotate/rotateKernel.cu
@@ -706,7 +706,7 @@ __global__ void rotateKernel_int8(const int nthreads, int8_4 *output,
 }
 
 template <typename T>
-void rotate(T *output, T *input, T *angle, T *center, int *input_dims,
+void rotate(T *output, T *input, T *angle, T *center, TRT_INT_TYPE *input_dims,
             RotateInterpolation interp, cudaStream_t stream) {
   int channel = input_dims[0];
   int height = input_dims[1];
@@ -719,7 +719,7 @@ void rotate(T *output, T *input, T *angle, T *center, int *input_dims,
 }
 
 void rotate_h2(__half2 *output, __half2 *input, __half *angle, __half *center,
-               int *input_dims, RotateInterpolation interp,
+               TRT_INT_TYPE *input_dims, RotateInterpolation interp,
                cudaStream_t stream) {
   int channel = input_dims[0];
   int height = input_dims[1];
@@ -734,7 +734,7 @@ void rotate_h2(__half2 *output, __half2 *input, __half *angle, __half *center,
 template <typename T>
 void rotate_int8(int8_4 *output, float scale_o, const int8_4 *input,
                  float scale_i, const T *angle, const T *center,
-                 int *input_dims, RotateInterpolation interp,
+                 TRT_INT_TYPE *input_dims, RotateInterpolation interp,
                  cudaStream_t stream) {
   int channel = input_dims[0];
   int height = input_dims[1];
@@ -748,19 +748,19 @@ void rotate_int8(int8_4 *output, float scale_o, const int8_4 *input,
 }
 
 template void rotate(float *output, float *input, float *angle, float *center,
-                     int *input_dims, RotateInterpolation interp,
+                     TRT_INT_TYPE *input_dims, RotateInterpolation interp,
                      cudaStream_t stream);
 
 template void rotate(__half *output, __half *input, __half *angle,
-                     __half *center, int *input_dims,
+                     __half *center, TRT_INT_TYPE *input_dims,
                      RotateInterpolation interp, cudaStream_t stream);
 
 template void rotate_int8(int8_4 *output, float scale_o, const int8_4 *input,
                           float scale_i, const float *angle,
-                          const float *center, int *input_dims,
+                          const float *center, TRT_INT_TYPE *input_dims,
                           RotateInterpolation interp, cudaStream_t stream);
 
 template void rotate_int8(int8_4 *output, float scale_o, const int8_4 *input,
                           float scale_i, const __half *angle,
-                          const __half *center, int *input_dims,
+                          const __half *center, TRT_INT_TYPE *input_dims,
                           RotateInterpolation interp, cudaStream_t stream);
diff --git a/TensorRT/plugin/rotate/rotateKernel.h b/TensorRT/plugin/rotate/rotateKernel.h
index d5808b9..e8f7030 100644
--- a/TensorRT/plugin/rotate/rotateKernel.h
+++ b/TensorRT/plugin/rotate/rotateKernel.h
@@ -8,21 +8,26 @@
 #include "cuda_int8.h"
 #include <cuda_fp16.h>
 #include <cuda_runtime.h>
+#include <NvInfer.h>
+#include <cuda_runtime_api.h>
+#include <type_traits>
+
+typedef std::conditional<NV_TENSORRT_MAJOR<10, int, int64_t>::type TRT_INT_TYPE;
 
 enum class RotateInterpolation { Bilinear, Nearest };
 
 template <typename T>
-void rotate(T *output, T *input, T *angle, T *center, int *input_dims,
+void rotate(T *output, T *input, T *angle, T *center, TRT_INT_TYPE *input_dims,
             RotateInterpolation interp, cudaStream_t stream);
 
 void rotate_h2(__half2 *output, __half2 *input, __half *angle, __half *center,
-               int *input_dims, RotateInterpolation interp,
+               TRT_INT_TYPE *input_dims, RotateInterpolation interp,
                cudaStream_t stream);
 
 template <typename T>
 void rotate_int8(int8_4 *output, float scale_o, const int8_4 *input,
                  float scale_i, const T *angle, const T *center,
-                 int *input_dims, RotateInterpolation interp,
+                 TRT_INT_TYPE *input_dims, RotateInterpolation interp,
                  cudaStream_t stream);
 
 #endif // TENSORRT_OPS_ROTATEKERNEL_H
diff --git a/det2trt/convert/pytorch2onnx.py b/det2trt/convert/pytorch2onnx.py
index 978e544..0cd9282 100644
--- a/det2trt/convert/pytorch2onnx.py
+++ b/det2trt/convert/pytorch2onnx.py
@@ -62,8 +62,8 @@ def pytorch2onnx(
         input_names=input_name,
         output_names=output_name,
         export_params=True,
-        keep_initializers_as_inputs=True,
-        do_constant_folding=False,
+        keep_initializers_as_inputs=False,
+        do_constant_folding=True,
         verbose=verbose,
         opset_version=opset_version,
         dynamic_axes=dynamic_axes,
diff --git a/det2trt/quantization/calibrator_trt.py b/det2trt/quantization/calibrator_trt.py
index af58840..59b9b30 100644
--- a/det2trt/quantization/calibrator_trt.py
+++ b/det2trt/quantization/calibrator_trt.py
@@ -60,7 +60,7 @@ def get_calibrator(calibrator):
                     return None
                 assert set(names) == self.names
                 # Assume self.batches is a generator that provides batch data.
-                data = self.iter.next()
+                data = next(self.iter)
                 self.decode_data(data)
                 # Assume that self.device_input is a device buffer allocated by the constructor.
                 [
diff --git a/det2trt/utils/tensorrt.py b/det2trt/utils/tensorrt.py
index 36198fc..44ffe5e 100644
--- a/det2trt/utils/tensorrt.py
+++ b/det2trt/utils/tensorrt.py
@@ -43,35 +43,42 @@ def allocate_buffers(engine, context, input_shapes, output_shapes):
     inputs = []
     outputs = []
     bindings = []
-    for binding_id, binding in enumerate(engine):
-        if engine.binding_is_input(binding):
+    tensor_names = [
+        engine.get_tensor_name(i) for i in range(engine.num_io_tensors)
+    ]
+    for binding in tensor_names:
+        tensor_mode = engine.get_tensor_mode(binding)
+        if tensor_mode == trt.TensorIOMode.INPUT:
             dims = input_shapes[binding]
-            context.set_binding_shape(binding_id, dims)
+            context.set_input_shape(binding, dims)
         else:
             dims = output_shapes[binding]
 
         size = trt.volume(dims)
         # The maximum batch size which can be used for inference.
-        dtype = trt.nptype(engine.get_binding_dtype(binding))
+        dtype = trt.nptype(engine.get_tensor_dtype(binding))
         assert dtype == np.float32, "Engine's inputs/outputs only support FP32."
         # Allocate host and device buffers
         host_mem = cuda.pagelocked_empty(size, dtype)
         device_mem = cuda.mem_alloc(host_mem.nbytes)
         # Append the device buffer to device bindings.
         bindings.append(int(device_mem))
-        if engine.binding_is_input(binding):
+        if tensor_mode == trt.TensorIOMode.INPUT:
             inputs.append(HostDeviceMem(binding, host_mem, device_mem))
         else:
             outputs.append(HostDeviceMem(binding, host_mem, device_mem))
+        # Setup context tensor address.
+        context.set_tensor_address(binding, int(device_mem))
+
     return inputs, outputs, bindings
 
 
-def do_inference(context, bindings, inputs, outputs, stream, batch_size=1):
+def do_inference(context, inputs, outputs, stream):
     [cuda.memcpy_htod_async(inp.device, inp.host, stream) for inp in inputs]
 
     stream.synchronize()
     t1 = time.time()
-    context.execute_async_v2(bindings=bindings, stream_handle=stream.handle)
+    context.execute_async_v3(stream_handle=stream.handle)
     stream.synchronize()
     t2 = time.time()
     [cuda.memcpy_dtoh_async(out.host, out.device, stream) for out in outputs]
diff --git a/tools/bevformer/evaluate_trt.py b/tools/bevformer/evaluate_trt.py
index 2648ecb..c572f0b 100644
--- a/tools/bevformer/evaluate_trt.py
+++ b/tools/bevformer/evaluate_trt.py
@@ -7,8 +7,9 @@ import mmcv
 import copy
 import numpy as np
 from mmcv import Config
-from mmdeploy.backend.tensorrt import load_tensorrt_plugin
 
+import os
+import ctypes
 import sys
 
 sys.path.append(".")
@@ -26,13 +27,31 @@ def parse_args():
     parser = argparse.ArgumentParser(description="MMDet test (and eval) a model")
     parser.add_argument("config", help="test config file path")
     parser.add_argument("trt_model", help="checkpoint file")
+    parser.add_argument("--trt_plugins", default="/workspace/BEVFormer_tensorrt/TensorRT/lib/libtensorrt_ops.so")
     args = parser.parse_args()
     return args
 
 
+def load_tensorrt_plugin(lib_path) -> bool:
+    """Load TensorRT plugins library.
+
+    Returns:
+        bool: True if TensorRT plugin library is successfully loaded.
+    """
+    success = False
+    if os.path.exists(lib_path):
+        ctypes.CDLL(lib_path)
+        print(f'Successfully loaded tensorrt plugins from {lib_path}')
+        success = True
+    else:
+        print(f'Could not load the library of tensorrt plugins. \
+            Because the file does not exist: {lib_path}')
+    return success
+
+
 def main():
     args = parse_args()
-    load_tensorrt_plugin()
+    load_tensorrt_plugin(args.trt_plugins)
 
     trt_model = args.trt_model
     config_file = args.config
@@ -134,7 +153,7 @@ def main():
                 raise RuntimeError(f"Cannot find input name {inp.name}.")
 
         trt_outputs, t = do_inference(
-            context, bindings=bindings, inputs=inputs, outputs=outputs, stream=stream
+            context, inputs=inputs, outputs=outputs, stream=stream
         )
 
         trt_outputs = {
