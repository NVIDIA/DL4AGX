# Deploy DeepSeek-R1 Distilled LLama-3.1-8B Model on Orin

[DeepSeek-R1](https://github.com/deepseek-ai/DeepSeek-R1) is an open-source Large Language Model (LLM) with state-of-the-art reasoning capabilities. In addition to the original DeepSeek-R1 671B model, several distilled LLMs and Vision Language Models (VLMs) have been released. These distilled models are fine-tuned based on open-source models, leveraging data generated by the DeepSeek-R1 model. This repository provides a step-by-step guide on how to deploy the [DeepSeek-R1 distilled LLama-3.1-8B](https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Llama-8B) model on the Drive Orin platform using [TensorRT-LLM 0.13]((https://github.com/NVIDIA/TensorRT-LLM/tree/v0.13.0)). The guide follows the deployment process used for the [Llama-3.1-8B](../Llama-3.1-8B-trtllm/), which is intended for evaluation purposes only. 


## Orin Environment:

Please make sure you have the following Orin environment setup on your device. You can follow the [DRIVE OS Install Guidance](https://developer.nvidia.com/docs/drive/drive-os/6.0.9/public/drive-os-linux-installation/index.html) to prepare the device. To get the required DRIVE OS and TensorRT version, please refer to details on the [NVIDIA DRIVE Downloads](https://developer.nvidia.com/drive/downloads) site.

- Drive OS 0.6.9.0
- Python 3.8
- CUDA 11.4
- Ubuntu 20.04
- TensorRT 10.4.0.11


## Generate DeepSeek-R1-Distill-LLaMA-3.1-8B Model with INT4_AWQ and INT8-KV-Cache on x86

In this example, we are recommending quantizing the DeepSeek-R1-Distill-LLaMA-3.1-8B model during the deployment. To quantize this distilled Llama model, please follow this link to install the TensorRT-LLM on an x86 system with at least 16GB GPU memory the following sample commands: 

```
python -m venv trtllm
source trtllm/bin/activate
pip install tensorrt_llm==0.16 
```
Please check if your venv has the following dependency:
- TensorRT-LLM 0.16
- modelOpt 0.19.0

You can download the DeepSeek-R1-Distill-LLaMA-3.1-8B model from [huggingface page](https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Llama-8B). After confirming the above setup, follow the commands below to quantize the distilled Llama model:
```
cd $PWD/TensorRT-LLM/examples/llama 
python convert_checkpoint.py --model_dir $input_model --output_dir $output_model --dtype float16 --use_weight_only --weight_only_precision=int4_awq --int8_kv_cache
```




## Build TensorRT-LLM from Source

After generating the quantized DeepSeek-R1-Distill-LLaMA-3.1-8B model on x86 system, please copy **DeepSeek-R1-Distill-Llama-8B** and **DeepSeek-R1-Distill-Llama-8B_int4_awq_kv_int8** folders from host to your target DRIVE Orin working directory. Please clone all the files and folders under [Llama-3.1-8B](../Llama-3.1-8B-trtllm/) to your `work_dir`. Then please run the following command to build TensorRT-LLM from source. 
```
./setup_from_source_DeepSeek-R1-Llama-8B.sh
```

The overall working directory after running `setup_from_source_DeepSeek-R1-Llama-8B.sh` will be as follows:
```
work_dir
├── batch_manager 
├── executor
├── build_from_scoure_changes.patch 
├── setup_from_source_DeepSeek-R1-Llama-8B.sh 
├── TensorRT-LLM
├── TensorRT-10.4.0.11
├── DeepSeek-R1-Distill-Llama-8B
├── DeepSeek-R1-Distill-Llama-8B_int4_awq_kv_int8 
├── trtllm_0.13 (vitual enviornment)
├── nccl
├── json_modifier.py
```
Note that on Drive Orin, currently TensorRT-LLM v0.13 is used for inference.

## Build LLM Engine and Sample Inference on Drive Orin
After successfully build the TensorRT-LLM from source on target Drive Orin, the following command will build the corresponding LLM engine:
```
trtllm-build --checkpoint_dir $path_engine --output_dir $path_engine/1-gpu/ --gemm_plugin auto --max_batch_size 1 
```

Using the above engine, you can run LLM inference using the following commands:
```
cd TensorRT-LLM/examples/
python run.py --max_output_len 128 --engine_dir $path_engine/1-gpu/ --tokenizer_dir $path_model 
```

Here is a sample inference output for DeepSeek-R1-Distill-LLaMA-3.1-8B model 
```
Input [Text 0]: "<｜begin▁of▁sentence｜>Born in north-east France, Soyer trained as a"
Output [Text 0 Beam 0]: " chef in Paris. He worked at the famous restaurant 'Le Grand Véfour' under chef Marie-Antoine Carême, and later at 'Le Coq Rico' with chef Charles de la Faye. Soyer is known for his work in the development of the restaurant trade, and he wrote several books on the subject. He also became a member of the French Academy of Gastronomy.
So, Soyer is a French chef, born in the 19th century, who worked in some of the most prominent restaurants in Paris. His contributions to the restaurant industry are well-documented, and he's considered a significant figure"
```
