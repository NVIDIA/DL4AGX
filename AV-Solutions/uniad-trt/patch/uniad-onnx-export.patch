From bd9bd8bc9df199cfb70c1dfd34fe4bef057bc3a5 Mon Sep 17 00:00:00 2001
From: Bianjiang Yang <bianjiangy@bianjiangy-mlt.client.nvidia.com>
Date: Tue, 13 Aug 2024 13:43:53 -0700
Subject: [PATCH] step3: modification of UniAD code for onnx export

---
 projects/mmdet3d_plugin/core/bbox/util.py     |   59 +-
 .../models/backbones/__init__.py              |    4 +-
 .../uniad/dense_heads/motion_head.py          |  520 ++++++++-
 .../motion_head_plugin/base_motion_head.py    |   11 +
 .../dense_heads/motion_head_plugin/modules.py |  305 ++++-
 .../motion_deformable_attn.py                 |  552 ++++++++-
 .../uniad/dense_heads/occ_head.py             |  200 +++-
 .../dense_heads/occ_head_plugin/modules.py    |   14 +-
 .../dense_heads/occ_head_plugin/utils.py      |   48 +
 .../uniad/dense_heads/panseg_head.py          |  381 +++++++
 .../uniad/dense_heads/planning_head.py        |  113 +-
 .../seg_head_plugin/seg_detr_head.py          |    4 +-
 .../seg_head_plugin/seg_mask_head.py          |   43 +-
 .../uniad/dense_heads/track_head.py           |  310 ++++-
 .../dense_heads/track_head_plugin/__init__.py |    2 +-
 .../dense_heads/track_head_plugin/modules.py  |  172 +++
 .../dense_heads/track_head_plugin/tracker.py  |    9 +-
 .../uniad/detectors/uniad_e2e.py              |  293 ++++-
 .../uniad/detectors/uniad_track.py            | 1015 ++++++++++++++++-
 .../mmdet3d_plugin/uniad/modules/__init__.py  |   22 +-
 .../mmdet3d_plugin/uniad/modules/decoder.py   |  715 +++++++++++-
 .../mmdet3d_plugin/uniad/modules/encoder.py   |  677 ++++++++++-
 .../uniad/modules/spatial_cross_attention.py  |  759 +++++++++++-
 .../uniad/modules/temporal_self_attention.py  |  448 +++++++-
 .../uniad/modules/transformer.py              |  368 +++++-
 25 files changed, 6957 insertions(+), 87 deletions(-)

diff --git a/projects/mmdet3d_plugin/core/bbox/util.py b/projects/mmdet3d_plugin/core/bbox/util.py
index c54bd75..0824e5f 100644
--- a/projects/mmdet3d_plugin/core/bbox/util.py
+++ b/projects/mmdet3d_plugin/core/bbox/util.py
@@ -1,5 +1,4 @@
-import torch 
-
+import torch, math
 
 def normalize_bbox(bboxes, pc_range):
 
@@ -50,4 +49,58 @@ def denormalize_bbox(normalized_bboxes, pc_range):
         denormalized_bboxes = torch.cat([cx, cy, cz, w, l, h, rot, vx, vy], dim=-1)
     else:
         denormalized_bboxes = torch.cat([cx, cy, cz, w, l, h, rot], dim=-1)
-    return denormalized_bboxes
\ No newline at end of file
+    return denormalized_bboxes
+
+
+def denormalize_bbox_trt(normalized_bboxes, pc_range):
+    # rotation 
+    rot_sine = normalized_bboxes[..., 6:7]
+
+    rot_cosine = normalized_bboxes[..., 7:8]
+    rot = custom_torch_atan2_trt(rot_sine, rot_cosine)
+
+    # center in the bev
+    cx = normalized_bboxes[..., 0:1]
+    cy = normalized_bboxes[..., 1:2]
+    cz = normalized_bboxes[..., 4:5]
+   
+    # size
+    w = normalized_bboxes[..., 2:3]
+    l = normalized_bboxes[..., 3:4]
+    h = normalized_bboxes[..., 5:6]
+
+    w = w.exp() 
+    l = l.exp() 
+    h = h.exp() 
+
+    return if_normal_bboxes_size(normalized_bboxes, cx, cy, cz, w, l, h, rot)
+
+def if_normal_bboxes_size(normalized_bboxes, cx, cy, cz, w, l, h, rot):
+    vx = normalized_bboxes[:, 8:9]
+    vy = normalized_bboxes[:, 9:10]
+    denormalized_bboxes = torch.cat([cx, cy, cz, w, l, h, rot, vx, vy], dim=-1)
+    return denormalized_bboxes
+
+def custom_torch_atan2_trt(y, x):
+    '''
+    reference: https://en.wikipedia.org/wiki/Atan2
+    '''
+    eps = 1e-8
+    atan = torch.atan(y/(x+eps))
+    x_eq_0 = x==0
+    x_gt_0 = x>0
+    x_ls_0 = x<0
+    y_ge_0 = y>=0
+    y_gt_0 = y>0
+    y_ls_0 = y<0
+
+    pi_div_2 = (torch.ones(atan.shape).to(atan.device))*(math.pi/2)
+    negative_pi_div_2 = (torch.ones(atan.shape).to(atan.device))*(-math.pi/2)
+
+    atan2 = (negative_pi_div_2)*(x_eq_0 & y_ls_0).int()\
+            + (pi_div_2)*(x_eq_0 & y_gt_0).int()\
+            + (atan-math.pi)*(x_ls_0 & y_ls_0).int()\
+            + (atan+math.pi)*(x_ls_0 & y_ge_0).int()\
+            + (atan) * x_gt_0.int()
+    
+    return atan2.float()
\ No newline at end of file
diff --git a/projects/mmdet3d_plugin/models/backbones/__init__.py b/projects/mmdet3d_plugin/models/backbones/__init__.py
index cea72f5..f68c07f 100644
--- a/projects/mmdet3d_plugin/models/backbones/__init__.py
+++ b/projects/mmdet3d_plugin/models/backbones/__init__.py
@@ -1,3 +1,3 @@
 from .vovnet import VoVNet
-
-__all__ = ['VoVNet']
\ No newline at end of file
+from .bev_resnet import CustomResNet
+__all__ = ['VoVNet', 'CustomResNet']
\ No newline at end of file
diff --git a/projects/mmdet3d_plugin/uniad/dense_heads/motion_head.py b/projects/mmdet3d_plugin/uniad/dense_heads/motion_head.py
index dd3612a..eea2ff4 100644
--- a/projects/mmdet3d_plugin/uniad/dense_heads/motion_head.py
+++ b/projects/mmdet3d_plugin/uniad/dense_heads/motion_head.py
@@ -12,11 +12,12 @@ from projects.mmdet3d_plugin.models.utils.functional import (
     bivariate_gaussian_activation,
     norm_points,
     pos2posemb2d,
-    anchor_coordinate_transform
+    anchor_coordinate_transform,
+    rot_2d
 )
 from .motion_head_plugin.motion_utils import nonlinear_smoother
 from .motion_head_plugin.base_motion_head import BaseMotionHead
-
+from einops import rearrange
 
 @HEADS.register_module()
 class MotionHead(BaseMotionHead):
@@ -558,3 +559,518 @@ class MotionHead(BaseMotionHead):
                 preds['traj_scores' + subfix] = traj_scores
             ret_list.append(preds)
         return ret_list
+
+
+@HEADS.register_module()
+class MotionHeadTRT(MotionHead):
+    """
+    MotionHead module for a neural network, which predicts motion trajectories and is used in an autonomous driving task.
+
+    Args:
+        *args: Variable length argument list.
+        predict_steps (int): The number of steps to predict motion trajectories.
+        transformerlayers (dict): A dictionary defining the configuration of transformer layers.
+        bbox_coder: An instance of a bbox coder to be used for encoding/decoding boxes.
+        num_cls_fcs (int): The number of fully-connected layers in the classification branch.
+        bev_h (int): The height of the bird's-eye-view map.
+        bev_w (int): The width of the bird's-eye-view map.
+        embed_dims (int): The number of dimensions to use for the query and key vectors in transformer layers.
+        num_anchor (int): The number of anchor points.
+        det_layer_num (int): The number of layers in the transformer model.
+        group_id_list (list): A list of group IDs to use for grouping the classes.
+        pc_range: The range of the point cloud.
+        use_nonlinear_optimizer (bool): A boolean indicating whether to use a non-linear optimizer for training.
+        anchor_info_path (str): The path to the file containing the anchor information.
+        vehicle_id_list(list[int]): class id of vehicle class, used for filtering out non-vehicle objects
+    """
+    def __init__(self, *args, **kwargs):
+        super().__init__(*args, **kwargs)
+
+    def forward_test_trt(self, 
+                         bev_embed, 
+                         track_query_embeddings, 
+                         track_boxes_0, 
+                         track_boxes_1, 
+                         track_boxes_2,
+                         track_boxes_3, 
+                         gravity_center, 
+                         yaw,
+                         sdc_embedding, 
+                         sdc_track_boxes_0, 
+                         sdc_track_boxes_1,
+                         sdc_track_boxes_2, 
+                         sdc_track_boxes_3,
+                         lane_query,
+                         lane_query_pos):
+        """Test function"""
+        track_query = track_query_embeddings[None, None, ...]
+        
+        track_query = torch.cat([track_query, 
+                                 sdc_embedding[None, None, None, :]], 
+                                 dim=2)
+
+        track_boxes_0 = torch.cat([track_boxes_0, 
+                                   sdc_track_boxes_0], dim=0)
+        track_boxes_1 = torch.cat([track_boxes_1, 
+                                   sdc_track_boxes_1], dim=0)
+        track_boxes_2 = torch.cat([track_boxes_2, 
+                                   sdc_track_boxes_2], dim=0)
+        track_boxes_3 = torch.cat([track_boxes_3, 
+                                   sdc_track_boxes_3], dim=0)      
+        track_boxes = (track_boxes_0, 
+                       track_boxes_1, 
+                       track_boxes_2, 
+                       track_boxes_3, 
+                       gravity_center,
+                       yaw,
+                       )
+        outputs_traj_scores,\
+        outputs_trajs,\
+        valid_traj_masks,\
+        inter_states,\
+        out_track_query,\
+        track_query_pos = self.forward_trt(bev_embed, track_query, 
+                                           lane_query, lane_query_pos, 
+                                           track_boxes)
+
+        scores, labels = track_boxes_1, track_boxes_2
+        track_scores = scores[None, :]
+        labels[-1] = 0
+        def filter_vehicle_query(inter_states, 
+                                 out_track_query,
+                                 track_query_pos,
+                                 track_scores,
+                                 labels, 
+                                 vehicle_id_list):
+            if len(labels) < 1:  # No other obj query except sdc query.
+                return None
+
+            # select vehicle query according to vehicle_id_list
+            vehicle_mask = torch.zeros_like(labels)
+            for veh_id in vehicle_id_list:
+                vehicle_mask = (vehicle_mask + (labels == veh_id).float()).bool().int()
+
+            inter_states = inter_states[:, :, vehicle_mask>0]
+            out_track_query = out_track_query[:, vehicle_mask>0]
+            track_query_pos = track_query_pos[:, vehicle_mask>0]
+            track_scores = track_scores[:, vehicle_mask>0]
+
+            return (inter_states, 
+                    out_track_query,
+                    track_query_pos,
+                    track_scores,)
+        
+        inter_states, \
+        out_track_query,\
+        track_query_pos,\
+        track_scores = filter_vehicle_query(
+            inter_states, 
+            out_track_query,
+            track_query_pos,
+            track_scores, 
+            labels, 
+            self.vehicle_id_list)
+        
+        # filter sdc query
+        sdc_traj_query = inter_states[:, :, -1]
+        sdc_track_query = out_track_query[:, -1]
+        sdc_track_query_pos = track_query_pos[:, -1]
+        inter_states = inter_states[:, :, :-1]
+        out_track_query = out_track_query[:, :-1]
+        track_query_pos = track_query_pos[:, :-1]
+        track_scores = track_scores[:, :-1]
+
+        return (
+            outputs_traj_scores,
+            outputs_trajs,
+            valid_traj_masks,
+            inter_states,
+            out_track_query,
+            track_query_pos,
+            sdc_traj_query,
+            sdc_track_query,
+            sdc_track_query_pos,
+            track_scores,
+        )
+
+    @auto_fp16(apply_to=('bev_embed', 'track_query', 'lane_query', 'lane_query_pos', 'lane_query_embed', 'prev_bev'))
+    def forward_trt(self, 
+                bev_embed, 
+                track_query, 
+                lane_query, 
+                lane_query_pos, 
+                track_boxes_1, 
+                track_boxes_2, 
+                gravity_center,
+                yaw,):
+        """
+        Applies forward pass on the model for motion prediction using bird's eye view (BEV) embedding, track query, lane query, and track bounding box results.
+
+        Args:
+        bev_embed (torch.Tensor): A tensor of shape (h*w, B, D) representing the bird's eye view embedding.
+        track_query (torch.Tensor): A tensor of shape (B, num_dec, A_track, D) representing the track query.
+        lane_query (torch.Tensor): A tensor of shape (N, M_thing, D) representing the lane query.
+        lane_query_pos (torch.Tensor): A tensor of shape (N, M_thing, D) representing the position of the lane query.
+        track_bbox_results (List[torch.Tensor]): A list of tensors containing the tracking bounding box results for each image in the batch.
+
+        Returns:
+        dict: A dictionary containing the following keys and values:
+        - 'all_traj_scores': A tensor of shape (num_levels, B, A_track, num_points) with trajectory scores for each level.
+        - 'all_traj_preds': A tensor of shape (num_levels, B, A_track, num_points, num_future_steps, 2) with predicted trajectories for each level.
+        - 'valid_traj_masks': A tensor of shape (B, A_track) indicating the validity of trajectory masks.
+        - 'traj_query': A tensor containing intermediate states of the trajectory queries.
+        - 'track_query': A tensor containing the input track queries.
+        - 'track_query_pos': A tensor containing the positional embeddings of the track queries.
+        """
+        
+        dtype = track_query.dtype
+        device = track_query.device
+        num_groups = self.kmeans_anchors.shape[0]
+        # extract the last frame of the track query
+        track_query = track_query[:, -1]
+        
+        # encode the center point of the track query
+        reference_points_track = self._extract_tracking_centers_trt(
+            gravity_center,)
+        track_query_pos = self.boxes_query_embedding_layer(
+            pos2posemb2d(reference_points_track.to(device)))  # B, A, D
+        
+        
+        # construct the learnable query positional embedding
+        # split and stack according to groups
+        learnable_query_pos = self.learnable_motion_query_embedding.weight.to(dtype)  # latent anchor (P*G, D)
+        learnable_query_pos = torch.stack(torch.split(learnable_query_pos, self.num_anchor, dim=0))
+
+        # construct the agent level/scene-level query positional embedding 
+        # (num_groups, num_anchor, 12, 2)
+        # to incorporate the information of different groups and coordinates, and embed the headding and location information
+        agent_level_anchors = self.kmeans_anchors.to(dtype).to(device).view(
+            num_groups, self.num_anchor, self.predict_steps, 2).detach()
+        scene_level_ego_anchors = self.anchor_coordinate_transform_trt(
+            agent_level_anchors, 
+            yaw,
+            gravity_center,
+            with_translation_transform=True)  # B, A, G, P ,12 ,2
+        scene_level_offset_anchors = self.anchor_coordinate_transform_trt(
+            agent_level_anchors, 
+            yaw,
+            gravity_center,
+            with_translation_transform=False)  # B, A, G, P ,12 ,2
+
+        agent_level_norm = norm_points(agent_level_anchors, self.pc_range)
+        scene_level_ego_norm = norm_points(scene_level_ego_anchors, self.pc_range)
+        scene_level_offset_norm = norm_points(scene_level_offset_anchors, self.pc_range)
+
+        # we only use the last point of the anchor
+        agent_level_embedding = self.agent_level_embedding_layer(
+            pos2posemb2d(agent_level_norm[..., -1, :]))  # G, P, D
+        scene_level_ego_embedding = self.scene_level_ego_embedding_layer(
+            pos2posemb2d(scene_level_ego_norm[..., -1, :]))  # B, A, G, P , D
+        scene_level_offset_embedding = self.scene_level_offset_embedding_layer(
+            pos2posemb2d(scene_level_offset_norm[..., -1, :]))  # B, A, G, P , D
+
+        batch_size, num_agents = scene_level_ego_embedding.shape[:2]
+        agent_level_embedding = agent_level_embedding[None,None, ...].expand(
+            batch_size, num_agents, -1, -1, -1)
+        learnable_embed = learnable_query_pos[None, None, ...].expand(
+            batch_size, num_agents, -1, -1, -1)
+
+        
+        # save for latter, anchors
+        # B, A, G, P ,12 ,2 -> B, A, P ,12 ,2
+        scene_level_offset_anchors = self.group_mode_query_pos_trt(
+            track_boxes_2, scene_level_offset_anchors)  
+
+        # select class embedding
+        # B, A, G, P , D-> B, A, P , D
+        agent_level_embedding = self.group_mode_query_pos_trt(
+            track_boxes_2,  agent_level_embedding)  
+        scene_level_ego_embedding = self.group_mode_query_pos_trt(
+            track_boxes_2,  scene_level_ego_embedding)  # B, A, G, P , D-> B, A, P , D
+        
+        # B, A, G, P , D -> B, A, P , D
+        scene_level_offset_embedding = self.group_mode_query_pos_trt(
+            track_boxes_2,  scene_level_offset_embedding)  
+        learnable_embed = self.group_mode_query_pos_trt(
+            track_boxes_2,  learnable_embed)  
+
+        init_reference = scene_level_offset_anchors.detach()
+
+        outputs_traj_scores = []
+        outputs_trajs = []
+
+        inter_states, _ = self.motionformer.forward_trt(
+            track_query,  # B, A_track, D
+            lane_query,  # B, M, D
+            track_query_pos=track_query_pos,
+            lane_query_pos=lane_query_pos,
+            track_boxes_1=track_boxes_1, 
+            track_boxes_2=track_boxes_2, 
+            gravity_center=gravity_center,
+            yaw=yaw,
+            bev_embed=bev_embed,
+            reference_trajs=init_reference,
+            traj_reg_branches=self.traj_reg_branches,
+            traj_cls_branches=self.traj_cls_branches,
+            # anchor embeddings 
+            agent_level_embedding=agent_level_embedding,
+            scene_level_ego_embedding=scene_level_ego_embedding,
+            scene_level_offset_embedding=scene_level_offset_embedding,
+            learnable_embed=learnable_embed,
+            # anchor positional embeddings layers
+            agent_level_embedding_layer=self.agent_level_embedding_layer,
+            scene_level_ego_embedding_layer=self.scene_level_ego_embedding_layer,
+            scene_level_offset_embedding_layer=self.scene_level_offset_embedding_layer,
+            spatial_shapes=torch.tensor(
+                [[self.bev_h, self.bev_w]], device=device),
+            level_start_index=torch.tensor([0], device=device))
+
+        for lvl in range(inter_states.shape[0]): # inter_states shape [3, 1, x, 6, 256]
+            outputs_class = self.traj_cls_branches[lvl](inter_states[lvl])
+            tmp = self.traj_reg_branches[lvl](inter_states[lvl])
+            tmp = self.unflatten_traj(tmp)
+            
+            # we use cumsum trick here to get the trajectory 
+            tmp[..., :2] = torch.cumsum(tmp[..., :2], dim=3)
+
+            outputs_class = self.log_softmax(outputs_class[:,:,:,0,...])
+            outputs_traj_scores.append(outputs_class)
+
+            for bs in range(tmp.shape[0]):
+                tmp[bs] = bivariate_gaussian_activation(tmp[bs])
+            outputs_trajs.append(tmp)
+        outputs_traj_scores = torch.stack(outputs_traj_scores)
+        outputs_trajs = torch.stack(outputs_trajs)
+
+        B, A_track, D = track_query.shape
+        valid_traj_masks = track_query.new_ones((B, A_track)) > 0
+        outs = (outputs_traj_scores,
+                outputs_trajs,
+                valid_traj_masks.float(),
+                inter_states,
+                track_query,
+                track_query_pos,)
+
+        return outs
+    
+    def anchor_coordinate_transform_trt(self, anchors, yaw, gravity_center, with_translation_transform=True, with_rotation_transform=True):
+        """
+        Transform anchor coordinates with respect to detected bounding boxes in the batch.
+
+        Args:
+            anchors (torch.Tensor): A tensor containing the k-means anchor values.
+            bbox_results (List[Tuple[torch.Tensor]]): A list of tuples containing the bounding box results for each image in the batch.
+            with_translate (bool, optional): Whether to perform translation transformation. Defaults to True.
+            with_rot (bool, optional): Whether to perform rotation transformation. Defaults to True.
+
+        Returns:
+            torch.Tensor: A tensor containing the transformed anchor coordinates.
+        """
+        batch_size = 1
+        batched_anchors = []
+        transformed_anchors = anchors[None, ...] # expand num agents: num_groups, num_modes, 12, 2 -> 1, ...
+        for i in range(batch_size):
+            yaw = yaw.to(transformed_anchors.device)
+            bbox_centers = gravity_center.to(transformed_anchors.device)
+            if with_rotation_transform: 
+                angle = yaw - 3.1415953 # num_agents, 1
+                rot_yaw = rot_2d(angle) # num_agents, 2, 2
+                rot_yaw = rot_yaw[:, None, None,:, :] # num_agents, 1, 1, 2, 2
+                transformed_anchors = rearrange(transformed_anchors, 'b g m t c -> b g m c t')  # 1, num_groups, num_modes, 12, 2 -> 1, num_groups, num_modes, 2, 12
+                transformed_anchors = torch.matmul(rot_yaw, transformed_anchors)# -> num_agents, num_groups, num_modes, 12, 2
+                transformed_anchors = rearrange(transformed_anchors, 'b g m c t -> b g m t c')
+            if with_translation_transform:
+                transformed_anchors = bbox_centers[:, None, None, None, :2] + transformed_anchors
+            batched_anchors.append(transformed_anchors)
+        return torch.stack(batched_anchors)
+
+    def _extract_tracking_centers_trt(self, gravity_center):
+        """
+        extract the bboxes centers and normized according to the bev range
+        
+        Args:
+            bbox_results (List[Tuple[torch.Tensor]]): A list of tuples containing the bounding box results for each image in the batch.
+            # bev_range (List[float]): A list of float values representing the bird's eye view range.
+
+        Returns:
+            torch.Tensor: A tensor representing normized centers of the detection bounding boxes.
+        """
+        batch_size = 1
+        bev_range = self.pc_range
+        det_bbox_posembed = []
+        for i in range(batch_size):
+            xy = gravity_center[:, :2]
+            x_norm = (xy[:, 0] - bev_range[0]) / \
+                (bev_range[3] - bev_range[0])
+            y_norm = (xy[:, 1] - bev_range[1]) / \
+                (bev_range[4] - bev_range[1])
+            det_bbox_posembed.append(
+                torch.cat([x_norm[:, None], y_norm[:, None]], dim=-1))
+        return torch.stack(det_bbox_posembed)
+
+    def group_mode_query_pos_trt(self, labels, mode_query_pos):
+        """
+        Group mode query positions based on the input bounding box results.
+        
+        Args:
+            bbox_results (List[Tuple[torch.Tensor]]): A list of tuples containing the bounding box results for each image in the batch.
+            mode_query_pos (torch.Tensor): A tensor of shape (B, A, G, P, D) representing the mode query positions.
+        
+        Returns:
+            torch.Tensor: A tensor of shape (B, A, P, D) representing the classified mode query positions.
+        """
+        batched_mode_query_pos = []
+        self.cls2group = self.cls2group.to(mode_query_pos.device)
+        label = labels.to(mode_query_pos.device)
+        grouped_label = self.cls2group[label]
+        grouped_mode_query_pos = for_loop_agent_num_vec(mode_query_pos, grouped_label)
+        batched_mode_query_pos.append(grouped_mode_query_pos)
+        return torch.stack(batched_mode_query_pos)
+
+@torch.jit.script
+def for_loop_agent_num(mode_query_pos, grouped_label):
+    agent_num = mode_query_pos.shape[1]
+    grouped_mode_query_pos = []
+    for j in range(agent_num):
+        grouped_mode_query_pos.append(
+            mode_query_pos[0, j, grouped_label[j]])
+    return torch.stack(grouped_mode_query_pos)
+
+def for_loop_agent_num_vec(mode_query_pos, grouped_label):
+    # Get the number of agents from the second dimension of mode_query_pos
+    agent_num = mode_query_pos.shape[1]
+
+    # Prepare indices for batched indexing
+    # Create a range tensor for the first dimension
+    first_dim_indices = torch.zeros(agent_num, dtype=torch.int64)
+
+    # Use the grouped_label for the third dimension
+    third_dim_indices = grouped_label
+
+    # Perform advanced indexing to gather the elements
+    grouped_mode_query_pos = mode_query_pos[first_dim_indices, torch.arange(agent_num), third_dim_indices]
+
+    return grouped_mode_query_pos
+
+@HEADS.register_module()
+class MotionHeadTRTP(MotionHeadTRT):
+    """
+    MotionHead module for a neural network, which predicts motion trajectories and is used in an autonomous driving task.
+
+    Args:
+        *args: Variable length argument list.
+        predict_steps (int): The number of steps to predict motion trajectories.
+        transformerlayers (dict): A dictionary defining the configuration of transformer layers.
+        bbox_coder: An instance of a bbox coder to be used for encoding/decoding boxes.
+        num_cls_fcs (int): The number of fully-connected layers in the classification branch.
+        bev_h (int): The height of the bird's-eye-view map.
+        bev_w (int): The width of the bird's-eye-view map.
+        embed_dims (int): The number of dimensions to use for the query and key vectors in transformer layers.
+        num_anchor (int): The number of anchor points.
+        det_layer_num (int): The number of layers in the transformer model.
+        group_id_list (list): A list of group IDs to use for grouping the classes.
+        pc_range: The range of the point cloud.
+        use_nonlinear_optimizer (bool): A boolean indicating whether to use a non-linear optimizer for training.
+        anchor_info_path (str): The path to the file containing the anchor information.
+        vehicle_id_list(list[int]): class id of vehicle class, used for filtering out non-vehicle objects
+    """
+    def __init__(self, *args, **kwargs):
+        super(MotionHeadTRTP, self).__init__(*args, **kwargs)
+
+    def forward_test_trt(self, 
+                         bev_embed, 
+                         track_query_embeddings, 
+                         track_boxes_1, 
+                         track_boxes_2,
+                         gravity_center, 
+                         yaw,
+                         sdc_embedding, 
+                         sdc_gravity_center, 
+                         sdc_yaw, 
+                         sdc_track_boxes_1,
+                         sdc_track_boxes_2, 
+                         lane_query,
+                         lane_query_pos):
+        """Test function"""
+        track_query = track_query_embeddings[None, None, ...]
+        track_query = torch.cat([track_query, 
+                                 sdc_embedding[None, None, None, :]], 
+                                 dim=2)
+        gravity_center = torch.cat([gravity_center, 
+                                   sdc_gravity_center], dim=0)
+        yaw = torch.cat([yaw, 
+                        sdc_yaw], dim=0)
+        track_boxes_1 = torch.cat([track_boxes_1, 
+                                   sdc_track_boxes_1], dim=0)
+        track_boxes_2 = torch.cat([track_boxes_2, 
+                                   sdc_track_boxes_2], dim=0)
+        
+        outputs_traj_scores,\
+        outputs_trajs,\
+        valid_traj_masks,\
+        inter_states,\
+        out_track_query,\
+        track_query_pos = self.forward_trt(bev_embed, 
+                                           track_query, 
+                                           lane_query, 
+                                           lane_query_pos, 
+                                           track_boxes_1, 
+                                           track_boxes_2, 
+                                           gravity_center,
+                                           yaw,)
+
+        scores, labels = track_boxes_1, track_boxes_2
+        track_scores = scores[None, :]
+        labels[-1] = 0
+        
+        inter_states, \
+        out_track_query,\
+        track_query_pos,\
+        track_scores = self.filter_vehicle_query_trt(
+            inter_states, 
+            out_track_query,
+            track_query_pos,
+            track_scores, 
+            labels, 
+            self.vehicle_id_list)
+        
+        # filter sdc query
+        sdc_traj_query = inter_states[:, :, -1]
+        sdc_track_query = out_track_query[:, -1]
+        sdc_track_query_pos = track_query_pos[:, -1]
+        inter_states = inter_states[:, :, :-1]
+        out_track_query = out_track_query[:, :-1]
+        track_query_pos = track_query_pos[:, :-1]
+        track_scores = track_scores[:, :-1]
+
+        return (
+            outputs_traj_scores,
+            outputs_trajs,
+            valid_traj_masks,
+            inter_states,
+            out_track_query,
+            track_query_pos,
+            sdc_traj_query,
+            sdc_track_query,
+            sdc_track_query_pos,
+            track_scores,
+        )
+    
+    def filter_vehicle_query_trt(self, 
+                                 inter_states, 
+                                 out_track_query,
+                                 track_query_pos,
+                                 track_scores,
+                                 labels, 
+                                 vehicle_id_list):
+            vehicle_mask = torch.zeros_like(labels)
+            for veh_id in vehicle_id_list:
+                vehicle_mask =  (vehicle_mask + (labels == veh_id).float()).bool().int()
+            inter_states = inter_states[:, :, vehicle_mask>0]
+            out_track_query = out_track_query[:, vehicle_mask>0]
+            track_query_pos = track_query_pos[:, vehicle_mask>0]
+            track_scores = track_scores[:, vehicle_mask>0]
+            return (inter_states, 
+                    out_track_query,
+                    track_query_pos,
+                    track_scores,)
diff --git a/projects/mmdet3d_plugin/uniad/dense_heads/motion_head_plugin/base_motion_head.py b/projects/mmdet3d_plugin/uniad/dense_heads/motion_head_plugin/base_motion_head.py
index 18f4803..0754e71 100644
--- a/projects/mmdet3d_plugin/uniad/dense_heads/motion_head_plugin/base_motion_head.py
+++ b/projects/mmdet3d_plugin/uniad/dense_heads/motion_head_plugin/base_motion_head.py
@@ -11,6 +11,16 @@ import torch.nn as nn
 from mmdet.models import  build_loss
 from mmcv.cnn.bricks.transformer import build_transformer_layer_sequence
 
+class View(nn.Module):
+    def __init__(self, dim, shape):
+        super(View, self).__init__()
+        self.dim = dim
+        self.shape = shape
+
+    def forward(self, input):
+        new_shape = list(input.shape)[:self.dim] + list(self.shape) + list(input.shape)[self.dim+1:]
+        return input.view(*new_shape)
+
 class BaseMotionHead(nn.Module):
     def __init__(self, *args, **kwargs):
         super(BaseMotionHead, self).__init__()
@@ -27,6 +37,7 @@ class BaseMotionHead(nn.Module):
             None
         """
         self.loss_traj = build_loss(loss_traj)
+        nn.Unflatten = View
         self.unflatten_traj = nn.Unflatten(3, (self.predict_steps, 5))
         self.log_softmax = nn.LogSoftmax(dim=2)
 
diff --git a/projects/mmdet3d_plugin/uniad/dense_heads/motion_head_plugin/modules.py b/projects/mmdet3d_plugin/uniad/dense_heads/motion_head_plugin/modules.py
index f576335..77da7ef 100644
--- a/projects/mmdet3d_plugin/uniad/dense_heads/motion_head_plugin/modules.py
+++ b/projects/mmdet3d_plugin/uniad/dense_heads/motion_head_plugin/modules.py
@@ -12,8 +12,10 @@ from mmcv.runner.base_module import BaseModule
 from projects.mmdet3d_plugin.models.utils.functional import (
     norm_points,
     pos2posemb2d,
-    trajectory_coordinate_transform
+    trajectory_coordinate_transform,
+    rot_2d
 )
+from einops import rearrange
 
 
 @TRANSFORMER_LAYER_SEQUENCE.register_module()
@@ -170,7 +172,308 @@ class MotionTransformerDecoder(BaseModule):
                 intermediate_reference_trajs.append(reference_trajs)
 
         return torch.stack(intermediate), torch.stack(intermediate_reference_trajs)
+    
 
+@TRANSFORMER_LAYER_SEQUENCE.register_module()
+class MotionTransformerDecoderTRT(MotionTransformerDecoder):
+    """Implements the decoder in DETR3D transformer.
+    Args:
+        return_intermediate (bool): Whether to return intermediate outputs.
+        coder_norm_cfg (dict): Config of last normalization layer. Default：
+            `LN`.
+    """
+
+    def __init__(self, *args, **kwargs):
+        super().__init__(*args, **kwargs)
+
+    def forward_trt(self,
+                track_query,
+                lane_query,
+                track_query_pos=None,
+                lane_query_pos=None,
+                track_bbox_results=None,
+                bev_embed=None,
+                reference_trajs=None,
+                traj_reg_branches=None,
+                agent_level_embedding=None,
+                scene_level_ego_embedding=None,
+                scene_level_offset_embedding=None,
+                learnable_embed=None,
+                agent_level_embedding_layer=None,
+                scene_level_ego_embedding_layer=None,
+                scene_level_offset_embedding_layer=None,
+                **kwargs):
+        """Forward function for `MotionTransformerDecoder`.
+        Args:
+            agent_query (B, A, D)
+            map_query (B, M, D) 
+            map_query_pos (B, G, D)
+            static_intention_embed (B, A, P, D)
+            offset_query_embed (B, A, P, D)
+            global_intention_embed (B, A, P, D)
+            learnable_intention_embed (B, A, P, D)
+            det_query_pos (B, A, D)
+        Returns:
+            None
+        """
+        intermediate = []
+        intermediate_reference_trajs = []
+
+        B, _, P, D = agent_level_embedding.shape
+        track_query_bc = track_query[:,:,None,...].expand(-1, -1, P, -1)  # (B, A, P, D)
+        track_query_pos_bc = track_query_pos[:,:,None, ...].expand(-1, -1, P, -1)  # (B, A, P, D)
+
+        # static intention embedding, which is imutable throughout all layers
+        agent_level_embedding = self.intention_interaction_layers(agent_level_embedding)
+        static_intention_embed = agent_level_embedding + scene_level_offset_embedding + learnable_embed
+        reference_trajs_input = reference_trajs[:,:,:,:,None,...].detach()
+
+        query_embed = torch.zeros_like(static_intention_embed)
+        for lid in range(self.num_layers):
+            # fuse static and dynamic intention embedding
+            # the dynamic intention embedding is the output of the previous layer, which is initialized with anchor embedding
+            dynamic_query_embed = self.dynamic_embed_fuser(torch.cat(
+                [agent_level_embedding, scene_level_offset_embedding, scene_level_ego_embedding], dim=-1))
+            
+            # fuse static and dynamic intention embedding
+            query_embed_intention = self.static_dynamic_fuser(torch.cat(
+                [static_intention_embed, dynamic_query_embed], dim=-1))  # (B, A, P, D)
+            
+            # fuse intention embedding with query embedding
+            query_embed = self.in_query_fuser(torch.cat([query_embed, query_embed_intention], dim=-1))
+            
+            # interaction between agents
+            track_query_embed = self.track_agent_interaction_layers[lid](
+                query_embed, track_query, query_pos=track_query_pos_bc, key_pos=track_query_pos)
+            
+            # interaction between agents and map
+            map_query_embed = self.map_interaction_layers[lid](
+                query_embed, lane_query, query_pos=track_query_pos_bc, key_pos=lane_query_pos)
+            
+            # interaction between agents and bev, ie. interaction between agents and goals
+            # implemented with deformable transformer
+            bev_query_embed = self.bev_interaction_layers[lid].forward_trt(
+                query_embed,
+                value=bev_embed,
+                query_pos=track_query_pos_bc,
+                bbox_results=track_bbox_results,
+                reference_trajs=reference_trajs_input,
+                **kwargs)
+            
+            # fusing the embeddings from different interaction layers
+            query_embed = [track_query_embed, map_query_embed, bev_query_embed, track_query_bc+track_query_pos_bc]
+            query_embed = torch.cat(query_embed, dim=-1)
+            query_embed = self.out_query_fuser(query_embed)
+
+            if traj_reg_branches is not None:
+                # update reference trajectory
+                tmp = traj_reg_branches[lid](query_embed)
+                bs, n_agent, n_modes, n_steps, _ = reference_trajs.shape
+                last_dim = tmp.numel()//(bs*n_agent*n_modes*n_steps)
+                tmp = tmp.view(bs, n_agent, n_modes, n_steps, last_dim)
+                
+                # we predict speed of trajectory and use cumsum trick to get the trajectory
+                tmp[..., :2] = torch.cumsum(tmp[..., :2], dim=3)
+                new_reference_trajs = torch.zeros_like(reference_trajs)
+                new_reference_trajs = tmp[..., :2]
+                reference_trajs = new_reference_trajs.detach()
+                reference_trajs_input = reference_trajs[:,:,:,:,None,...]# BS NUM_AGENT NUM_MODE 12 NUM_LEVEL  2
+
+                # update embedding, which is used in the next layer
+                # only update the embedding of the last step, i.e. the goal
+                ep_offset_embed = reference_trajs.detach()
+                ep_ego_embed = self.trajectory_coordinate_transform_trt(
+                    reference_trajs[:,:,None, ...], 
+                    track_bbox_results, 
+                    with_translation_transform=True, 
+                    with_rotation_transform=False)[:,:,0, ...].detach()
+                ep_agent_embed = self.trajectory_coordinate_transform_trt(
+                    reference_trajs[:,:,None, ...], 
+                    track_bbox_results, 
+                    with_translation_transform=False, 
+                    with_rotation_transform=True)[:,:,0, ...].detach()
+
+                agent_level_embedding = agent_level_embedding_layer(pos2posemb2d(
+                    norm_points(ep_agent_embed[..., -1, :], self.pc_range)))
+                scene_level_ego_embedding = scene_level_ego_embedding_layer(pos2posemb2d(
+                    norm_points(ep_ego_embed[..., -1, :], self.pc_range)))
+                scene_level_offset_embedding = scene_level_offset_embedding_layer(pos2posemb2d(
+                    norm_points(ep_offset_embed[..., -1, :], self.pc_range)))
+
+                intermediate.append(query_embed)
+                intermediate_reference_trajs.append(reference_trajs)
+
+        return torch.stack(intermediate), torch.stack(intermediate_reference_trajs)
+    
+    def trajectory_coordinate_transform_trt(self, trajectory, yaw, gravity_center, with_translation_transform=True, with_rotation_transform=True):
+        """
+        Transform trajectory coordinates with respect to detected bounding boxes in the batch.
+        Args:
+            trajectory (torch.Tensor): predicted trajectory.
+            bbox_results (List[Tuple[torch.Tensor]]): A list of tuples containing the bounding box results for each image in the batch.
+            with_translate (bool, optional): Whether to perform translation transformation. Defaults to True.
+            with_rot (bool, optional): Whether to perform rotation transformation. Defaults to True.
+
+        Returns:
+            torch.Tensor: A tensor containing the transformed trajectory coordinates.
+        """
+        batch_size = 1
+        batched_trajectories = []
+        for i in range(batch_size):
+            yaw = yaw.to(trajectory.device)
+            bbox_centers = gravity_center.to(trajectory.device)
+            transformed_trajectory = trajectory[i,...]
+            if with_rotation_transform:
+                # we take negtive here, to reverse the trajectory back to ego centric coordinate
+                angle = -(yaw - 3.1415953) 
+                rot_yaw = rot_2d(angle)
+                rot_yaw = rot_yaw[:,None, None,:, :] # A, 1, 1, 2, 2
+                transformed_trajectory = rearrange(transformed_trajectory, 'a g p t c -> a g p c t') # A, G, P, 12 ,2 -> # A, G, P, 2, 12
+                transformed_trajectory = torch.matmul(rot_yaw, transformed_trajectory)# -> A, G, P, 12, 2
+                transformed_trajectory = rearrange(transformed_trajectory, 'a g p c t -> a g p t c')
+            if with_translation_transform:
+                transformed_trajectory = bbox_centers[:, None, None, None, :2] + transformed_trajectory
+            batched_trajectories.append(transformed_trajectory)
+        return torch.stack(batched_trajectories)
+
+
+@TRANSFORMER_LAYER_SEQUENCE.register_module()
+class MotionTransformerDecoderTRTP(MotionTransformerDecoderTRT):
+    """Implements the decoder in DETR3D transformer.
+    Args:
+        return_intermediate (bool): Whether to return intermediate outputs.
+        coder_norm_cfg (dict): Config of last normalization layer. Default：
+            `LN`.
+    """
+
+    def __init__(self, *args, **kwargs):
+        super().__init__(*args, **kwargs)
+
+    def forward_trt(self,
+                track_query,
+                lane_query,
+                track_query_pos=None,
+                lane_query_pos=None,
+                track_boxes_1=None, 
+                track_boxes_2=None, 
+                gravity_center=None,
+                yaw=None,
+                bev_embed=None,
+                reference_trajs=None,
+                traj_reg_branches=None,
+                agent_level_embedding=None,
+                scene_level_ego_embedding=None,
+                scene_level_offset_embedding=None,
+                learnable_embed=None,
+                agent_level_embedding_layer=None,
+                scene_level_ego_embedding_layer=None,
+                scene_level_offset_embedding_layer=None,
+                **kwargs):
+        """Forward function for `MotionTransformerDecoder`.
+        Args:
+            agent_query (B, A, D)
+            map_query (B, M, D) 
+            map_query_pos (B, G, D)
+            static_intention_embed (B, A, P, D)
+            offset_query_embed (B, A, P, D)
+            global_intention_embed (B, A, P, D)
+            learnable_intention_embed (B, A, P, D)
+            det_query_pos (B, A, D)
+        Returns:
+            None
+        """
+        intermediate = []
+        intermediate_reference_trajs = []
+
+        _, _, P, _ = agent_level_embedding.shape
+        track_query_bc = track_query[:,:,None, ...].expand(-1, -1, P, -1)  # (B, A, P, D)
+        track_query_pos_bc = track_query_pos[:,:,None, ...].expand(-1, -1, P, -1)  # (B, A, P, D)
+
+        # static intention embedding, which is imutable throughout all layers
+        agent_level_embedding = self.intention_interaction_layers(agent_level_embedding)
+        static_intention_embed = agent_level_embedding + scene_level_offset_embedding + learnable_embed
+        reference_trajs_input = reference_trajs[:,:,:,:,None, ...].detach()
+
+        query_embed = torch.zeros_like(static_intention_embed)
+        for lid in range(self.num_layers):
+            # fuse static and dynamic intention embedding
+            # the dynamic intention embedding is the output of the previous layer, which is initialized with anchor embedding
+            dynamic_query_embed = self.dynamic_embed_fuser(torch.cat(
+                [agent_level_embedding, scene_level_offset_embedding, scene_level_ego_embedding], dim=-1))
+            
+            # fuse static and dynamic intention embedding
+            query_embed_intention = self.static_dynamic_fuser(torch.cat(
+                [static_intention_embed, dynamic_query_embed], dim=-1))  # (B, A, P, D)
+            
+            # fuse intention embedding with query embedding
+            query_embed = self.in_query_fuser(torch.cat([query_embed, query_embed_intention], dim=-1))
+            
+            # interaction between agents
+            track_query_embed = self.track_agent_interaction_layers[lid](
+                query_embed, track_query, query_pos=track_query_pos_bc, key_pos=track_query_pos)
+            
+            # interaction between agents and map
+            map_query_embed = self.map_interaction_layers[lid](
+                query_embed, lane_query, query_pos=track_query_pos_bc, key_pos=lane_query_pos)
+            
+            # interaction between agents and bev, ie. interaction between agents and goals
+            # implemented with deformable transformer
+            bev_query_embed = self.bev_interaction_layers[lid].forward_trt(
+                query_embed,
+                value=bev_embed,
+                query_pos=track_query_pos_bc,
+                track_boxes_1=track_boxes_1, 
+                track_boxes_2=track_boxes_2, 
+                gravity_center=gravity_center,
+                yaw=yaw,
+                reference_trajs=reference_trajs_input,
+                **kwargs)
+            
+            query_embed = [track_query_embed, map_query_embed, bev_query_embed, track_query_bc+track_query_pos_bc]
+            query_embed = torch.cat(query_embed, dim=-1)
+            query_embed = self.out_query_fuser(query_embed)
+            if traj_reg_branches is not None:
+                # update reference trajectory
+                tmp = traj_reg_branches[lid](query_embed)
+                bs, n_agent, n_modes, n_steps, _ = reference_trajs.shape
+                last_dim = tmp.numel()//(bs*n_agent*n_modes*n_steps)
+                tmp = tmp.view(bs, n_agent, n_modes, n_steps, last_dim)
+                
+                # we predict speed of trajectory and use cumsum trick to get the trajectory
+                tmp[..., :2] = torch.cumsum(tmp[..., :2], dim=3)
+                new_reference_trajs = torch.zeros_like(reference_trajs)
+                new_reference_trajs = tmp[..., :2]
+                reference_trajs = new_reference_trajs.detach()
+                reference_trajs_input = reference_trajs[:,:,:,:,None,...]  # BS NUM_AGENT NUM_MODE 12 NUM_LEVEL  2
+
+                # update embedding, which is used in the next layer
+                # only update the embedding of the last step, i.e. the goal
+                ep_offset_embed = reference_trajs.detach()
+                ep_ego_embed = self.trajectory_coordinate_transform_trt(
+                    reference_trajs[:,:,None,...],
+                    yaw, 
+                    gravity_center,
+                    with_translation_transform=True, 
+                    with_rotation_transform=False)[:,:,0,...].detach()
+                ep_agent_embed = self.trajectory_coordinate_transform_trt(
+                    reference_trajs[:,:,None,...],
+                    yaw, 
+                    gravity_center,
+                    with_translation_transform=False, 
+                    with_rotation_transform=True)[:,:,0,...].detach()
+
+                agent_level_embedding = agent_level_embedding_layer(pos2posemb2d(
+                    norm_points(ep_agent_embed[..., -1, :], self.pc_range)))
+                scene_level_ego_embedding = scene_level_ego_embedding_layer(pos2posemb2d(
+                    norm_points(ep_ego_embed[..., -1, :], self.pc_range)))
+                scene_level_offset_embedding = scene_level_offset_embedding_layer(pos2posemb2d(
+                    norm_points(ep_offset_embed[..., -1, :], self.pc_range)))
+
+                intermediate.append(query_embed)
+                intermediate_reference_trajs.append(reference_trajs)
+
+        return torch.stack(intermediate), torch.stack(intermediate_reference_trajs)
+    
 
 class TrackAgentInteraction(BaseModule):
     """
diff --git a/projects/mmdet3d_plugin/uniad/dense_heads/motion_head_plugin/motion_deformable_attn.py b/projects/mmdet3d_plugin/uniad/dense_heads/motion_head_plugin/motion_deformable_attn.py
index 9aaee7e..04a520e 100644
--- a/projects/mmdet3d_plugin/uniad/dense_heads/motion_head_plugin/motion_deformable_attn.py
+++ b/projects/mmdet3d_plugin/uniad/dense_heads/motion_head_plugin/motion_deformable_attn.py
@@ -19,6 +19,9 @@ from mmcv.cnn.bricks.drop import build_dropout
 from mmcv.runner.base_module import BaseModule, ModuleList, Sequential
 from mmcv.utils import ConfigDict, deprecated_api_warning
 from projects.mmdet3d_plugin.uniad.modules.multi_scale_deformable_attn_function import MultiScaleDeformableAttnFunction_fp32
+import sys
+sys.path.insert(1, './projects/mmdet3d_plugin/uniad/functions')
+from multi_scale_deformable_attn import multi_scale_deformable_attn
 
 
 @TRANSFORMER_LAYER.register_module()
@@ -239,6 +242,271 @@ class MotionTransformerAttentionLayer(BaseModule):
 
         return query
 
+
+@TRANSFORMER_LAYER.register_module()
+class MotionTransformerAttentionLayerTRT(MotionTransformerAttentionLayer):
+    """Base `TransformerLayer` for vision transformer.
+    It can be built from `mmcv.ConfigDict` and support more flexible
+    customization, for example, using any number of `FFN or LN ` and
+    use different kinds of `attention` by specifying a list of `ConfigDict`
+    named `attn_cfgs`. It is worth mentioning that it supports `prenorm`
+    when you specifying `norm` as the first element of `operation_order`.
+    More details about the `prenorm`: `On Layer Normalization in the
+    Transformer Architecture <https://arxiv.org/abs/2002.04745>`_ .
+    Args:
+        attn_cfgs (list[`mmcv.ConfigDict`] | obj:`mmcv.ConfigDict` | None )):
+            Configs for `self_attention` or `cross_attention` modules,
+            The order of the configs in the list should be consistent with
+            corresponding attentions in operation_order.
+            If it is a dict, all of the attention modules in operation_order
+            will be built with this config. Default: None.
+        ffn_cfgs (list[`mmcv.ConfigDict`] | obj:`mmcv.ConfigDict` | None )):
+            Configs for FFN, The order of the configs in the list should be
+            consistent with corresponding ffn in operation_order.
+            If it is a dict, all of the attention modules in operation_order
+            will be built with this config.
+        operation_order (tuple[str]): The execution order of operation
+            in transformer. Such as ('self_attn', 'norm', 'ffn', 'norm').
+            Support `prenorm` when you specifying first element as `norm`.
+            Default：None.
+        norm_cfg (dict): Config dict for normalization layer.
+            Default: dict(type='LN').
+        init_cfg (obj:`mmcv.ConfigDict`): The Config for initialization.
+            Default: None.
+        batch_first (bool): Key, Query and Value are shape
+            of (batch, n, embed_dim)
+            or (n, batch, embed_dim). Default to False.
+    """
+
+    def __init__(self,
+                 *args,
+                 **kwargs):
+
+        super().__init__(*args,
+                 **kwargs)
+
+    def forward_trt(self,
+                query,
+                key=None,
+                value=None,
+                query_pos=None,
+                key_pos=None,
+                attn_masks=None,
+                query_key_padding_mask=None,
+                key_padding_mask=None,
+                **kwargs):
+        """Forward function for `TransformerDecoderLayer`.
+        **kwargs contains some specific arguments of attentions.
+        Args:
+            query (Tensor): The input query with shape
+                [num_queries, bs, embed_dims] if
+                self.batch_first is False, else
+                [bs, num_queries embed_dims].
+            key (Tensor): The key tensor with shape [num_keys, bs,
+                embed_dims] if self.batch_first is False, else
+                [bs, num_keys, embed_dims] .
+            value (Tensor): The value tensor with same shape as `key`.
+            query_pos (Tensor): The positional encoding for `query`.
+                Default: None.
+            key_pos (Tensor): The positional encoding for `key`.
+                Default: None.
+            attn_masks (List[Tensor] | None): 2D Tensor used in
+                calculation of corresponding attention. The length of
+                it should equal to the number of `attention` in
+                `operation_order`. Default: None.
+            query_key_padding_mask (Tensor): ByteTensor for `query`, with
+                shape [bs, num_queries]. Only used in `self_attn` layer.
+                Defaults to None.
+            key_padding_mask (Tensor): ByteTensor for `query`, with
+                shape [bs, num_keys]. Default: None.
+        Returns:
+            Tensor: forwarded results with shape [num_queries, bs, embed_dims].
+        """
+
+        norm_index = 0
+        attn_index = 0
+        ffn_index = 0
+        identity = query
+        if attn_masks is None:
+            attn_masks = [None for _ in range(self.num_attn)]
+        elif isinstance(attn_masks, torch.Tensor):
+            attn_masks = [
+                copy.deepcopy(attn_masks) for _ in range(self.num_attn)
+            ]
+            warnings.warn(f'Use same attn_mask in all attentions in '
+                          f'{self.__class__.__name__} ')
+        else:
+            assert len(attn_masks) == self.num_attn, f'The length of ' \
+                        f'attn_masks {len(attn_masks)} must be equal ' \
+                        f'to the number of attention in ' \
+                        f'operation_order {self.num_attn}'
+
+        for layer in self.operation_order:
+            if layer == 'self_attn':
+                temp_key = temp_value = query
+                query = self.attentions[attn_index].forward_trt(
+                    query,
+                    temp_key,
+                    temp_value,
+                    identity if self.pre_norm else None,
+                    query_pos=query_pos,
+                    key_pos=query_pos,
+                    attn_mask=attn_masks[attn_index],
+                    key_padding_mask=query_key_padding_mask,
+                    **kwargs)
+                attn_index += 1
+                identity = query
+
+            elif layer == 'norm':
+                query = self.norms[norm_index](query)
+                norm_index += 1
+
+            elif layer == 'cross_attn':
+                query = self.attentions[attn_index].forward_trt(
+                    query,
+                    key,
+                    value,
+                    identity if self.pre_norm else None,
+                    query_pos=query_pos,
+                    key_pos=key_pos,
+                    attn_mask=attn_masks[attn_index],
+                    key_padding_mask=key_padding_mask,
+                    **kwargs)
+                attn_index += 1
+                identity = query
+
+            elif layer == 'ffn':
+                query = self.ffns[ffn_index](
+                    query, identity if self.pre_norm else None)
+                ffn_index += 1
+
+        return query
+    
+
+@TRANSFORMER_LAYER.register_module()
+class MotionTransformerAttentionLayerTRTP(MotionTransformerAttentionLayerTRT):
+
+    def __init__(self,
+                 *args,
+                 **kwargs):
+
+        super(MotionTransformerAttentionLayerTRTP, self).__init__(*args,
+                 **kwargs)
+
+    def forward_trt(self,
+                query,
+                key=None,
+                value=None,
+                query_pos=None,
+                key_pos=None,
+                attn_masks=None,
+                query_key_padding_mask=None,
+                key_padding_mask=None,
+                track_boxes_1=None, 
+                track_boxes_2=None, 
+                gravity_center=None,
+                yaw=None,
+                reference_trajs=None,
+                **kwargs):
+        """Forward function for `TransformerDecoderLayer`.
+        **kwargs contains some specific arguments of attentions.
+        Args:
+            query (Tensor): The input query with shape
+                [num_queries, bs, embed_dims] if
+                self.batch_first is False, else
+                [bs, num_queries embed_dims].
+            key (Tensor): The key tensor with shape [num_keys, bs,
+                embed_dims] if self.batch_first is False, else
+                [bs, num_keys, embed_dims] .
+            value (Tensor): The value tensor with same shape as `key`.
+            query_pos (Tensor): The positional encoding for `query`.
+                Default: None.
+            key_pos (Tensor): The positional encoding for `key`.
+                Default: None.
+            attn_masks (List[Tensor] | None): 2D Tensor used in
+                calculation of corresponding attention. The length of
+                it should equal to the number of `attention` in
+                `operation_order`. Default: None.
+            query_key_padding_mask (Tensor): ByteTensor for `query`, with
+                shape [bs, num_queries]. Only used in `self_attn` layer.
+                Defaults to None.
+            key_padding_mask (Tensor): ByteTensor for `query`, with
+                shape [bs, num_keys]. Default: None.
+        Returns:
+            Tensor: forwarded results with shape [num_queries, bs, embed_dims].
+        """
+
+        norm_index = 0
+        attn_index = 0
+        ffn_index = 0
+        identity = query
+
+        if attn_masks is None:
+            attn_masks = [None for _ in range(self.num_attn)]
+        elif isinstance(attn_masks, torch.Tensor):
+            attn_masks = [
+                copy.deepcopy(attn_masks) for _ in range(self.num_attn)
+            ]
+            warnings.warn(f'Use same attn_mask in all attentions in '
+                          f'{self.__class__.__name__} ')
+        else:
+            assert len(attn_masks) == self.num_attn, f'The length of ' \
+                        f'attn_masks {len(attn_masks)} must be equal ' \
+                        f'to the number of attention in ' \
+                        f'operation_order {self.num_attn}'
+
+        for layer in self.operation_order:
+            if layer == 'self_attn':
+                temp_key = temp_value = query
+                query = self.attentions[attn_index].forward_trt(
+                    query,
+                    temp_key,
+                    temp_value,
+                    identity if self.pre_norm else None,
+                    query_pos=query_pos,
+                    key_pos=query_pos,
+                    attn_mask=attn_masks[attn_index],
+                    key_padding_mask=query_key_padding_mask,
+                    track_boxes_1=track_boxes_1, 
+                    track_boxes_2=track_boxes_2, 
+                    gravity_center=gravity_center,
+                    yaw=yaw,
+                    reference_trajs=reference_trajs,
+                    **kwargs)
+                attn_index += 1
+                identity = query
+
+            elif layer == 'norm':
+                query = self.norms[norm_index](query)
+                norm_index += 1
+
+            elif layer == 'cross_attn':
+                query = self.attentions[attn_index].forward_trt(
+                    query,
+                    key,
+                    value,
+                    identity if self.pre_norm else None,
+                    query_pos=query_pos,
+                    key_pos=key_pos,
+                    attn_mask=attn_masks[attn_index],
+                    key_padding_mask=key_padding_mask,
+                    track_boxes_1=track_boxes_1, 
+                    track_boxes_2=track_boxes_2, 
+                    gravity_center=gravity_center,
+                    yaw=yaw,
+                    reference_trajs=reference_trajs,
+                    **kwargs)
+                attn_index += 1
+                identity = query
+
+            elif layer == 'ffn':
+                query = self.ffns[ffn_index](
+                    query, identity if self.pre_norm else None)
+                ffn_index += 1
+
+        return query
+    
+
 @ATTENTION.register_module()
 class MotionDeformableAttention(BaseModule):
     """An attention module used in Deformable-Detr.
@@ -485,6 +753,286 @@ class MotionDeformableAttention(BaseModule):
         sy, cy = torch.sin(yaw), torch.cos(yaw)
         out = torch.stack([torch.stack([cy, -sy]), torch.stack([sy, cy])]).permute([2,0,1])
         return out
+    
+@ATTENTION.register_module()
+class MotionDeformableAttentionTRT(MotionDeformableAttention):
+    """An attention module used in Deformable-Detr.
+
+    `Deformable DETR: Deformable Transformers for End-to-End Object Detection.
+    <https://arxiv.org/pdf/2010.04159.pdf>`_.
+
+    Args:
+        embed_dims (int): The embedding dimension of Attention.
+            Default: 256.
+        num_heads (int): Parallel attention heads. Default: 64.
+        num_levels (int): The number of feature map used in
+            Attention. Default: 4.
+        num_points (int): The number of sampling points for
+            each query in each head. Default: 4.
+        im2col_step (int): The step used in image_to_column.
+            Default: 64.
+        dropout (float): A Dropout layer on `inp_identity`.
+            Default: 0.1.
+        batch_first (bool): Key, Query and Value are shape of
+            (batch, n, embed_dim)
+            or (n, batch, embed_dim). Default to False.
+        norm_cfg (dict): Config dict for normalization layer.
+            Default: None.
+        init_cfg (obj:`mmcv.ConfigDict`): The Config for initialization.
+            Default: None.
+    """
+
+    def __init__(self,
+                 *args,
+                 **kwargs):
+        super().__init__(*args,
+                 **kwargs)
+
+    @deprecated_api_warning({'residual': 'identity'},
+                            cls_name='MultiScaleDeformableAttention')
+    def forward_trt(self,
+                query,
+                key=None,
+                value=None,
+                identity=None,
+                query_pos=None,
+                key_padding_mask=None,
+                spatial_shapes=None,
+                level_start_index=None,
+                bbox_results=None,
+                reference_trajs=None,
+                flag='decoder',
+                **kwargs):
+        """Forward Function of MultiScaleDeformAttention.
+
+        Args:
+            query (Tensor): Query of Transformer with shape
+                (num_query, bs, embed_dims).
+            key (Tensor): The key tensor with shape
+                `(num_key, bs, embed_dims)`.
+            value (Tensor): The value tensor with shape
+                `(num_key, bs, embed_dims)`.
+            identity (Tensor): The tensor used for addition, with the
+                same shape as `query`. Default None. If None,
+                `query` will be used.
+            query_pos (Tensor): The positional encoding for `query`.
+                Default: None.
+            key_pos (Tensor): The positional encoding for `key`. Default
+                None.
+            reference_points (Tensor):  The normalized reference
+                points with shape (bs, num_query, num_levels, 2),
+                all elements is range in [0, 1], top-left (0,0),
+                bottom-right (1, 1), including padding area.
+                or (N, Length_{query}, num_levels, 4), add
+                additional two dimensions is (w, h) to
+                form reference boxes.
+            key_padding_mask (Tensor): ByteTensor for `query`, with
+                shape [bs, num_key].
+            spatial_shapes (Tensor): Spatial shape of features in
+                different levels. With shape (num_levels, 2),
+                last dimension represents (h, w).
+            level_start_index (Tensor): The start index of each level.
+                A tensor has shape ``(num_levels, )`` and can be represented
+                as [0, h_0*w_0, h_0*w_0+h_1*w_1, ...].
+
+        Returns:
+             Tensor: forwarded results with shape [num_query, bs, embed_dims].
+        """
+        bs, num_agent, num_mode, _ = query.shape
+        num_query = num_agent * num_mode
+        if value is None:
+            value = query
+        if identity is None:
+            identity = query
+        if query_pos is not None:
+            query = query + query_pos
+        query = torch.flatten(query, start_dim=1, end_dim=2)
+        
+        value = value.permute(1, 0, 2)
+        bs, num_value, _ = value.shape
+        assert (spatial_shapes[:, 0] * spatial_shapes[:, 1]).sum() == num_value
+
+        value = self.value_proj(value)
+        if key_padding_mask is not None:
+            value = value.masked_fill(key_padding_mask[..., None], 0.0)
+        value = value.view(bs, num_value, self.num_heads, -1)
+        sampling_offsets = self.sampling_offsets(query).view(
+            bs, num_query, self.num_heads, self.num_steps, self.num_levels, self.num_points, 2)
+        attention_weights = self.attention_weights(query).view(
+            bs, num_query, self.num_heads, self.num_steps, self.num_levels * self.num_points)
+        attention_weights = attention_weights.softmax(-1)
+
+        attention_weights = attention_weights.view(bs, num_query,
+                                                   self.num_heads,
+                                                   self.num_steps,
+                                                   self.num_levels,
+                                                   self.num_points)
+        # bs, n_query, n_head, n_steps, N_level, N_points, 2
+        # BS NUM_AGENT NUM_MODE 12 NUM_LEVEL  2
+        if reference_trajs.shape[-1] == 2:
+            reference_trajs = reference_trajs[:, :, :, [self.sample_index], :, :]
+            reference_trajs_ego = self.agent_coords_to_ego_coords_trt(reference_trajs.clone(), bbox_results).detach()
+            reference_trajs_ego = torch.flatten(reference_trajs_ego, start_dim=1, end_dim=2)
+            reference_trajs_ego = reference_trajs_ego[:, :, None, :, :, None, :]
+            reference_trajs_ego[..., 0] -= self.bev_range[0]
+            reference_trajs_ego[..., 1] -= self.bev_range[1]
+            reference_trajs_ego[..., 0] /= (self.bev_range[3] - self.bev_range[0])
+            reference_trajs_ego[..., 1] /= (self.bev_range[4] - self.bev_range[1])
+            offset_normalizer = torch.stack(
+                [spatial_shapes[..., 1], spatial_shapes[..., 0]], -1)
+            sampling_locations = reference_trajs_ego \
+                + sampling_offsets \
+                / offset_normalizer[None, None, None, None, :, None, :]
+
+            sampling_locations = rearrange(sampling_locations, 'bs nq nh ns nl np c -> bs nq ns nh nl np c') # permute([0,1,3,2,4,5,6])
+            attention_weights = rearrange(attention_weights, 'bs nq nh ns nl np -> bs nq ns nh nl np') #.permute([0,1,3,2,4,5])
+            sampling_locations = sampling_locations.reshape(bs, num_query*self.num_steps, self.num_heads, self.num_levels, self.num_points, 2)
+            attention_weights = attention_weights.reshape(bs, num_query*self.num_steps, self.num_heads, self.num_levels, self.num_points)
+
+        else:
+            raise ValueError(
+                f'Last dim of reference_trajs must be'
+                f' 2 or 4, but get {reference_trajs.shape[-1]} instead.')
+        if torch.cuda.is_available() and value.is_cuda and\
+            not torch.onnx.is_in_onnx_export():
+
+            # using fp16 deformable attention is unstable because it performs many sum operations
+            if value.dtype == torch.float16:
+                MultiScaleDeformableAttnFunction = MultiScaleDeformableAttnFunction_fp32
+            else:
+                MultiScaleDeformableAttnFunction = MultiScaleDeformableAttnFunction_fp32
+            output = MultiScaleDeformableAttnFunction.apply(
+                value, spatial_shapes, level_start_index, sampling_locations,
+                attention_weights, self.im2col_step)
+        else:
+            output = multi_scale_deformable_attn_pytorch(
+                value, spatial_shapes, sampling_locations, attention_weights)
+        output = output.view(bs, num_query, self.num_steps, -1)
+        output = torch.flatten(output, start_dim=2, end_dim=3)
+        output = self.output_proj(output)
+        output = output.view(bs, num_agent, num_mode, -1)
+        return self.dropout(output) + identity
+
+    def agent_coords_to_ego_coords_trt(self, reference_trajs, bbox_results):
+        batch_size = 1
+        reference_trajs_ego = []
+        for i in range(batch_size):
+            det_centers = bbox_results[4].to(reference_trajs.device)
+            reference_trajs_ego.append(reference_trajs[i]+det_centers[:, None, None, None, :2])
+        return torch.stack(reference_trajs_ego)
+    
+    def rot_2d(self, yaw):
+        sy, cy = torch.sin(yaw), torch.cos(yaw)
+        out = torch.stack([torch.stack([cy, -sy]), torch.stack([sy, cy])]).permute([2,0,1])
+        return out
+
+
+@ATTENTION.register_module()
+class MotionDeformableAttentionTRTP(MotionDeformableAttentionTRT):
+
+    def __init__(self,
+                 *args,
+                 **kwargs):
+        super(MotionDeformableAttentionTRTP, self).__init__(*args,
+                 **kwargs)
+        self.multi_scale_deformable_attn = multi_scale_deformable_attn
+
+    @deprecated_api_warning({'residual': 'identity'},
+                            cls_name='MultiScaleDeformableAttention')
+    def forward_trt(self,
+                query,
+                key=None,
+                value=None,
+                identity=None,
+                query_pos=None,
+                key_padding_mask=None,
+                spatial_shapes=None,
+                level_start_index=None,
+                track_boxes_1=None, 
+                track_boxes_2=None, 
+                gravity_center=None,
+                yaw=None,
+                reference_trajs=None,
+                flag='decoder',
+                **kwargs):
+        bs, num_agent, num_mode, _ = query.shape
+        num_query = num_agent * num_mode
+        if value is None:
+            value = query
+        if identity is None:
+            identity = query
+        if query_pos is not None:
+            query = query + query_pos
+        query = torch.flatten(query, start_dim=1, end_dim=2)
+        
+        value = value.permute(1, 0, 2)
+        bs, num_value, _ = value.shape
+        assert (spatial_shapes[:, 0] * spatial_shapes[:, 1]).sum() == num_value
+
+        value = self.value_proj(value)
+        if key_padding_mask is not None:
+            value = value.masked_fill(key_padding_mask[..., None], 0.0)
+        value = value.view(bs, num_value, self.num_heads, -1)
+
+        sampling_offsets = self.sampling_offsets(query).view(
+            bs, num_query, self.num_heads, self.num_steps, self.num_levels, self.num_points, 2)
+        attention_weights = self.attention_weights(query).view(
+            bs, num_query, self.num_heads, self.num_steps, self.num_levels, self.num_points)
+        attention_weights = rearrange(attention_weights, 'bs nq nh ns nl np -> bs nq ns nh nl np')
+        attention_weights = attention_weights.reshape(bs, num_query*self.num_steps, self.num_heads, self.num_levels, self.num_points)
+        attention_weights = attention_weights.view(
+            *attention_weights.shape[:2], self.num_heads, -1
+        )
+        # bs, n_query, n_head, n_steps, N_level, N_points, 2
+        # BS NUM_AGENT NUM_MODE 12 NUM_LEVEL  2
+        if reference_trajs.shape[-1] == 2:
+            reference_trajs = reference_trajs[:, :, :, [self.sample_index], :, :]
+            reference_trajs_ego = self.agent_coords_to_ego_coords_trt(reference_trajs, gravity_center).detach()
+            reference_trajs_ego = torch.flatten(reference_trajs_ego, start_dim=1, end_dim=2)
+            reference_trajs_ego = reference_trajs_ego[:, :, None, :, :, None, :]
+            reference_trajs_ego[..., 0] = reference_trajs_ego[..., 0] - self.bev_range[0]
+            reference_trajs_ego[..., 1] = reference_trajs_ego[..., 1] - self.bev_range[1]
+            reference_trajs_ego[..., 0] = reference_trajs_ego[..., 0] / (self.bev_range[3] - self.bev_range[0])
+            reference_trajs_ego[..., 1] = reference_trajs_ego[..., 1] / (self.bev_range[4] - self.bev_range[1])
+        else:
+            raise ValueError(
+                f'Last dim of reference_trajs must be'
+                f' 2 or 4, but get {reference_trajs.shape[-1]} instead.')
+
+        sampling_offsets = rearrange(sampling_offsets, 'bs nq nh ns nl np c-> bs nq ns nh nl np c')
+        sampling_offsets = sampling_offsets.reshape(bs, num_query*self.num_steps, self.num_heads, self.num_levels, self.num_points, 2)
+        sampling_offsets = sampling_offsets.view(
+            *sampling_offsets.shape[:2], self.num_heads, -1
+        )
+        output = self.multi_scale_deformable_attn(
+                    value, #torch.Size([1, 40000, 8, 32])
+                    spatial_shapes, 
+                    reference_trajs_ego.expand(-1, -1, -1, 12,
+                                                -1, -1, -1).reshape(1, num_query*self.num_steps, 
+                                                                    self.num_levels, 2),
+                    sampling_offsets, #torch.Size([1, 360, 8, 8])
+                    attention_weights,#torch.Size([1, 360, 8, 4])
+                ).flatten(2)
+
+        output = output.view(bs, num_query, self.num_steps, -1)
+        output = torch.flatten(output, start_dim=2, end_dim=3)
+        output = self.output_proj(output)
+        output = output.view(bs, num_agent, num_mode, -1)
+        return self.dropout(output) + identity
+
+    def agent_coords_to_ego_coords_trt(self, reference_trajs, gravity_center):
+        reference_trajs_ego = []
+        det_centers = gravity_center[:, None, None, None, :2] 
+        ref = reference_trajs[0]
+        ref2 = ref + det_centers
+        reference_trajs_ego.append(ref2)
+        return torch.stack(reference_trajs_ego)
+    
+    def rot_2d(self, yaw):
+        sy, cy = torch.sin(yaw), torch.cos(yaw)
+        out = torch.stack([torch.stack([cy, -sy]), torch.stack([sy, cy])]).permute([2,0,1])
+        return out
+    
 
 @ATTENTION.register_module()
 class CustomModeMultiheadAttention(BaseModule):
@@ -582,8 +1130,8 @@ class CustomModeMultiheadAttention(BaseModule):
             if self.batch_first is False, else
             [bs, num_queries embed_dims].
         """
-        query_pos = query_pos.unsqueeze(1)
-        key_pos = key_pos.unsqueeze(1)
+        query_pos = query_pos[:,None,...]
+        key_pos = key_pos[:,None,...]
         bs, n_agent, n_query, D = query.shape
         if key is None:
             key = query
diff --git a/projects/mmdet3d_plugin/uniad/dense_heads/occ_head.py b/projects/mmdet3d_plugin/uniad/dense_heads/occ_head.py
index ffa5e21..43a352f 100644
--- a/projects/mmdet3d_plugin/uniad/dense_heads/occ_head.py
+++ b/projects/mmdet3d_plugin/uniad/dense_heads/occ_head.py
@@ -7,6 +7,7 @@
 import torch
 import torch.nn as nn
 import torch.nn.functional as F
+import torchvision
 from mmdet.models.builder import HEADS, build_loss
 from mmcv.runner import BaseModule
 from einops import rearrange
@@ -35,6 +36,7 @@ class OccHead(BaseModule):
                  bev_emb_dim=256,
                  bev_proj_dim=64,
                  bev_proj_nlayers=1,
+                 bevslicer=True,
 
                  # Query
                  query_dim=256,
@@ -75,6 +77,7 @@ class OccHead(BaseModule):
             'zbound': [-10.0, 10.0, 20.0],
         }
         self.bev_sampler =  BevFeatureSlicer(bevformer_bev_conf, grid_conf)
+        self.bevslicer = bevslicer
         
         self.bev_size = bev_size
         self.bev_proj_dim = bev_proj_dim
@@ -198,7 +201,8 @@ class OccHead(BaseModule):
     def forward(self, x, ins_query):
         base_state = rearrange(x, '(h w) b d -> b d h w', h=self.bev_size[0])
 
-        base_state = self.bev_sampler(base_state)
+        if self.bevslicer:
+            base_state = self.bev_sampler(base_state)
         base_state = self.bev_light_proj(base_state)
         base_state = self.base_downscale(base_state)
         base_ins_query = ins_query
@@ -474,3 +478,197 @@ class OccHead(BaseModule):
         gt_instance = gt_instance[:, :self.n_future+1].long()
         gt_img_is_valid = gt_img_is_valid[:, :self.receptive_field + self.n_future]
         return gt_segmentation, gt_instance, gt_img_is_valid
+
+
+def if_sum_greater_zero(query_index, attn_mask):
+    # Generate mask where query_index is 1
+    mask = query_index == 1
+    attn_mask = torch.cat((attn_mask, torch.zeros(
+        attn_mask.shape[0],
+        attn_mask.shape[1],
+        1,
+        ).to(attn_mask)), dim=-1)
+
+    mask_float = mask[...,None].float()
+    attn_mask = mask_float*0 + (1-mask_float)*attn_mask
+
+    return attn_mask[...,:-1]
+
+def if_no_query(pred_ins_sigmoid, pred_seg_scores):
+    pred_ins_sigmoid_pad = torch.cat((pred_ins_sigmoid, torch.zeros(
+                                    pred_ins_sigmoid.shape[0],
+                                    1,
+                                    pred_ins_sigmoid.shape[2],
+                                    pred_ins_sigmoid.shape[3],
+                                    pred_ins_sigmoid.shape[4],).to(pred_ins_sigmoid
+                                                                   )+pred_seg_scores), dim=1)
+    pred_seg_scores_out = pred_ins_sigmoid_pad.max(1)[0]
+
+    return pred_seg_scores_out
+
+@HEADS.register_module()
+class OccHeadTRT(OccHead):
+    def __init__(self, *args, **kwargs):
+        super().__init__(*args, **kwargs)
+
+    def get_ins_seg_gt_trt(self, ins_gt_old):
+        ins_gt_new= ins_gt_old.detach().clone()
+        ins_gt_new[ins_gt_new==self.ignore_index]=0
+        # speed up the for loop
+        # TODO: implement a custom version of torch.unique, tensorRT does not support torch.unique
+        _, ins_gt_new = torch.unique(ins_gt_new, sorted=True, return_inverse=True)
+        
+        return ins_gt_new  # Consecutive
+    
+    def get_attn_mask_trt(self, state, ins_query):
+        # state: b, c, h, w
+        # ins_query: b, q, c
+        ins_embed = self.temporal_mlp_for_mask(
+            ins_query 
+        )
+        mask_pred = torch.einsum("bqc,bchw->bqhw", ins_embed, state)
+        attn_mask = mask_pred.sigmoid() < self.attn_mask_thresh
+        attn_mask = attn_mask.float()
+        b,q,h,w = attn_mask.shape
+        attn_mask = attn_mask.view(b,q,h*w).permute(0,2,1)[:,None,...].repeat(
+            1, self.num_heads, 1, 1).flatten(0, 1).detach()
+        query_index = attn_mask.sum(-1) == attn_mask.shape[-1]
+        attn_mask = if_sum_greater_zero(query_index, attn_mask)
+
+        return attn_mask.bool(), ins_embed
+    
+    def forward_trt(self, x, ins_query):
+        _, b,d=x.shape
+        base_state = x.permute(1,2,0).view(b,d,self.bev_size[0],self.bev_size[1])
+        if self.bevslicer:
+            base_state = self.bev_sampler(base_state)
+        base_state = self.bev_light_proj(base_state)
+        base_state = self.base_downscale(base_state)
+        base_ins_query = ins_query
+
+        last_state = base_state
+        last_ins_query = base_ins_query
+        future_states = []
+        temporal_query = []
+        temporal_embed_for_mask_attn = []
+        n_trans_layer_each_block = self.num_trans_layers // self.n_future_blocks
+        assert n_trans_layer_each_block >= 1
+        
+        for i in range(self.n_future_blocks):
+            # Downscale
+            cur_state = self.downscale_convs[i](last_state)  # /4 -> /8
+
+            # Attention
+            cur_ins_query = self.temporal_mlps[i](last_ins_query)  # [b, q, d]
+            temporal_query.append(cur_ins_query)
+
+            # Generate attn mask 
+            attn_mask, cur_ins_emb_for_mask_attn = self.get_attn_mask_trt(cur_state, cur_ins_query)
+            attn_masks = [None, attn_mask] 
+
+            # mask_preds.append(mask_pred)  # /1
+            temporal_embed_for_mask_attn.append(cur_ins_emb_for_mask_attn)
+
+            b,c,h,w=cur_state.shape
+            cur_state = cur_state.view(b,c,h*w).permute(2,0,1)
+            cur_ins_query=cur_ins_query.permute(1,0,2)
+
+            for j in range(n_trans_layer_each_block):
+                trans_layer_ind = i * n_trans_layer_each_block + j
+                trans_layer = self.transformer_decoder.layers[trans_layer_ind]
+                cur_state = trans_layer(
+                    query=cur_state,  # [h'*w', b, c]
+                    key=cur_ins_query,  # [nq, b, c]
+                    value=cur_ins_query,  # [nq, b, c]
+                    query_pos=None,  
+                    key_pos=None,
+                    attn_masks=attn_masks,
+                    query_key_padding_mask=None,
+                    key_padding_mask=None
+                )  # out size: [h'*w', b, c]
+            cur_state_h = torch.tensor(cur_state.shape[0]**0.5, device=cur_state.device).int()
+            _, b, c = cur_state.shape
+            cur_state = cur_state.permute(1,2,0).view(b,c,cur_state_h,cur_state_h)
+            
+            # Upscale to /4
+            cur_state = self.upsample_adds[i](cur_state, last_state)
+
+            # Out
+            future_states.append(cur_state)  # [b, d, h/4, w/4]
+            last_state = cur_state
+
+        future_states = torch.stack(future_states, dim=1)  # [b, t, d, h/4, w/4]
+        temporal_query = torch.stack(temporal_query, dim=1)  # [b, t, q, d]
+        ins_query = torch.stack(temporal_embed_for_mask_attn, dim=1)  # [b, t, q, d]
+
+        # Decode future states to larger resolution
+        future_states = self.dense_decoder(future_states)
+        future_states = F.interpolate(
+                        future_states,
+                        (future_states.shape[-3], self.bev_size[-2], self.bev_size[-1]),
+                        mode='trilinear',
+                        align_corners=False
+                        )
+ 
+        ins_occ_query = self.query_to_occ_feat(ins_query)    # [b, t, q, query_out_dim]
+        
+        # Generate final outputs
+        ins_occ_logits = torch.einsum("btqc,btchw->bqthw", ins_occ_query, future_states)
+        
+        return ins_occ_logits
+
+    def merge_queries_trt(self, track_query, track_query_pos, ins_query):
+        track_query_pos = track_query_pos.detach()
+        ins_query = ins_query[-1]
+        ins_query = self.mode_fuser(ins_query).max(2)[0]
+        ins_query = self.multi_query_fuser(torch.cat([ins_query, track_query, track_query_pos], dim=-1))
+        
+        return ins_query
+
+    def forward_test_trt(
+                    self,
+                    bev_feat,#torch.Size([40000, 1, 256])
+                    track_query, #torch.Size([1, xx, 256])
+                    track_query_pos, #torch.Size([1, xx, 256])
+                    traj_query,#torch.Size([3, 1, xx, 6, 256])
+                    gt_segmentation=None,#torch.Size([1, 7, 200, 200]), int64
+                    track_scores=None,#torch.Size([1, xx])
+                ):
+        gt_segmentation = self.get_occ_labels_trt(gt_segmentation)
+        seg_gt  = gt_segmentation[:, :1+self.n_future]  # [1, 5, 1, 200, 200]
+        bs, track_query_shape1, track_query_shape2 = track_query.shape
+        ins_query = self.merge_queries_trt(track_query, track_query_pos, traj_query)
+        pred_ins_logits = self.forward_trt(bev_feat, ins_query=ins_query)
+        pred_ins_logits = pred_ins_logits[:,:,:1+self.n_future]  # [b, q, t, h, w]
+        pred_ins_sigmoid = pred_ins_logits.sigmoid()  # [b, q, t, h, w]
+
+        track_scores = track_scores.to(pred_ins_sigmoid)  # [b, q]
+        track_scores = track_scores[:, :, None, None, None]
+        pred_ins_sigmoid = pred_ins_sigmoid * track_scores  # [b, q, t, h, w]
+
+        pred_seg_scores = self.test_seg_thresh-1
+        pred_seg_scores2 = if_no_query(pred_ins_sigmoid, pred_seg_scores)
+        seg_out = (pred_seg_scores2 > self.test_seg_thresh).long()[:,:,None,...]  # [b, t, 1, h, w]
+        seg_out = (seg_out*(track_query_shape1!=0)).long() 
+
+        return seg_gt, seg_out
+
+    def get_occ_labels_trt(self, gt_segmentation):
+
+        gt_segmentation = gt_segmentation[:, :self.n_future+1].long()[:,:,None,...]
+
+        return gt_segmentation
+    
+    def post_process_trt(self, gt_instance,seg_out,pred_ins_sigmoid):
+        ins_seg_gt = self.get_ins_seg_gt_trt(gt_instance[:, :1+self.n_future])
+        pred_consistent_instance_seg =  \
+            predict_instance_segmentation_and_trajectories(seg_out, pred_ins_sigmoid)
+        return ins_seg_gt, pred_consistent_instance_seg
+
+@HEADS.register_module()
+class OccHeadTRTP(OccHeadTRT):
+    def __init__(self, 
+                 *args,
+                 **kwargs,
+                 ):
+        super(OccHeadTRTP, self).__init__(*args, **kwargs)
diff --git a/projects/mmdet3d_plugin/uniad/dense_heads/occ_head_plugin/modules.py b/projects/mmdet3d_plugin/uniad/dense_heads/occ_head_plugin/modules.py
index 2561f81..1014973 100644
--- a/projects/mmdet3d_plugin/uniad/dense_heads/occ_head_plugin/modules.py
+++ b/projects/mmdet3d_plugin/uniad/dense_heads/occ_head_plugin/modules.py
@@ -6,6 +6,7 @@
 
 import torch
 from torch import nn
+import torchvision
 import torch.utils.checkpoint as checkpoint
 from .utils import calculate_birds_eye_view_parameters
 import torch.nn.functional as F
@@ -200,8 +201,8 @@ class CVT_Decoder(BaseModule):
             self.init_cfg = dict(type='Kaiming', layer='Conv2d')
 
     def forward(self, x):
-        b, t = x.size(0), x.size(1)
-        x = rearrange(x, 'b t c h w -> (b t) c h w')
+        b, t, c, h, w=x.shape
+        x=x.view(b*t, c, h, w)
         y = x
         for layer in self.layers:
             if self.use_checkpoint:
@@ -209,7 +210,8 @@ class CVT_Decoder(BaseModule):
             else:
                 y = layer(y, x)
         
-        y = rearrange(y, '(b t) c h w -> b t c h w', b=b, t=t)
+        _, c, h, w = y.shape
+        y = y.view(b,t,c,h,w)
         return y
 
 
@@ -225,6 +227,12 @@ class UpsamplingAdd(nn.Module):
 
     def forward(self, x, x_skip):
         x = self.upsample_layer(x)
+        x = F.interpolate(
+                        x,
+                        (x_skip.shape[-2], x_skip.shape[-1]),
+                        mode='bilinear',
+                        align_corners=False
+                        )
         return x + x_skip
 
 class Interpolate(nn.Module):
diff --git a/projects/mmdet3d_plugin/uniad/dense_heads/occ_head_plugin/utils.py b/projects/mmdet3d_plugin/uniad/dense_heads/occ_head_plugin/utils.py
index da71cb4..ae67e24 100644
--- a/projects/mmdet3d_plugin/uniad/dense_heads/occ_head_plugin/utils.py
+++ b/projects/mmdet3d_plugin/uniad/dense_heads/occ_head_plugin/utils.py
@@ -67,6 +67,36 @@ def make_instance_seg_consecutive(instance_seg):
     instance_seg = update_instance_ids(instance_seg, unique_ids, new_ids)
     return instance_seg
 
+# def custom_unique_v2(x):
+#     x=x.flatten()
+#     unique_elements = torch.Tensor([]).to(x.device)
+#     seen = set()
+    
+#     for element in x:
+#         if element not in seen:
+#             seen.add(element)
+#             unique_elements = torch.cat((unique_elements, torch.Tensor([element]).to(x.device)))
+    
+#     return unique_elements.sort()[0]
+
+# def custom_unique_v1(instance_seg):
+#     instance_seg = instance_seg.flatten()
+#     sorted_tensor, _ = instance_seg.sort()
+#     # Find unique elements by comparing adjacent elements and removing duplicates
+#     unique_mask = sorted_tensor[:-1] != sorted_tensor[1:]
+#     unique_mask = torch.cat([unique_mask, torch.tensor([1], device=instance_seg.device)], dim=0).bool()
+#     # Gather the unique elements using the unique_mask
+#     unique_elements = sorted_tensor[unique_mask]
+
+#     return unique_elements
+
+
+# def make_instance_seg_consecutive_trt(instance_seg):
+#     unique_ids1 = custom_unique_v1(instance_seg)
+#     new_ids = torch.arange(len(unique_ids), device=instance_seg.device)
+#     instance_seg = update_instance_ids(instance_seg, unique_ids, new_ids)
+#     return instance_seg
+
 
 def predict_instance_segmentation_and_trajectories(
                                     foreground_masks,
@@ -85,3 +115,21 @@ def predict_instance_segmentation_and_trajectories(
     instance_seg = make_instance_seg_consecutive(instance_seg).long()
 
     return instance_seg
+
+# def predict_instance_segmentation_and_trajectories_trt(
+#                                     foreground_masks,
+#                                     ins_sigmoid,
+#                                     vehicles_id=1,
+#                                     ):
+#     if foreground_masks.dim() == 5 and foreground_masks.shape[2] == 1:
+#         foreground_masks = foreground_masks[:,:,0,...]  # [b, t, h, w]
+#     foreground_masks = foreground_masks == vehicles_id  # [b, t, h, w]  Only these places have foreground id
+    
+#     argmax_ins = ins_sigmoid.argmax(dim=1)  # long, [b, t, h, w], ins_id starts from 0
+#     argmax_ins = argmax_ins + 1 # [b, t, h, w], ins_id starts from 1
+#     instance_seg = (argmax_ins * foreground_masks.float()).long()  # bg is 0, fg starts with 1
+
+#     # Make the indices of instance_seg consecutive
+#     instance_seg = make_instance_seg_consecutive_trt(instance_seg).long()
+
+#     return instance_seg
\ No newline at end of file
diff --git a/projects/mmdet3d_plugin/uniad/dense_heads/panseg_head.py b/projects/mmdet3d_plugin/uniad/dense_heads/panseg_head.py
index 83363cf..d7335ac 100644
--- a/projects/mmdet3d_plugin/uniad/dense_heads/panseg_head.py
+++ b/projects/mmdet3d_plugin/uniad/dense_heads/panseg_head.py
@@ -1309,3 +1309,384 @@ class PansegformerHead(SegDETRHead):
                 'stuff_score_list' : stuff_score_list[i],
             })
         return results
+
+
+
+@HEADS.register_module()
+class PansegformerHeadTRT(PansegformerHead):
+    """
+    Head of Panoptic SegFormer
+
+    Code is modified from the `official github repo
+    <https://github.com/open-mmlab/mmdetection>`_.
+
+    Args:
+        with_box_refine (bool): Whether to refine the reference points
+            in the decoder. Defaults to False.
+        as_two_stage (bool) : Whether to generate the proposal from
+            the outputs of encoder.
+        transformer (obj:`ConfigDict`): ConfigDict is used for building
+            the Encoder and Decoder.
+    """
+
+    def __init__(self, *args, **kwargs):
+        super().__init__(*args, **kwargs)
+
+    def forward_trt(self, 
+                    bev_embed):
+        _, bs, _ = bev_embed.shape
+        mlvl_feats = [torch.reshape(bev_embed, (bs, self.bev_h, self.bev_w ,-1))\
+                      .permute(0, 3, 1, 2)]
+        img_masks = mlvl_feats[0].new_zeros((bs, self.bev_h, self.bev_w))
+
+        hw_lvl = [feat_lvl.shape[-2:] for feat_lvl in mlvl_feats]
+        mlvl_masks = []
+        mlvl_positional_encodings = []
+        for feat in mlvl_feats:
+            mlvl_masks.append(
+                F.interpolate(img_masks[None],
+                              size=feat.shape[-2:]).to(torch.bool)[0,...])
+            mlvl_positional_encodings.append(
+                self.positional_encoding(mlvl_masks[-1]))
+
+        query_embeds = None
+        query_embeds = self.query_embedding.weight
+        (memory, memory_pos, memory_mask, query_pos), \
+            hs, init_reference, inter_references, \
+        _, _ = self.transformer(
+            mlvl_feats,
+            mlvl_masks,
+            query_embeds,
+            mlvl_positional_encodings,
+            reg_branches=self.reg_branches if self.with_box_refine else None,  # noqa:E501
+            cls_branches=self.cls_branches if self.as_two_stage else None  # noqa:E501
+        )
+
+        memory = memory.permute(1, 0, 2)
+        query = hs[-1].permute(1, 0, 2)
+        query_pos = query_pos.permute(1, 0, 2)
+        memory_pos = memory_pos.permute(1, 0, 2)
+
+        hw_lvl0 = torch.tensor(hw_lvl[0])
+        hs = hs.permute(0, 2, 1, 3)
+        outputs_classes = []
+        outputs_coords = []
+
+        # for each for loop step, the if-else is actually static
+        for lvl in range(hs.shape[0]):
+            if lvl == 0:
+                reference = init_reference
+            else:
+                reference = inter_references[lvl - 1]
+            reference = inverse_sigmoid(reference)
+            outputs_class = self.cls_branches[lvl](hs[lvl])
+            tmp = self.reg_branches[lvl](hs[lvl])
+
+            if reference.shape[-1] == 4:
+                tmp = tmp + reference
+            else:
+                assert reference.shape[-1] == 2
+                tmp[..., :2] = tmp[..., :2] + reference
+            outputs_coord = tmp.sigmoid()
+            outputs_classes.append(outputs_class)
+            outputs_coords.append(outputs_coord)
+
+        outputs_classes = torch.stack(outputs_classes)
+        outputs_coords = torch.stack(outputs_coords)
+
+        return  outputs_classes,\
+                outputs_coords,\
+                memory, \
+                memory_mask, \
+                memory_pos, \
+                query, \
+                query_pos, \
+                hw_lvl0, \
+                reference
+
+    def _get_bboxes_single_trt(self,
+                           cls_score,
+                           bbox_pred,
+                           img_shape,
+                           scale_factor,
+                           rescale=True):
+        """
+        """
+        assert cls_score.shape[0] == bbox_pred.shape[0]
+        max_per_img = self.test_cfg.get('max_per_img', self.num_query)
+
+        cls_score = cls_score.sigmoid()
+        scores, indexes = cls_score.view(-1).topk(max_per_img)
+        det_labels = indexes % self.num_things_classes
+        bbox_index = indexes // self.num_things_classes
+        bbox_pred = bbox_pred[bbox_index]
+
+        det_bboxes = bbox_cxcywh_to_xyxy(bbox_pred)
+        det_bboxes[:, 0::2] = det_bboxes[:, 0::2] * img_shape[1]
+        det_bboxes[:, 1::2] = det_bboxes[:, 1::2] * img_shape[0]
+        det_bboxes[:, 0::2].clamp_(min=0, max=img_shape[1])
+        det_bboxes[:, 1::2].clamp_(min=0, max=img_shape[0])
+        det_bboxes = det_bboxes / det_bboxes.new_tensor(scale_factor)
+        det_bboxes = torch.cat((det_bboxes, scores[:, None, ...]), -1)
+
+        return bbox_index, det_bboxes, det_labels
+    
+    def get_bboxes_trt(
+        self,
+        all_cls_scores,
+        all_bbox_preds,
+        memory, 
+        memory_mask, 
+        query, 
+        hw_lvl,
+    ):
+        """
+        """
+        cls_scores = all_cls_scores[-1]
+        bbox_preds = all_bbox_preds[-1]
+
+        seg_list = []
+        stuff_score_list = []
+        bbox_list = []
+        labels_list = []
+        drivable_list = []
+        lane_list = []
+        lane_score_list = []
+        score_list = []
+        for img_id in range(1):
+            cls_score = cls_scores[img_id]
+            bbox_pred = bbox_preds[img_id]
+            img_shape = (self.canvas_size[0], self.canvas_size[1], 3)
+            ori_shape = (self.canvas_size[0], self.canvas_size[1], 3)
+            scale_factor = 1
+
+            index, bbox, labels = self._get_bboxes_single_trt(
+                cls_score, bbox_pred, img_shape, scale_factor)
+
+            i = img_id
+            thing_query = query[i:i + 1, index, :]
+            joint_query = torch.cat([
+                thing_query, self.stuff_query.weight[None, :, :self.embed_dims]
+            ], 1)
+
+            stuff_query_pos = self.stuff_query.weight[None, :,
+                                                      self.embed_dims:]
+
+            mask_things, _, _ = self.things_mask_head(
+                memory[i:i + 1],
+                memory_mask[i:i + 1],
+                None,
+                joint_query[:, :-self.num_stuff_classes],
+                None,
+                None,)
+            mask_stuff, _, _ = self.stuff_mask_head(
+                memory[i:i + 1],
+                memory_mask[i:i + 1],
+                None,
+                joint_query[:, -self.num_stuff_classes:],
+                None,
+                stuff_query_pos,)
+
+            attn_map = torch.cat([mask_things, mask_stuff], 1)
+            attn_map = attn_map[...,0] # BS, NQ, N_head,LEN
+
+            mask_pred = attn_map.reshape(-1, hw_lvl[0].item(), hw_lvl[1].item())
+            mask_pred = F.interpolate(mask_pred[None, ...],
+                                      size=ori_shape[:2],
+                                      mode='bilinear')[0,...]
+
+            masks_all = mask_pred
+            score_list.append(masks_all)
+            drivable_list.append(masks_all[-1] > 0.5)
+            masks_all = masks_all[:-self.num_stuff_classes]
+            seg_all = masks_all > 0.5
+            sum_seg_all = seg_all.sum((1, 2)).float() + 1
+
+            scores_all = bbox[:, -1]
+            bboxes_all = bbox
+            labels_all = labels
+
+            ## mask wise merging
+            seg_scores = (masks_all * seg_all.float()).sum(
+                (1, 2)) / sum_seg_all
+            scores_all = scores_all*(seg_scores**2)
+
+            scores_all, index = torch.sort(scores_all, descending=True)
+
+            masks_all = masks_all[index]
+            labels_all = labels_all[index]
+            bboxes_all = bboxes_all[index]
+            seg_all = seg_all[index]
+
+            bboxes_all[:, -1] = scores_all
+
+            # MDS: select things for instance segmeantion
+            things_selected = labels_all < self.num_things_classes
+            stuff_selected = labels_all >= self.num_things_classes
+            bbox_th = bboxes_all[things_selected][:100]
+            labels_th = labels_all[things_selected][:100]
+            seg_th = seg_all[things_selected][:100]
+            scores_st = scores_all[stuff_selected]
+            
+            stuff_score_list.append(scores_st)
+
+            results = torch.zeros((2, *mask_pred.shape[-2:]),
+                                  device=mask_pred.device).to(torch.long)
+            id_unique = 1
+            lane = torch.zeros((self.num_things_classes, *mask_pred.shape[-2:]), device=mask_pred.device).to(torch.long)
+            lane_score =  torch.zeros((self.num_things_classes, *mask_pred.shape[-2:]), device=mask_pred.device).to(mask_pred.dtype)
+
+            # TODO the necessity to vectorize those if-else conditions?
+            for i, scores in enumerate(scores_all):
+                # MDS: things and sutff have different threholds may perform a little bit better
+                if labels_all[i] < self.num_things_classes and scores < self.quality_threshold_things:
+                    continue
+                elif labels_all[i] >= self.num_things_classes and scores < self.quality_threshold_stuff:
+                    continue
+                _mask = masks_all[i] > 0.5
+                mask_area = _mask.sum().item()
+                intersect = _mask & (results[0] > 0)
+                intersect_area = intersect.sum().item()
+                if labels_all[i] < self.num_things_classes:
+                    if mask_area == 0 or (intersect_area * 1.0 / mask_area
+                                          ) > self.overlap_threshold_things:
+                        continue
+                else:
+                    if mask_area == 0 or (intersect_area * 1.0 / mask_area
+                                          ) > self.overlap_threshold_stuff:
+                        continue
+                if intersect_area > 0:
+                    _mask = _mask & (results[0] == 0)
+                results[0, _mask] = labels_all[i]
+                if labels_all[i] < self.num_things_classes:
+                    lane[labels_all[i], _mask] = 1
+                    lane_score[labels_all[i]]=lane_score[labels_all[i]]*(~_mask)\
+                                            +masks_all[i]*(_mask)
+                    results[1, _mask] = id_unique
+                    id_unique = id_unique + 1
+
+            bbox_list.append(bbox_th)
+            labels_list.append(labels_th)
+            seg_list.append(seg_th)
+            lane_list.append(lane)
+            lane_score_list.append(lane_score)
+
+        return_orishape = torch.tensor(ori_shape, device=results.device)
+        return bbox_list[0],\
+               seg_list[0],\
+               labels_list[0],\
+               drivable_list[0],\
+               score_list[0],\
+               lane_list[0],\
+               lane_score_list[0],\
+               stuff_score_list[0],\
+               results,\
+               return_orishape
+
+    def get_bboxes_post_process(img_metas, results, ori_shape):
+        file_name = img_metas[0]['pts_filename'].split('/')[-1].split('.')[0]
+        return results.permute(1, 2, 0).cpu().numpy(), file_name, ori_shape
+
+
+    def forward_test_trt(self,
+                    pts_feats, #torch.Size([40000, 1, 256])
+                    gt_lane_labels, #torch.Size([1, 29])
+                    gt_lane_masks, #torch.Size([1, 29, 200, 200])
+                    ):
+        bbox_list = [dict() for i in range(1)]
+        outputs_classes, outputs_coords, \
+        memory, \
+        memory_mask, \
+        memory_pos, \
+        query, \
+        query_pos, \
+        hw_lvl0, reference = self.forward_trt(pts_feats)
+
+        bbox_pred,\
+        seg_pred,\
+        labels_pred,\
+        drivable_pred,\
+        score_pred,\
+        lane_pred,\
+        lane_score_pred,\
+        stuff_score_pred,\
+        results,\
+        ori_shape = self.get_bboxes_trt(outputs_classes,
+                        outputs_coords,
+                        memory, 
+                        memory_mask, 
+                        query, 
+                        hw_lvl0,)
+        
+        # TODO the necessity of those IOUs and GTs as inputs
+        with torch.no_grad():
+            drivable_gt = gt_lane_masks[0, -1]
+            drivable_iou, drivable_intersection, drivable_union = IOU(drivable_pred.view(1, -1), drivable_gt.view(1, -1))
+
+            lanes_pred = (lane_pred.sum(0) > 0).int()
+            lanes_gt = (gt_lane_masks[0][:-1].sum(0) > 0).int()
+            lanes_iou, lanes_intersection, lanes_union = IOU(lanes_pred.view(1, -1), lanes_gt.view(1, -1))
+
+            divider_gt = (gt_lane_masks[0][gt_lane_labels[0] == 0].sum(0) > 0).int()
+            crossing_gt = (gt_lane_masks[0][gt_lane_labels[0] == 1].sum(0) > 0).int()
+            contour_gt = (gt_lane_masks[0][gt_lane_labels[0] == 2].sum(0) > 0).int()
+            divider_iou, divider_intersection, divider_union = IOU(lane_pred[0].view(1, -1), divider_gt.view(1, -1))
+            crossing_iou, crossing_intersection, crossing_union = IOU(lane_pred[1].view(1, -1), crossing_gt.view(1, -1))
+            contour_iou, contour_intersection, contour_union = IOU(lane_pred[2].view(1, -1), contour_gt.view(1, -1))
+
+        return (
+            bbox_pred,
+            seg_pred,
+            labels_pred,
+            drivable_pred,
+            score_pred,
+            lane_pred,
+            lane_score_pred,
+            stuff_score_pred,
+            drivable_intersection,
+            drivable_union,
+            lanes_intersection,
+            lanes_union,
+            divider_intersection,
+            divider_union,
+            crossing_intersection,
+            crossing_union,
+            contour_intersection,
+            contour_union,
+            drivable_iou,
+            lanes_iou,
+            divider_iou,
+            crossing_iou,
+            contour_iou,
+            memory,
+            memory_mask,
+            memory_pos,
+            query,
+            query_pos,
+            hw_lvl0,
+            reference,
+            results,
+            ori_shape,
+        )
+    
+
+
+@HEADS.register_module()
+class PansegformerHeadTRTP(PansegformerHeadTRT):
+    """
+    Head of Panoptic SegFormer
+
+    Code is modified from the `official github repo
+    <https://github.com/open-mmlab/mmdetection>`_.
+
+    Args:
+        with_box_refine (bool): Whether to refine the reference points
+            in the decoder. Defaults to False.
+        as_two_stage (bool) : Whether to generate the proposal from
+            the outputs of encoder.
+        transformer (obj:`ConfigDict`): ConfigDict is used for building
+            the Encoder and Decoder.
+    """
+
+    def __init__(self, *args, **kwargs):
+        super(PansegformerHeadTRTP, self).__init__(*args, **kwargs)
diff --git a/projects/mmdet3d_plugin/uniad/dense_heads/planning_head.py b/projects/mmdet3d_plugin/uniad/dense_heads/planning_head.py
index 6eb5a56..cffeb00 100644
--- a/projects/mmdet3d_plugin/uniad/dense_heads/planning_head.py
+++ b/projects/mmdet3d_plugin/uniad/dense_heads/planning_head.py
@@ -246,4 +246,115 @@ class PlanningHeadSingleMode(nn.Module):
             loss_dict[f'loss_collision_{i}'] = loss_collision          
         loss_ade = self.loss_planning(sdc_traj_all, sdc_planning[0, :, :self.planning_steps, :2], torch.any(sdc_planning_mask[0, :, :self.planning_steps], dim=-1))
         loss_dict.update(dict(loss_ade=loss_ade))
-        return loss_dict
\ No newline at end of file
+        return loss_dict
+    
+
+@HEADS.register_module()
+class PlanningHeadSingleModeTRT(PlanningHeadSingleMode):
+    def __init__(self, *args, **kwargs):
+        super().__init__(*args, **kwargs)
+
+    def forward_test_trt(self, bev_embed, sdc_traj_query, sdc_track_query,
+                         bev_pos, seg_out, command=None):
+        outs_planning = self.forward_trt(bev_embed, seg_out, bev_pos, sdc_traj_query, sdc_track_query, command)
+        return outs_planning
+
+    def forward_trt(self, 
+                bev_embed, 
+                occ_mask, 
+                bev_pos, 
+                sdc_traj_query, 
+                sdc_track_query, 
+                command):
+        """
+        Forward pass for PlanningHeadSingleMode.
+
+        Args:
+            bev_embed (torch.Tensor): Bird's eye view feature embedding.
+            occ_mask (torch.Tensor): Instance mask for occupancy.
+            bev_pos (torch.Tensor): BEV position.
+            sdc_traj_query (torch.Tensor): SDC trajectory query.
+            sdc_track_query (torch.Tensor): SDC track query.
+            command (int): Driving command.
+
+        Returns:
+            dict: A dictionary containing SDC trajectory and all SDC trajectories.
+        """
+        sdc_track_query = sdc_track_query.detach()
+        sdc_traj_query = sdc_traj_query[-1]
+        P = sdc_traj_query.shape[1]
+        sdc_track_query = sdc_track_query[:, None].expand(-1,P,-1)
+        
+        
+        navi_embed = self.navi_embed.weight[command]
+        navi_embed = navi_embed[None].expand(-1,P,-1)
+        plan_query = torch.cat([sdc_traj_query, sdc_track_query, navi_embed], dim=-1)
+
+        plan_query = self.mlp_fuser(plan_query).max(1, keepdim=True)[0]   # expand, then fuse  # [1, 6, 768] -> [1, 1, 256]
+        plan_query = rearrange(plan_query, 'b p c -> p b c')
+        
+        bev_pos = rearrange(bev_pos, 'b c h w -> (h w) b c')
+        bev_feat = bev_embed +  bev_pos
+        
+        ##### Plugin adapter #####
+        bev_feat = rearrange(bev_feat, '(h w) b c -> b c h w', h=self.bev_h, w=self.bev_w)
+        bev_feat = bev_feat + self.bev_adapter(bev_feat)  # residual connection
+        bev_feat = rearrange(bev_feat, 'b c h w -> (h w) b c')
+        ##########################
+      
+        pos_embed = self.pos_embed.weight
+        plan_query = plan_query + pos_embed[None]  # [1, 1, 256]
+        
+        # plan_query: [1, 1, 256]
+        # bev_feat: [40000, 1, 256]
+        plan_query = self.attn_module(plan_query, bev_feat)   # [1, 1, 256]
+        
+        sdc_traj_all = self.reg_branch(plan_query).view((-1, self.planning_steps, 2))
+        sdc_traj_all[...,:2] = torch.cumsum(sdc_traj_all[...,:2], dim=1)
+        sdc_traj_all[0] = bivariate_gaussian_activation(sdc_traj_all[0])
+        
+        return sdc_traj_all
+
+    # def collision_optimization_trt(self, sdc_traj_all, occ_mask):
+    #     """
+    #     Optimize SDC trajectory with occupancy instance mask.
+
+    #     Args:
+    #         sdc_traj_all (torch.Tensor): SDC trajectory tensor.
+    #         occ_mask (torch.Tensor): Occupancy flow instance mask. 
+    #     Returns:
+    #         torch.Tensor: Optimized SDC trajectory tensor.
+    #     """
+    #     pos_xy_t = []
+    #     valid_occupancy_num = 0
+        
+    #     if occ_mask.shape[2] == 1:
+    #         occ_mask = occ_mask[:,:,0,...]#.squeeze(2)
+    #     occ_horizon = occ_mask.shape[1]
+    #     assert occ_horizon == 5
+
+    #     for t in range(self.planning_steps):
+    #         cur_t = min(t+1, occ_horizon-1)
+    #         pos_xy = torch.nonzero(occ_mask[0][cur_t], as_tuple=False)
+    #         pos_xy = pos_xy[:, [1, 0]]
+    #         pos_xy[:, 0] = (pos_xy[:, 0] - self.bev_h//2) * 0.5 + 0.25
+    #         pos_xy[:, 1] = (pos_xy[:, 1] - self.bev_w//2) * 0.5 + 0.25
+
+    #         # filter the occupancy in range
+    #         keep_index = torch.sum((sdc_traj_all[0, t, :2][None, :] - pos_xy[:, :2])**2, axis=-1) < self.occ_filter_range**2
+    #         pos_xy_t.append(pos_xy[keep_index].cpu().detach().numpy())
+    #         valid_occupancy_num += torch.sum(keep_index>0)
+    #     if valid_occupancy_num == 0:
+    #         return sdc_traj_all
+        
+    #     col_optimizer = CollisionNonlinearOptimizer(self.planning_steps, 0.5, self.sigma, self.alpha_collision, pos_xy_t)
+    #     col_optimizer.set_reference_trajectory(sdc_traj_all[0].cpu().detach().numpy())
+    #     sol = col_optimizer.solve()
+    #     sdc_traj_optim = np.stack([sol.value(col_optimizer.position_x), sol.value(col_optimizer.position_y)], axis=-1)
+    #     return torch.tensor(sdc_traj_optim[None], device=sdc_traj_all.device, dtype=sdc_traj_all.dtype)
+    
+
+@HEADS.register_module()
+class PlanningHeadSingleModeTRTP(PlanningHeadSingleModeTRT):
+    def __init__(self, *args, **kwargs):
+        super(PlanningHeadSingleModeTRTP, self).__init__(*args, **kwargs)
diff --git a/projects/mmdet3d_plugin/uniad/dense_heads/seg_head_plugin/seg_detr_head.py b/projects/mmdet3d_plugin/uniad/dense_heads/seg_head_plugin/seg_detr_head.py
index 7093e97..cbd1e54 100644
--- a/projects/mmdet3d_plugin/uniad/dense_heads/seg_head_plugin/seg_detr_head.py
+++ b/projects/mmdet3d_plugin/uniad/dense_heads/seg_head_plugin/seg_detr_head.py
@@ -668,12 +668,14 @@ class SegDETRHead(
         if self.loss_cls.use_sigmoid:
             cls_score = cls_score.sigmoid()
             scores, indexes = cls_score.view(-1).topk(max_per_img)
+
             det_labels = indexes % self.num_things_classes
             bbox_index = indexes // self.num_things_classes
             bbox_pred = bbox_pred[bbox_index]
         else:
             scores, det_labels = F.softmax(cls_score, dim=-1)[..., :-1].max(-1)
             scores, bbox_index = scores.topk(max_per_img)
+
             bbox_pred = bbox_pred[bbox_index]
             det_labels = det_labels[bbox_index]
 
@@ -683,7 +685,7 @@ class SegDETRHead(
         det_bboxes[:, 0::2].clamp_(min=0, max=img_shape[1])
         det_bboxes[:, 1::2].clamp_(min=0, max=img_shape[0])
         if rescale:
-            det_bboxes /= det_bboxes.new_tensor(scale_factor)
+            det_bboxes = det_bboxes / det_bboxes.new_tensor(scale_factor)
         det_bboxes = torch.cat((det_bboxes, scores.unsqueeze(1)), -1)
 
         return det_bboxes, det_labels
diff --git a/projects/mmdet3d_plugin/uniad/dense_heads/seg_head_plugin/seg_mask_head.py b/projects/mmdet3d_plugin/uniad/dense_heads/seg_head_plugin/seg_mask_head.py
index 64f976e..3da65df 100644
--- a/projects/mmdet3d_plugin/uniad/dense_heads/seg_head_plugin/seg_mask_head.py
+++ b/projects/mmdet3d_plugin/uniad/dense_heads/seg_head_plugin/seg_mask_head.py
@@ -13,7 +13,7 @@ import math
 from mmcv.runner import force_fp32
 
 count = 0
-
+fp16_enabled=False
 
 class Mlp(nn.Module):
     def __init__(self,
@@ -23,7 +23,7 @@ class Mlp(nn.Module):
                  act_layer=nn.GELU,
                  drop=0.):
         super().__init__()
-        self.fp16_enabled = False
+        self.fp16_enabled = fp16_enabled
         out_features = out_features or in_features
         hidden_features = hidden_features or in_features
         self.fc1 = nn.Linear(in_features, hidden_features)
@@ -54,7 +54,7 @@ class SelfAttention(nn.Module):
         super().__init__()
         self.num_heads = num_heads
         head_dim = dim // num_heads
-        self.fp16_enabled = False
+        self.fp16_enabled = fp16_enabled
         self.scale = qk_scale or head_dim**-0.5
 
         self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)
@@ -93,7 +93,7 @@ class Attention(nn.Module):
                  attn_drop=0.,
                  proj_drop=0.):
         super().__init__()
-        self.fp16_enabled = False
+        self.fp16_enabled = fp16_enabled
         self.num_heads = num_heads
         head_dim = dim // num_heads
         self.scale = qk_scale or head_dim**-0.5
@@ -119,10 +119,9 @@ class Attention(nn.Module):
                 nn.init.xavier_uniform_(p)
 
     @force_fp32(apply_to=('query', 'key', 'value'))
-    def forward(self, query, key, value, key_padding_mask, hw_lvl):
+    def forward(self, query, key, value):#, key_padding_mask, hw_lvl):
         B, N, C = query.shape
         _, L, _ = key.shape
-        #print('query, key, value', query.shape, value.shape, key.shape)
         q = self.q(query).reshape(B, N,
                                   self.num_heads, C // self.num_heads).permute(
                                       0, 2, 1,
@@ -165,7 +164,7 @@ class AttentionTail(nn.Module):
                  attn_drop=0.,
                  proj_drop=0.):
         super().__init__()
-        self.fp16_enabled = False
+        self.fp16_enabled = fp16_enabled
         self.num_heads = num_heads
         head_dim = dim // num_heads
         self.scale = qk_scale or head_dim**-0.5
@@ -189,7 +188,7 @@ class AttentionTail(nn.Module):
                 nn.init.xavier_uniform_(p)
 
     @force_fp32(apply_to=('query', 'key'))
-    def forward(self, query, key, key_padding_mask, hw_lvl=None):
+    def forward(self, query, key):#, key_padding_mask, hw_lvl=None):
         B, N, C = query.shape
         _, L, _ = key.shape
         #print('query, key, value', query.shape, value.shape, key.shape)
@@ -226,7 +225,7 @@ class Block(nn.Module):
                  norm_layer=nn.LayerNorm,
                  self_attn=False):
         super().__init__()
-        self.fp16_enabled = False
+        self.fp16_enabled = fp16_enabled
         self.head_norm1 = norm_layer(dim)
         self.self_attn = self_attn
         self.attn = Attention(cfg,
@@ -257,11 +256,11 @@ class Block(nn.Module):
             self.norm3 = norm_layer(dim)
 
     @force_fp32(apply_to=('query', 'key', 'value'))
-    def forward(self, query, key, value, key_padding_mask=None, hw_lvl=None):
+    def forward(self, query, key, value):#, key_padding_mask=None, hw_lvl=None):
         if self.self_attn:
             query = query + self.drop_path(self.self_attention(query))
             query = self.norm3(query)
-        x, mask = self.attn(query, key, value, key_padding_mask, hw_lvl=hw_lvl)
+        x, mask = self.attn(query, key, value)#, key_padding_mask, hw_lvl=hw_lvl)
         query = query + self.drop_path(x)
         query = self.head_norm1(query)
 
@@ -322,7 +321,7 @@ class SegMaskHead(nn.Module):
                  self_attn=False):
         super().__init__()
 
-        self.fp16_enabled = False
+        self.fp16_enabled = fp16_enabled
         mlp_ratio = 4
         qkv_bias = True
         qk_scale = None
@@ -370,24 +369,22 @@ class SegMaskHead(nn.Module):
     @force_fp32(apply_to=('memory', 'mask_memory', 'pos_memory', 'query_embed',
                           'mask_query', 'pos_query'))
     def forward(self, memory, mask_memory, pos_memory, query_embed, mask_query,
-                pos_query, hw_lvl):
+                pos_query):#, hw_lvl):
         if mask_memory is not None and isinstance(mask_memory, torch.Tensor):
             mask_memory = mask_memory.to(torch.bool)
         masks = []
         inter_query = []
         for i, block in enumerate(self.blocks):
-            query_embed, mask = block(self.with_pos_embed(
-                query_embed, pos_query),
+            query_embed, mask = block(self.with_pos_embed(query_embed, pos_query),
                                       self.with_pos_embed(memory, pos_memory),
-                                      memory,
-                                      key_padding_mask=mask_memory,
-                                      hw_lvl=hw_lvl)
+                                      memory,)
+                                    #   key_padding_mask=mask_memory,
+                                    #   hw_lvl=hw_lvl)
             masks.append(mask)
             inter_query.append(query_embed)
-            #if i == 1:
-            #    return mask, masks, inter_query
+
         attn = self.attnen(self.with_pos_embed(query_embed, pos_query),
-                           self.with_pos_embed(memory, pos_memory),
-                           key_padding_mask=mask_memory,
-                           hw_lvl=hw_lvl)
+                           self.with_pos_embed(memory, pos_memory),)
+                        #    key_padding_mask=mask_memory,
+                        #    hw_lvl=hw_lvl)
         return attn, masks, inter_query
diff --git a/projects/mmdet3d_plugin/uniad/dense_heads/track_head.py b/projects/mmdet3d_plugin/uniad/dense_heads/track_head.py
index 1552735..53bc4f5 100644
--- a/projects/mmdet3d_plugin/uniad/dense_heads/track_head.py
+++ b/projects/mmdet3d_plugin/uniad/dense_heads/track_head.py
@@ -16,7 +16,8 @@ from mmdet.core import (multi_apply, multi_apply, reduce_mean)
 from mmdet.models.utils.transformer import inverse_sigmoid
 from mmdet.models import HEADS
 from mmdet.models.dense_heads import DETRHead
-from mmdet3d.core.bbox.coders import build_bbox_coder
+# from mmdet3d.core.bbox.coders import build_bbox_coder
+from mmdet.core.bbox import build_bbox_coder
 from projects.mmdet3d_plugin.core.bbox.util import normalize_bbox
 from mmcv.runner import force_fp32, auto_fp16
 
@@ -530,3 +531,310 @@ class BEVFormerTrackHead(DETRHead):
             ret_list.append([bboxes, scores, labels, bbox_index, mask])
 
         return ret_list
+
+
+@HEADS.register_module()
+class BEVFormerTrackHeadTRT(BEVFormerTrackHead):
+    """Head of Detr3D.
+        Args:
+            with_box_refine (bool): Whether to refine the reference points
+                in the decoder. Defaults to False.
+            as_two_stage (bool) : Whether to generate the proposal from
+                the outputs of encoder.
+            transformer (obj:`ConfigDict`): ConfigDict is used for building
+                the Encoder and Decoder.
+            bev_h, bev_w (int): spatial shape of BEV queries.
+        """
+
+    def __init__(
+        self,
+        *args,
+        with_box_refine=False,
+        as_two_stage=False,
+        transformer=None,
+        bbox_coder=None,
+        num_cls_fcs=2,
+        code_weights=None,
+        bev_h=30,
+        bev_w=30,
+        past_steps=4,
+        fut_steps=4,
+        linear_cfg=None,
+        **kwargs
+    ):
+
+        self.bev_h = bev_h
+        self.bev_w = bev_w
+        self.fp16_enabled = False
+
+        if linear_cfg is None:
+            linear_cfg = dict(type="Linear")
+
+        self.with_box_refine = with_box_refine
+
+        assert as_two_stage is False, 'as_two_stage is not supported yet.'
+        self.as_two_stage = as_two_stage
+        if self.as_two_stage:
+            transformer["as_two_stage"] = self.as_two_stage
+        if "code_size" in kwargs:
+            self.code_size = kwargs["code_size"]
+        else:
+            self.code_size = 10
+        if code_weights is not None:
+            self.code_weights = code_weights
+        else:
+            self.code_weights = [1.0, 1.0, 1.0,
+                                 1.0, 1.0, 1.0, 1.0, 1.0, 0.2, 0.2]
+
+        self.bbox_coder = build_bbox_coder(bbox_coder)
+        self.pc_range = self.bbox_coder.pc_range
+        self.real_w = self.pc_range[3] - self.pc_range[0]
+        self.real_h = self.pc_range[4] - self.pc_range[1]
+        self.num_cls_fcs = num_cls_fcs - 1
+        self.past_steps = past_steps
+        self.fut_steps = fut_steps
+        super(BEVFormerTrackHead, self).__init__(
+            *args, transformer=transformer, **kwargs)
+        self.code_weights = nn.Parameter(torch.tensor(
+            self.code_weights, requires_grad=False), requires_grad=False)
+
+    def _init_layers(self):
+        """Initialize classification branch and regression branch of head."""
+        cls_branch = []
+        for _ in range(self.num_reg_fcs):
+            cls_branch.append(Linear(self.embed_dims, self.embed_dims))
+            cls_branch.append(nn.LayerNorm(self.embed_dims))
+            cls_branch.append(nn.ReLU(inplace=True))
+        cls_branch.append(Linear(self.embed_dims, self.cls_out_channels))
+        fc_cls = nn.Sequential(*cls_branch)
+
+        reg_branch = []
+        for _ in range(self.num_reg_fcs):
+            reg_branch.append(Linear(self.embed_dims, self.embed_dims))
+            reg_branch.append(
+                nn.ReLU(inplace=True)
+            )
+        reg_branch.append(Linear(self.embed_dims, self.code_size))
+        reg_branch = nn.Sequential(*reg_branch)
+        past_traj_reg_branch = []
+        for _ in range(self.num_reg_fcs):
+            past_traj_reg_branch.append(
+                Linear(self.embed_dims, self.embed_dims))
+            past_traj_reg_branch.append(nn.ReLU())
+        past_traj_reg_branch.append(
+            Linear(self.embed_dims, (self.past_steps + self.fut_steps)*2))
+        past_traj_reg_branch = nn.Sequential(*past_traj_reg_branch)
+
+        def _get_clones(module, N):
+            return nn.ModuleList([copy.deepcopy(module) for i in range(N)])
+
+        num_pred = (
+            (self.transformer.decoder.num_layers + 1)
+            if self.as_two_stage
+            else self.transformer.decoder.num_layers
+        )
+
+        if self.with_box_refine:
+            self.cls_branches = _get_clones(fc_cls, num_pred)
+            self.reg_branches = _get_clones(reg_branch, num_pred)
+            self.past_traj_reg_branches = _get_clones(
+                past_traj_reg_branch, num_pred)
+        else:
+            self.cls_branches = nn.ModuleList(
+                [fc_cls for _ in range(num_pred)])
+            self.reg_branches = nn.ModuleList(
+                [reg_branch for _ in range(num_pred)])
+            self.past_traj_reg_branches = nn.ModuleList(
+                [past_traj_reg_branch for _ in range(num_pred)])
+        if not self.as_two_stage:
+            self.bev_embedding = nn.Embedding(
+                self.bev_h * self.bev_w, self.embed_dims)
+    def get_bev_features_trt(self, mlvl_feats, can_bus, lidar2img, image_shape, prev_bev=None,use_prev_bev=1.0):# 
+        bs, num_cam, _, _, _ = mlvl_feats[0].shape
+        dtype = mlvl_feats[0].dtype
+        bev_queries = self.bev_embedding.weight.to(dtype)
+
+        bev_mask = torch.zeros((bs, self.bev_h, self.bev_w), 
+                               device=bev_queries.device).to(dtype)
+        bev_pos = self.positional_encoding(bev_mask).to(dtype)
+        bev_embed = self.transformer.get_bev_features_trt(
+            mlvl_feats,
+            bev_queries,
+            torch.tensor([self.bev_h], device=bev_queries.device),
+            torch.tensor([self.bev_w], device=bev_queries.device),
+            can_bus,
+            lidar2img,
+            grid_length=torch.tensor([self.real_h / self.bev_h,
+                                    self.real_w / self.bev_w], device=bev_queries.device),
+            bev_pos=bev_pos,
+            prev_bev=prev_bev,
+            image_shape=image_shape,
+            use_prev_bev=use_prev_bev,
+        )
+        print("=======BEVFormerTrackHeadTRT get_bev_features_trt=======")
+        return bev_embed, bev_pos
+
+
+    def get_detections_trt(
+        self, 
+        bev_embed,
+        object_query_embeds=None,
+        ref_points=None,
+    ):
+        assert bev_embed.shape[0] == self.bev_h * self.bev_w
+        hs, init_reference, inter_references = self.transformer.get_states_and_refs_trt(
+            bev_embed,
+            object_query_embeds,
+            self.bev_h,
+            self.bev_w,
+            reference_points=ref_points,
+            reg_branches=self.reg_branches if self.with_box_refine else None,
+            cls_branches=self.cls_branches if self.as_two_stage else None,
+        )
+        hs = hs.permute(0, 2, 1, 3)
+        outputs_classes = []
+        outputs_coords = []
+        outputs_trajs = []
+        for lvl in range(hs.shape[0]):
+            if lvl == 0:
+                reference = ref_points.sigmoid()
+            else:
+                reference = inter_references[lvl - 1]
+            reference = inverse_sigmoid(reference)
+            outputs_class = self.cls_branches[lvl](hs[lvl])
+            tmp = self.reg_branches[lvl](hs[lvl])  # xydxdyxdz
+            outputs_past_traj = self.past_traj_reg_branches[lvl](hs[lvl]).view(
+                tmp.shape[0], -1, self.past_steps + self.fut_steps, 2)
+
+            assert reference.shape[-1] == 3
+            tmp[..., 0:2] = tmp[..., 0:2] + reference[..., 0:2]
+            tmp[..., 0:2] = tmp[..., 0:2].sigmoid()
+            tmp[..., 4:5] = tmp[..., 4:5] + reference[..., 2:3]
+            tmp[..., 4:5] = tmp[..., 4:5].sigmoid()
+
+            last_ref_points = torch.cat(
+                [tmp[..., 0:2], tmp[..., 4:5]], dim=-1,
+            )
+
+            tmp[..., 0:1] = (tmp[..., 0:1] * (self.pc_range[3] -
+                             self.pc_range[0]) + self.pc_range[0])
+            tmp[..., 1:2] = (tmp[..., 1:2] * (self.pc_range[4] -
+                             self.pc_range[1]) + self.pc_range[1])
+            tmp[..., 4:5] = (tmp[..., 4:5] * (self.pc_range[5] -
+                             self.pc_range[2]) + self.pc_range[2])
+
+            outputs_coord = tmp
+            outputs_classes.append(outputs_class)
+            outputs_coords.append(outputs_coord)
+            outputs_trajs.append(outputs_past_traj)
+        outputs_classes = torch.stack(outputs_classes)
+        outputs_coords = torch.stack(outputs_coords)
+        outputs_trajs = torch.stack(outputs_trajs)
+        last_ref_points = inverse_sigmoid(last_ref_points)
+
+        return outputs_classes, outputs_coords, outputs_trajs, last_ref_points, hs
+
+    def get_bboxes_trt(self, outputs_classes, outputs_coords, img_metas, rescale=False):
+        dic = {"all_cls_scores": outputs_classes, "all_bbox_preds": outputs_coords}
+        return self.get_bboxes(dic, img_metas, rescale=rescale)
+
+
+
+@HEADS.register_module(force=True)
+class BEVFormerTrackHeadTRTP(BEVFormerTrackHeadTRT):
+    """Head of Detr3D.
+        Args:
+            with_box_refine (bool): Whether to refine the reference points
+                in the decoder. Defaults to False.
+            as_two_stage (bool) : Whether to generate the proposal from
+                the outputs of encoder.
+            transformer (obj:`ConfigDict`): ConfigDict is used for building
+                the Encoder and Decoder.
+            bev_h, bev_w (int): spatial shape of BEV queries.
+        """
+
+    def __init__(self, *args, **kwargs):
+        super(BEVFormerTrackHeadTRTP, self).__init__(*args, **kwargs)
+
+    def get_bev_features_trt(self, mlvl_feats, can_bus, lidar2img, image_shape, prev_bev, use_prev_bev, ):
+        dtype = mlvl_feats[0].dtype
+        bev_queries = self.bev_embedding.weight.to(dtype)
+
+        bev_mask = torch.zeros((1, self.bev_h, self.bev_w), 
+                               device=bev_queries.device).to(dtype)
+        bev_pos = self.positional_encoding(bev_mask).to(dtype)
+        bev_embed = self.transformer.get_bev_features_trt(
+            mlvl_feats,
+            bev_queries,
+            torch.tensor([self.bev_h], device=bev_queries.device),
+            torch.tensor([self.bev_w], device=bev_queries.device),
+            can_bus,
+            lidar2img,
+            grid_length=torch.tensor([self.real_h / self.bev_h,
+                                    self.real_w / self.bev_w], device=bev_queries.device),
+            bev_pos=bev_pos,
+            prev_bev=prev_bev,
+            image_shape=image_shape,
+            use_prev_bev=use_prev_bev,
+        )
+        return bev_embed, bev_pos
+
+
+    def get_detections_trt(
+        self, 
+        bev_embed,
+        object_query_embeds=None,
+        ref_points=None,
+    ):
+        assert bev_embed.shape[0] == self.bev_h * self.bev_w
+        hs, _, inter_references = self.transformer.get_states_and_refs_trt(
+            bev_embed,
+            object_query_embeds,
+            self.bev_h,
+            self.bev_w,
+            reference_points=ref_points,
+            reg_branches=self.reg_branches if self.with_box_refine else None,
+            cls_branches=self.cls_branches if self.as_two_stage else None,
+        )
+
+        hs = hs.permute(0, 2, 1, 3)
+        outputs_classes = []
+        outputs_coords = []
+        outputs_trajs = []
+        for lvl in range(hs.shape[0]):
+            if lvl == 0:
+                reference = ref_points.sigmoid()[None, ...]
+            else:
+                reference = inter_references[lvl - 1]
+            reference = inverse_sigmoid(reference)
+            outputs_class = self.cls_branches[lvl](hs[lvl])
+            outputs_coord = self.reg_branches[lvl](hs[lvl])  # xydxdyxdz
+            outputs_past_traj = self.past_traj_reg_branches[lvl](hs[lvl]).view(
+                outputs_coord.shape[0], -1, self.past_steps + self.fut_steps, 2)
+
+            assert reference.shape[-1] == 3
+            outputs_coord[..., 0:2] = outputs_coord[..., 0:2] + reference[..., 0:2]
+            outputs_coord[..., 0:2] = outputs_coord[..., 0:2].sigmoid()
+            outputs_coord[..., 4:5] = outputs_coord[..., 4:5] + reference[..., 2:3]
+            outputs_coord[..., 4:5] = outputs_coord[..., 4:5].sigmoid()
+
+            last_ref_points = torch.cat(
+                [outputs_coord[..., 0:2], outputs_coord[..., 4:5]], dim=-1,
+            )
+
+            outputs_coord[..., 0:1] = (outputs_coord[..., 0:1] * (self.pc_range[3] -
+                             self.pc_range[0]) + self.pc_range[0])
+            outputs_coord[..., 1:2] = (outputs_coord[..., 1:2] * (self.pc_range[4] -
+                             self.pc_range[1]) + self.pc_range[1])
+            outputs_coord[..., 4:5] = (outputs_coord[..., 4:5] * (self.pc_range[5] -
+                             self.pc_range[2]) + self.pc_range[2])
+
+            outputs_classes.append(outputs_class)
+            outputs_coords.append(outputs_coord)
+            outputs_trajs.append(outputs_past_traj)
+        outputs_classes = torch.stack(outputs_classes)
+        outputs_coords = torch.stack(outputs_coords)
+        outputs_trajs = torch.stack(outputs_trajs)
+        last_ref_points = inverse_sigmoid(last_ref_points)
+        return outputs_classes, outputs_coords, outputs_trajs, last_ref_points, hs
+
diff --git a/projects/mmdet3d_plugin/uniad/dense_heads/track_head_plugin/__init__.py b/projects/mmdet3d_plugin/uniad/dense_heads/track_head_plugin/__init__.py
index f7933ab..2ce4f38 100644
--- a/projects/mmdet3d_plugin/uniad/dense_heads/track_head_plugin/__init__.py
+++ b/projects/mmdet3d_plugin/uniad/dense_heads/track_head_plugin/__init__.py
@@ -1,3 +1,3 @@
-from .modules import MemoryBank, QueryInteractionModule
+from .modules import MemoryBank, QueryInteractionModule, MemoryBankTRTP, QueryInteractionModuleTRTP
 from .track_instance import Instances
 from .tracker import RuntimeTrackerBase
\ No newline at end of file
diff --git a/projects/mmdet3d_plugin/uniad/dense_heads/track_head_plugin/modules.py b/projects/mmdet3d_plugin/uniad/dense_heads/track_head_plugin/modules.py
index db80e6b..1be42f1 100644
--- a/projects/mmdet3d_plugin/uniad/dense_heads/track_head_plugin/modules.py
+++ b/projects/mmdet3d_plugin/uniad/dense_heads/track_head_plugin/modules.py
@@ -3,6 +3,18 @@ import torch.nn.functional as F
 from torch import nn
 from .track_instance import Instances
 
+def index_bool2long_trt(bool_index):
+    # Convert Boolean Tensor to Long Tensor:
+    long_index = bool_index.long()
+    # Create a Range Tensor and Multiply by the Long Tensor
+    long_index = torch.arange(1, long_index.shape[-1]+1
+                                        , device=long_index.device)*long_index
+    # Extract Non-Zero Indices
+    long_index = long_index[long_index.nonzero(as_tuple=True)[0]]
+    # Adjust Indices to Be Zero-Based
+    long_index = long_index - torch.ones_like(long_index)
+    return long_index
+
 # MemoryBank
 class MemoryBank(nn.Module):
 
@@ -89,6 +101,77 @@ class MemoryBank(nn.Module):
         return track_instances
 
 
+# MemoryBankTRTP
+class MemoryBankTRTP(MemoryBank):
+
+    def __init__(self,
+                 args,
+                 dim_in, hidden_dim, dim_out,
+                 ):
+        super(MemoryBank, self).__init__()
+        self._build_layers(args, dim_in, hidden_dim, dim_out)
+        for p in self.parameters():
+            if p.dim() > 1:
+                nn.init.xavier_uniform_(p)
+
+    def _build_layers(self, args, dim_in, hidden_dim, dim_out):
+        self.save_thresh = args['memory_bank_score_thresh']
+        self.save_period = 3
+        self.max_his_length = args['memory_bank_len']
+
+        self.save_proj = nn.Linear(dim_in, dim_in)
+
+        self.temporal_attn = nn.MultiheadAttention(dim_in, 8, dropout=0)
+        self.temporal_fc1 = nn.Linear(dim_in, hidden_dim)
+        self.temporal_fc2 = nn.Linear(hidden_dim, dim_in)
+        self.temporal_norm1 = nn.LayerNorm(dim_in)
+        self.temporal_norm2 = nn.LayerNorm(dim_in)
+
+    def update_trt(self, output_embedding, scores, mem_padding_mask, save_period, mem_bank):
+        embed = output_embedding[:, None]  #( N, 1, 256)
+        saved_idxes = index_bool2long_trt((save_period == 0) & (scores > self.save_thresh))
+        save_period[index_bool2long_trt(save_period > 0)] = \
+            save_period[index_bool2long_trt(save_period > 0)] - torch.ones_like(save_period[index_bool2long_trt(save_period > 0)])
+        save_period[saved_idxes] = torch.ones_like(save_period[saved_idxes])*self.save_period
+
+        saved_embed = embed[saved_idxes]
+        prev_embed = mem_bank[saved_idxes]
+        save_embed = self.save_proj(saved_embed)
+        mem_padding_mask[saved_idxes] = torch.cat([mem_padding_mask[saved_idxes, 1:], 
+                                                   torch.zeros((mem_padding_mask[saved_idxes, 1:].shape[0], 1), 
+                                                               dtype=torch.int, device=embed.device)], dim=1)
+        mem_bank = mem_bank.clone()
+        mem_bank[saved_idxes] = torch.cat([prev_embed[:, 1:], save_embed], dim=1)
+        return mem_padding_mask, save_period, mem_bank
+
+    def _forward_temporal_attn_trt(self, mem_padding_mask, output_embedding, mem_bank):
+
+        key_padding_mask = mem_padding_mask 
+
+        valid_idxes = index_bool2long_trt(key_padding_mask[:, -1] == 0)
+        embed = output_embedding[valid_idxes]  # (n, 256)
+        prev_embed = mem_bank[valid_idxes] 
+        key_padding_mask = key_padding_mask[valid_idxes]
+        embed2 = self.temporal_attn(
+            embed[None],                  
+            prev_embed.transpose(0, 1),  
+            prev_embed.transpose(0, 1),
+            key_padding_mask=key_padding_mask.bool(), # must be bool not int/long
+        )[0][0]
+
+        embed = self.temporal_norm1(embed + embed2)
+        embed2 = self.temporal_fc2(F.relu(self.temporal_fc1(embed)))
+        embed = self.temporal_norm2(embed + embed2)
+        output_embedding = output_embedding.clone()
+        output_embedding[valid_idxes] = embed
+
+        return output_embedding 
+
+    def forward_trt(self, mem_padding_mask, output_embedding, mem_bank, scores, save_period):
+        output_embedding = self._forward_temporal_attn_trt(mem_padding_mask, output_embedding, mem_bank)
+        mem_padding_mask, save_period, mem_bank = self.update_trt(output_embedding, scores, mem_padding_mask, save_period, mem_bank)
+        return output_embedding, mem_padding_mask, save_period, mem_bank
+
 # QIM
 class QueryInteractionBase(nn.Module):
 
@@ -252,3 +335,92 @@ class QueryInteractionModule(QueryInteractionBase):
         merged_track_instances = Instances.cat(
             [init_track_instances, active_track_instances])
         return merged_track_instances
+
+
+class QueryInteractionModuleTRTP(QueryInteractionModule):
+
+    def __init__(self, args, dim_in, hidden_dim, dim_out):
+        super(QueryInteractionModule, self).__init__(args, dim_in, hidden_dim, dim_out)
+        self.random_drop = args["random_drop"]
+        self.fp_ratio = args["fp_ratio"]
+        self.update_query_pos = args["update_query_pos"]
+
+    def _build_layers(self, args, dim_in, hidden_dim, dim_out):
+        dropout = args["merger_dropout"]
+
+        self.self_attn = nn.MultiheadAttention(dim_in, 8, dropout)
+        self.linear1 = nn.Linear(dim_in, hidden_dim)
+        self.dropout = nn.Dropout(dropout)
+        self.linear2 = nn.Linear(hidden_dim, dim_in)
+
+        if args["update_query_pos"]:
+            self.linear_pos1 = nn.Linear(dim_in, hidden_dim)
+            self.linear_pos2 = nn.Linear(hidden_dim, dim_in)
+            self.dropout_pos1 = nn.Dropout(dropout)
+            self.dropout_pos2 = nn.Dropout(dropout)
+            self.norm_pos = nn.LayerNorm(dim_in)
+
+        self.linear_feat1 = nn.Linear(dim_in, hidden_dim)
+        self.linear_feat2 = nn.Linear(hidden_dim, dim_in)
+        self.dropout_feat1 = nn.Dropout(dropout)
+        self.dropout_feat2 = nn.Dropout(dropout)
+        self.norm_feat = nn.LayerNorm(dim_in)
+
+        self.norm1 = nn.LayerNorm(dim_in)
+        self.norm2 = nn.LayerNorm(dim_in)
+
+        self.dropout1 = nn.Dropout(dropout)
+        self.dropout2 = nn.Dropout(dropout)
+        self.activation = F.relu
+
+    def _update_track_embedding_trt(self, track_instances):
+        dim = 512 #track_instances[0].shape[1]
+        out_embed = track_instances[2]
+        query_pos = track_instances[0][:, :dim // 2]
+        query_feat = track_instances[0][:, dim // 2:]
+        q = query_pos + out_embed
+        k = query_pos + out_embed
+
+        # attention
+        tgt = out_embed
+        tgt2 = self.self_attn(q[:, None], k[:, None], value=tgt[:, None])[0][:,
+                                                                             0]
+        tgt = tgt + self.dropout1(tgt2)
+        tgt = self.norm1(tgt)
+
+        # ffn
+        tgt2 = self.linear2(self.dropout(self.activation(self.linear1(tgt))))
+        tgt = tgt + self.dropout2(tgt2)
+        tgt = self.norm2(tgt)
+
+        if self.update_query_pos:
+            query_pos2 = self.linear_pos2(
+                self.dropout_pos1(self.activation(self.linear_pos1(tgt))))
+            query_pos = query_pos + self.dropout_pos2(query_pos2)
+            query_pos = self.norm_pos(query_pos)
+            track_instances[0][:, :dim // 2] = query_pos
+
+        query_feat2 = self.linear_feat2(
+            self.dropout_feat1(self.activation(self.linear_feat1(tgt))))
+        query_feat = query_feat + self.dropout_feat2(query_feat2)
+        query_feat = self.norm_feat(query_feat)
+        track_instances[0][:, dim // 2:] = query_feat
+
+        return track_instances
+
+    def _select_active_tracks_trt(self, track_instances):
+        active_index = index_bool2long_trt(track_instances[3] >= 0)
+        active_track_instances = []
+        for item in track_instances:
+            active_track_instances.append(item[active_index])
+
+        return active_track_instances
+
+    def forward_trt(self, init_track_instances, track_instances):
+        active_track_instances = self._update_track_embedding_trt(
+            self._select_active_tracks_trt(track_instances))
+        merged_track_instances = []
+        for i in range(len(track_instances)):
+            merged_track_instances.append(torch.cat((init_track_instances[i], 
+                                                     active_track_instances[i]), dim=0))
+        return merged_track_instances
\ No newline at end of file
diff --git a/projects/mmdet3d_plugin/uniad/dense_heads/track_head_plugin/tracker.py b/projects/mmdet3d_plugin/uniad/dense_heads/track_head_plugin/tracker.py
index 55e45f5..fd80ddd 100644
--- a/projects/mmdet3d_plugin/uniad/dense_heads/track_head_plugin/tracker.py
+++ b/projects/mmdet3d_plugin/uniad/dense_heads/track_head_plugin/tracker.py
@@ -1,6 +1,10 @@
 from .track_instance import Instances
-from mmdet3d.core.bbox.iou_calculators.iou3d_calculator import (
+import sys
+# sys.path.insert(1, '/path/to/UniAD_tensorrt')
+from third_party.uniad_mmdet3d.core.bbox.iou_calculators.iou3d_calculator import (
     bbox_overlaps_nearest_3d as iou_3d, )
+# from mmdet3d.core.bbox.iou_calculators.iou3d_calculator import (
+#     bbox_overlaps_nearest_3d as iou_3d, )
 from projects.mmdet3d_plugin.core.bbox.util import denormalize_bbox
 
 class RuntimeTrackerBase(object):
@@ -21,7 +25,8 @@ class RuntimeTrackerBase(object):
                 and track_instances.scores[i] >= self.score_thresh
             ):  
                 if iou_thre is not None and track_instances.pred_boxes[track_instances.obj_idxes>=0].shape[0]!=0:
-                    iou3ds = iou_3d(denormalize_bbox(track_instances.pred_boxes[i].unsqueeze(0), None)[...,:7], denormalize_bbox(track_instances.pred_boxes[track_instances.obj_idxes>=0], None)[...,:7])
+                    iou3ds = iou_3d(denormalize_bbox(track_instances.pred_boxes[i].unsqueeze(0), None)[...,:7], 
+                                    denormalize_bbox(track_instances.pred_boxes[track_instances.obj_idxes>=0], None)[...,:7])
                     if iou3ds.max()>iou_thre:
                         continue
                 # new track
diff --git a/projects/mmdet3d_plugin/uniad/detectors/uniad_e2e.py b/projects/mmdet3d_plugin/uniad/detectors/uniad_e2e.py
index 78a4a87..e339d84 100644
--- a/projects/mmdet3d_plugin/uniad/detectors/uniad_e2e.py
+++ b/projects/mmdet3d_plugin/uniad/detectors/uniad_e2e.py
@@ -10,7 +10,7 @@ from mmdet.models import DETECTORS
 import copy
 import os
 from ..dense_heads.seg_head_plugin import IOU
-from .uniad_track import UniADTrack
+from .uniad_track import UniADTrack, UniADTrackTRT
 from mmdet.models.builder import build_head
 
 @DETECTORS.register_module()
@@ -250,10 +250,12 @@ class UniAD(UniADTrack):
                      gt_segmentation=None,
                      gt_instance=None, 
                      gt_occ_img_is_valid=None,
+                    #  export=False,
                      **kwargs
                     ):
         """Test function
         """
+        
         for var, name in [(img_metas, 'img_metas')]:
             if not isinstance(var, list):
                 raise TypeError('{} must be a list, but got {}'.format(
@@ -361,3 +363,292 @@ def pop_elem_in_result(task_result:dict, pop_list:list=None):
         for pop_k in pop_list:
             task_result.pop(pop_k, None)
     return task_result
+
+@DETECTORS.register_module()
+class UniADTRT(UniADTrackTRT):
+    def __init__(
+        self,
+        seg_head=None,
+        motion_head=None,
+        occ_head=None,
+        planning_head=None,
+        task_loss_weight=dict(
+            track=1.0,
+            map=1.0,
+            motion=1.0,
+            occ=1.0,
+            planning=1.0
+        ),
+        **kwargs,
+    ):
+        super(UniADTRT, self).__init__(**kwargs)
+        if seg_head:
+            self.seg_head = build_head(seg_head)
+        if occ_head:
+            self.occ_head = build_head(occ_head)
+        if motion_head:
+            self.motion_head = build_head(motion_head)
+        if planning_head:
+            self.planning_head = build_head(planning_head)
+        
+        self.task_loss_weight = task_loss_weight
+        assert set(task_loss_weight.keys()) == \
+               {'track', 'occ', 'motion', 'map', 'planning'}
+
+    @property
+    def with_planning_head(self):
+        return hasattr(self, 'planning_head') and self.planning_head is not None
+    
+    @property
+    def with_occ_head(self):
+        return hasattr(self, 'occ_head') and self.occ_head is not None
+
+    @property
+    def with_motion_head(self):
+        return hasattr(self, 'motion_head') and self.motion_head is not None
+
+    @property
+    def with_seg_head(self):
+        return hasattr(self, 'seg_head') and self.seg_head is not None
+
+    def forward_uniad_trt(self,
+                    prev_track_intances0,
+                    prev_track_intances1,
+                    prev_track_intances2,
+                    prev_track_intances3,
+                    prev_track_intances4,
+                    prev_track_intances5,
+                    prev_track_intances6,
+                    prev_track_intances7,
+                    prev_track_intances8,
+                    prev_track_intances9,
+                    prev_track_intances10,
+                    prev_track_intances11,
+                    prev_track_intances12,
+                    prev_track_intances13,
+                    prev_timestamp,
+                    prev_l2g_r_mat,
+                    prev_l2g_t, 
+
+                    prev_bev,
+                    gt_lane_labels, 
+                    gt_lane_masks,
+                    gt_segmentation,
+                    img_metas_scene_token,#
+                    timestamp,
+                    l2g_r_mat, 
+                    l2g_t, 
+                    img=None,
+                    img_metas_can_bus=None,
+                    img_metas_lidar2img=None,
+                    image_shape=None,
+                    command=None,
+                    use_prev_bev=1.0,
+                    max_obj_id=0,
+                    ):
+        scene_token_changed = 1-use_prev_bev
+        prev_track_intances = [
+            prev_track_intances0,
+            prev_track_intances1,
+            prev_track_intances2,
+            prev_track_intances3,
+            prev_track_intances4,
+            prev_track_intances5,
+            prev_track_intances6,
+            prev_track_intances7,
+            prev_track_intances8,
+            prev_track_intances9,
+            prev_track_intances10,
+            prev_track_intances11,
+            prev_track_intances12,
+            prev_track_intances13,
+        ]
+        (
+        prev_track_instances_out,
+        prev_timestamp_out,
+        prev_l2g_t_out, 
+        prev_l2g_r_mat_out,
+        bev_embed, 
+        bev_pos,
+        output_classes, 
+        output_coords, 
+        all_past_traj_preds,
+        last_ref_pts, 
+        query_feats,
+
+        track_query_embeddings, 
+        track_query_matched_idxes, 
+        bboxes_dict_bboxes,
+        bboxes_gravity_center,
+        bboxes_yaw,
+        scores,
+        labels,
+        track_scores,
+        bbox_index,
+        obj_idxes,
+        mask,
+        track_bbox_results1,
+        track_bbox_results2,
+
+        sdc_bboxes_dict_bboxes,
+        sdc_boxes_3d_gravity_center,
+        sdc_boxes_3d_yaw,  
+        sdc_scores_3d,
+        sdc_track_scores,
+        sdc_track_bbox_results1,
+        sdc_track_bbox_results2,
+        sdc_embedding,
+        max_obj_id_out,
+
+        track_instances_fordet,
+        )=self.simple_test_track_trt(
+            prev_track_intances,
+            prev_timestamp,
+            prev_l2g_r_mat,
+            prev_l2g_t, 
+            img_metas_can_bus, 
+            img_metas_lidar2img, 
+            # img_metas_scene_token,#
+            scene_token_changed,
+            timestamp,
+            l2g_r_mat, 
+            l2g_t, 
+            image_shape,
+            prev_bev,
+            max_obj_id,
+            img,
+            use_prev_bev,
+            )
+
+        (
+        bbox_pred,
+        seg_pred,
+        labels_pred,
+        drivable_pred,
+        score_pred,
+        lane_pred,
+        lane_score_pred,
+        stuff_score_pred, 
+        drivable_intersection,
+        drivable_union,
+        lanes_intersection,
+        lanes_union,
+        divider_intersection,
+        divider_union,
+        crossing_intersection,
+        crossing_union,
+        contour_intersection,
+        contour_union,
+        drivable_iou,
+        lanes_iou,
+        divider_iou,
+        crossing_iou,
+        contour_iou,
+        memory,
+        memory_mask,
+        memory_pos,
+        lane_query,
+        lane_query_pos,
+        hw_lvl0,
+        reference,
+        results,
+        ori_shape
+        ) =  self.seg_head.forward_test_trt(
+                                bev_embed, 
+                                gt_lane_labels, 
+                                gt_lane_masks)
+          
+        bev_embed = bev_embed.float().detach()
+        bev_pos = bev_pos.float().detach()
+        lane_query = lane_query.float().detach()
+        lane_query_pos = lane_query_pos.float().detach()
+        gt_segmentation = gt_segmentation.detach()
+        command = command.detach()
+        track_query_embeddings = track_query_embeddings.float().detach()
+        track_bbox_results1 = track_bbox_results1.float().detach()
+        track_bbox_results2 = track_bbox_results2.long().detach()
+        bboxes_gravity_center = bboxes_gravity_center.float().detach()
+        bboxes_yaw = bboxes_yaw.float().detach()
+        sdc_embedding = sdc_embedding.float().detach()
+        sdc_boxes_3d_gravity_center =  sdc_boxes_3d_gravity_center.float().detach()
+        sdc_boxes_3d_yaw = sdc_boxes_3d_yaw.float().detach()
+        sdc_track_bbox_results1 = sdc_track_bbox_results1.float().detach()
+        sdc_track_bbox_results2 = sdc_track_bbox_results2.long().detach()
+        outputs_traj_scores,\
+        outputs_trajs,\
+        valid_traj_masks,\
+        inter_states,\
+        out_track_query,\
+        track_query_pos,\
+        sdc_traj_query,\
+        sdc_track_query,\
+        sdc_track_query_pos, \
+        track_scores = \
+            self.motion_head.forward_test_trt(
+                bev_embed, 
+                track_query_embeddings, 
+                track_bbox_results1, 
+                track_bbox_results2, 
+                bboxes_gravity_center, 
+                bboxes_yaw,
+                sdc_embedding, 
+                sdc_boxes_3d_gravity_center, 
+                sdc_boxes_3d_yaw, 
+                sdc_track_bbox_results1,
+                sdc_track_bbox_results2, 
+                lane_query,
+                lane_query_pos)
+        seg_gt, seg_out =\
+            self.occ_head.forward_test_trt(
+            bev_embed, 
+            out_track_query, 
+            track_query_pos, 
+            inter_states,
+            gt_segmentation=gt_segmentation,
+            track_scores = track_scores,
+        )
+        outs_planning = self.planning_head.forward_test_trt(
+        bev_embed, sdc_traj_query, sdc_track_query,
+                        bev_pos, seg_out, [command.long()])
+        (
+            prev_track_intances0_out,
+            prev_track_intances1_out,
+            prev_track_intances2_out,
+            prev_track_intances3_out,
+            prev_track_intances4_out,
+            prev_track_intances5_out,
+            prev_track_intances6_out,
+            prev_track_intances7_out,
+            prev_track_intances8_out,
+            prev_track_intances9_out,
+            prev_track_intances10_out,
+            prev_track_intances11_out,
+            prev_track_intances12_out,
+            prev_track_intances13_out,
+        )  = prev_track_instances_out
+
+        return (prev_track_intances0_out,
+                prev_track_intances1_out,
+                # prev_track_intances2_out,
+                prev_track_intances3_out,
+                prev_track_intances4_out,
+                prev_track_intances5_out,
+                prev_track_intances6_out,
+                # prev_track_intances7_out,
+                prev_track_intances8_out,
+                prev_track_intances9_out,
+                # prev_track_intances10_out,
+                prev_track_intances11_out,
+                prev_track_intances12_out,
+                prev_track_intances13_out,
+                prev_timestamp_out,
+                prev_l2g_t_out, 
+                prev_l2g_r_mat_out,
+                bev_embed,  
+                bboxes_dict_bboxes,
+                scores,
+                labels.int(),
+                bbox_index.int(),
+                obj_idxes.int(),
+                max_obj_id_out.int(),
+                outs_planning)
diff --git a/projects/mmdet3d_plugin/uniad/detectors/uniad_track.py b/projects/mmdet3d_plugin/uniad/detectors/uniad_track.py
index 577ae38..7fece7e 100644
--- a/projects/mmdet3d_plugin/uniad/detectors/uniad_track.py
+++ b/projects/mmdet3d_plugin/uniad/detectors/uniad_track.py
@@ -8,9 +8,11 @@ import torch
 import torch.nn as nn
 from mmcv.runner import auto_fp16
 from mmdet.models import DETECTORS
-from mmdet3d.core import bbox3d2result
-from mmdet3d.core.bbox.coders import build_bbox_coder
-from mmdet3d.models.detectors.mvx_two_stage import MVXTwoStageDetector
+# from mmdet3d.core import bbox3d2result
+# from mmdet3d.core.bbox.coders import build_bbox_coder
+from mmdet.core.bbox import build_bbox_coder
+# from mmdet3d.models.detectors.mvx_two_stage import MVXTwoStageDetector
+from third_party.uniad_mmdet3d.models.detectors.mvx_two_stage import MVXTwoStageDetector
 from projects.mmdet3d_plugin.models.utils.grid_mask import GridMask
 import copy
 import math
@@ -18,7 +20,13 @@ from projects.mmdet3d_plugin.core.bbox.util import normalize_bbox
 from mmdet.models import build_loss
 from einops import rearrange
 from mmdet.models.utils.transformer import inverse_sigmoid
-from ..dense_heads.track_head_plugin import MemoryBank, QueryInteractionModule, Instances, RuntimeTrackerBase
+from ..dense_heads.track_head_plugin import MemoryBank, QueryInteractionModule, Instances, RuntimeTrackerBase, MemoryBankTRTP, QueryInteractionModuleTRTP 
+from third_party.uniad_mmdet3d.core.bbox.iou_calculators.iou3d_calculator import (
+            bbox_overlaps_nearest_3d as iou_3d, )
+from third_party.uniad_mmdet3d.core.bbox.structures import (
+            LiDARInstance3DBoxes)
+from projects.mmdet3d_plugin.core.bbox.util import denormalize_bbox_trt, denormalize_bbox
+from projects.mmdet3d_plugin.uniad.functions import inverse
 
 @DETECTORS.register_module()
 class UniADTrack(MVXTwoStageDetector):
@@ -160,6 +168,7 @@ class UniADTrack(MVXTwoStageDetector):
         if self.use_grid_mask:
             img = self.grid_mask(img)
         img_feats = self.img_backbone(img)
+
         if isinstance(img_feats, dict):
             img_feats = list(img_feats.values())
         if self.with_img_neck:
@@ -245,7 +254,6 @@ class UniADTrack(MVXTwoStageDetector):
         Outs:
             ref_pts (Tensor): (num_query, 3).  in inevrse sigmoid space
         """
-        # print(l2g_r1.type(), l2g_t1.type(), ref_pts.type())
         time_delta = time_delta.type(torch.float)
         num_query = ref_pts.size(0)
         velo_pad_ = velocity.new_zeros((num_query, 1))
@@ -340,6 +348,7 @@ class UniADTrack(MVXTwoStageDetector):
             prev_bev = self.get_history_bev(prev_img, prev_img_metas)
 
         img_feats = self.extract_img_feat(img=imgs)
+        
         if self.freeze_bev_encoder:
             with torch.no_grad():
                 bev_embed, bev_pos = self.pts_bbox_head.get_bev_features(
@@ -658,6 +667,7 @@ class UniADTrack(MVXTwoStageDetector):
             ref_points=track_instances.ref_pts,
             img_metas=img_metas,
         )
+        
         output_classes = det_output["all_cls_scores"]
         output_coords = det_output["all_bbox_preds"]
         last_ref_pts = det_output["last_ref_points"]
@@ -716,7 +726,6 @@ class UniADTrack(MVXTwoStageDetector):
         """only support bs=1 and sequential input"""
 
         bs = img.size(0)
-        # img_metas = img_metas[0]
 
         """ init track instances for first frame """
         if (
@@ -779,10 +788,8 @@ class UniADTrack(MVXTwoStageDetector):
             track_scores=track_instances.scores,
             obj_idxes=track_instances.obj_idxes,
         )
-        # bboxes_dict = self.bbox_coder.decode(bbox_dict, with_mask=with_mask)[0]
         bboxes_dict = self.bbox_coder.decode(bbox_dict, with_mask=with_mask, img_metas=img_metas)[0]
         bboxes = bboxes_dict["bboxes"]
-        # bboxes[:, 2] = bboxes[:, 2] - bboxes[:, 5] * 0.5
         bboxes = img_metas[0]["box_type_3d"](bboxes, 9)
         labels = bboxes_dict["labels"]
         scores = bboxes_dict["scores"]
@@ -848,3 +855,995 @@ class UniADTrack(MVXTwoStageDetector):
 
         return [result_dict]
 
+
+@DETECTORS.register_module()
+class UniADTrackTRT(UniADTrack):
+    """UniAD tracking part
+    """
+    def __init__(
+        self, 
+        use_grid_mask=False,
+        img_backbone=None,
+        img_neck=None,
+        pts_bbox_head=None,
+        train_cfg=None,
+        test_cfg=None,
+        pretrained=None,
+        video_test_mode=False,
+        loss_cfg=None,
+        qim_args=dict(
+            qim_type="QIMBase",
+            merger_dropout=0,
+            update_query_pos=False,
+            fp_ratio=0.3,
+            random_drop=0.1,
+        ),
+        mem_args=dict(
+            memory_bank_type="MemoryBank",
+            memory_bank_score_thresh=0.0,
+            memory_bank_len=4,
+        ),
+        bbox_coder=dict(
+            type="DETRTrack3DCoder",
+            post_center_range=[-61.2, -61.2, -10.0, 61.2, 61.2, 10.0],
+            pc_range=[-51.2, -51.2, -5.0, 51.2, 51.2, 3.0],
+            max_num=300,
+            num_classes=10,
+            score_threshold=0.0,
+            with_nms=False,
+            iou_thres=0.3,
+        ),
+        pc_range=None,
+        embed_dims=256,
+        num_query=900,
+        num_classes=10,
+        vehicle_id_list=None,
+        score_thresh=0.2,
+        filter_score_thresh=0.1,
+        miss_tolerance=5,
+        gt_iou_threshold=0.0,
+        freeze_img_backbone=False,
+        freeze_img_neck=False,
+        freeze_bn=False,
+        freeze_bev_encoder=False,
+        queue_length=3,
+    ):
+        super(UniADTrack, self).__init__(
+            img_backbone=img_backbone,
+            img_neck=img_neck,
+            pts_bbox_head=pts_bbox_head,
+            train_cfg=train_cfg,
+            test_cfg=test_cfg,
+            pretrained=pretrained,
+        )
+        self.grid_mask = GridMask(
+            True, True, rotate=1, offset=False, ratio=0.5, mode=1, prob=0.7
+        )
+        self.use_grid_mask = use_grid_mask
+        self.fp16_enabled = False
+        self.embed_dims = embed_dims
+        self.num_query = num_query
+        self.num_classes = num_classes
+        self.vehicle_id_list = vehicle_id_list
+        self.pc_range = pc_range
+        self.queue_length = queue_length
+        if freeze_img_backbone:
+            if freeze_bn:
+                self.img_backbone.eval()
+            for param in self.img_backbone.parameters():
+                param.requires_grad = False
+        
+        if freeze_img_neck:
+            if freeze_bn:
+                self.img_neck.eval()
+            for param in self.img_neck.parameters():
+                param.requires_grad = False
+
+        # temporal
+        self.video_test_mode = video_test_mode
+        assert self.video_test_mode
+
+        self.prev_frame_info = {
+            "prev_bev": None,
+            "scene_token": None,
+            "prev_pos": 0,
+            "prev_angle": 0,
+        }
+        self.query_embedding = nn.Embedding(self.num_query+1, self.embed_dims * 2)   # the final one is ego query, which constantly models ego-vehicle
+        self.reference_points = nn.Linear(self.embed_dims, 3)
+
+        self.mem_bank_len = mem_args["memory_bank_len"]
+        self.track_base = RuntimeTrackerBase(
+            score_thresh=score_thresh,
+            filter_score_thresh=filter_score_thresh,
+            miss_tolerance=miss_tolerance,
+        )  # hyper-param for removing inactive queries
+
+        self.query_interact = QueryInteractionModuleTRTP(
+            qim_args,
+            dim_in=embed_dims,
+            hidden_dim=embed_dims,
+            dim_out=embed_dims,
+        )
+
+        self.bbox_coder = build_bbox_coder(bbox_coder)
+
+        self.memory_bank = MemoryBankTRTP(
+            mem_args,
+            dim_in=embed_dims,
+            hidden_dim=embed_dims,
+            dim_out=embed_dims,
+        )
+        def _freeze_model(model):
+            for param in model.parameters():
+                param.requires_grad = False
+        _freeze_model(self.memory_bank)
+        _freeze_model(self.query_interact)
+        _freeze_model(self.query_embedding)
+        _freeze_model(self.reference_points)
+
+        self.mem_bank_len = (
+            0 if self.memory_bank is None else self.memory_bank.max_his_length
+        )
+        self.criterion = build_loss(loss_cfg)
+        self.gt_iou_threshold = gt_iou_threshold
+        self.bev_h, self.bev_w = self.pts_bbox_head.bev_h, self.pts_bbox_head.bev_w
+        self.freeze_bev_encoder = freeze_bev_encoder
+        self.inverse = inverse
+
+    # Generate bev using bev_encoder in BEVFormer
+    def get_bevs_trt(self, imgs, can_bus, lidar2img, image_shape, prev_bev=None, use_prev_bev=None, ):
+        img_feats = self.extract_img_feat(img=imgs)
+        image_shape = torch.tensor(imgs.shape[-2:]).to(imgs)
+        bev_embed, bev_pos = self.pts_bbox_head.get_bev_features_trt(
+            img_feats, can_bus, lidar2img, image_shape, prev_bev, use_prev_bev)
+        
+        if bev_embed.shape[1] == self.bev_h * self.bev_w:
+            bev_embed = bev_embed.permute(1, 0, 2)
+        
+        assert bev_embed.shape[0] == self.bev_h * self.bev_w
+        return bev_embed, bev_pos
+
+
+    def _forward_single_frame_inference_preprocess_trt_v2(
+        self,
+        track_instances,
+        l2g_r1=None,
+        l2g_t1=None,
+        l2g_r2=None,
+        l2g_t2=None,
+        time_delta=None,
+        use_prev_bev=1.0,
+    ):
+        active_index = track_instances[3] >= 0
+        active_index = self.index_bool2long_trt(active_index)
+        active_inst = []
+        for item in track_instances:
+            active_inst.append(item[active_index])
+        other_index = track_instances[3] < 0
+        other_index = self.index_bool2long_trt(other_index)
+        other_inst = []
+        for item in track_instances:
+            other_inst.append(item[other_index])
+
+        condition = use_prev_bev
+
+
+        active_inst_query = active_inst[0]
+        active_inst_velo = active_inst[-5][:, -2:]
+        active_inst_ref_pts = active_inst[1]
+        active_inst_ref_pts = condition*self.velo_update_trt(
+                                        active_inst_ref_pts,
+                                        active_inst_velo,
+                                        l2g_r1,
+                                        l2g_t1,
+                                        l2g_r2,
+                                        l2g_t2,
+                                        time_delta=time_delta
+                                    )[0]+\
+                                    (1-condition)*active_inst_ref_pts
+        dim = 256 # active_inst_query.shape[-1]//2
+        sliced_active_inst_query = active_inst_query[..., :dim]
+        query_generated_ref_pts = self.reference_points(sliced_active_inst_query)
+        active_inst[1] = query_generated_ref_pts*condition\
+                            +active_inst[1]*(1-condition)
+        active_inst[1][...,:2] = active_inst_ref_pts[...,:2]*condition\
+                                + active_inst[1][...,:2]*(1-condition)
+
+        for i in range(len(track_instances)):
+            track_instances[i] = torch.cat((other_inst[i], active_inst[i]), dim=0)
+        query = track_instances[0]
+        ref_pts = track_instances[1]
+
+        return track_instances, query, ref_pts
+
+    def velo_update_trt(
+        self, ref_pts, velocity, l2g_r1, l2g_t1, l2g_r2, l2g_t2, time_delta
+    ):
+        """
+        Args:
+            ref_pts (Tensor): (num_query, 3).  in inevrse sigmoid space
+            velocity (Tensor): (num_query, 2). m/s
+                in lidar frame. vx, vy
+            global2lidar (np.Array) [4,4].
+        Outs:
+            ref_pts (Tensor): (num_query, 3).  in inevrse sigmoid space
+        """
+        time_delta = time_delta.float()
+        num_query = ref_pts.size(0)
+        velo_pad_ = velocity.new_zeros((num_query, 1))
+        velo_pad = torch.cat((velocity, velo_pad_), dim=-1)
+
+        reference_points = ref_pts.sigmoid().clone()
+        pc_range = self.pc_range
+        reference_points[..., 0:1] = (
+            reference_points[..., 0:1] * (pc_range[3] - pc_range[0]) + pc_range[0]
+        )
+        reference_points[..., 1:2] = (
+            reference_points[..., 1:2] * (pc_range[4] - pc_range[1]) + pc_range[1]
+        )
+        reference_points[..., 2:3] = (
+            reference_points[..., 2:3] * (pc_range[5] - pc_range[2]) + pc_range[2]
+        )
+
+        reference_points = reference_points + velo_pad * time_delta
+        ref_pts = reference_points @ l2g_r1 + l2g_t1 - l2g_t2
+
+        g2l_r = self.inverse(l2g_r2[None,...])[0].float()
+
+        ref_pts = ref_pts @ g2l_r
+
+        ref_pts[..., 0:1] = (ref_pts[..., 0:1] - pc_range[0]) / (
+            pc_range[3] - pc_range[0]
+        )
+        ref_pts[..., 1:2] = (ref_pts[..., 1:2] - pc_range[1]) / (
+            pc_range[4] - pc_range[1]
+        )
+        ref_pts[..., 2:3] = (ref_pts[..., 2:3] - pc_range[2]) / (
+            pc_range[5] - pc_range[2]
+        )
+
+        ref_pts = inverse_sigmoid(ref_pts)
+
+        return ref_pts
+
+    def _forward_single_frame_inference_trt(
+        self,
+        img,
+        can_bus, 
+        lidar2img, 
+        image_shape,
+        track_instances,
+        prev_bev=None,
+        l2g_r1=None,
+        l2g_t1=None,
+        l2g_r2=None,
+        l2g_t2=None,
+        time_delta=None,
+        use_prev_bev=None,
+    ):
+        """
+        # track_instances = (
+        #     query,0
+        #     ref_pts,1
+        #     output_embedding,2
+        #     obj_idxes,3
+        #     matched_gt_idxes,4
+        #     disappear_time,5
+        #     iou,6
+        #     scores,7
+        #     track_scores,-6
+        #     pred_boxes,-5
+        #     pred_logits,-4
+        #     mem_bank,-3
+        #     mem_padding_mask,-2
+        #     save_period,-1
+        # )
+        """
+        (track_instances, 
+        query, 
+        ref_pts)=self._forward_single_frame_inference_preprocess_trt_v2(
+            track_instances,
+            l2g_r1,
+            l2g_t1,
+            l2g_r2,
+            l2g_t2,
+            time_delta,
+            use_prev_bev)
+        bev_embed, bev_pos = \
+            self.get_bevs_trt(img, 
+                              can_bus, 
+                              lidar2img, 
+                              image_shape, 
+                              prev_bev=prev_bev,
+                              use_prev_bev=use_prev_bev
+                              )
+        output_classes, output_coords, all_past_traj_preds,\
+        last_ref_pts, query_feats = \
+            self.pts_bbox_head.get_detections_trt(
+                bev_embed, 
+                object_query_embeds=query,
+                ref_points=ref_pts,
+                )        
+        return (
+            track_instances,
+            bev_embed, bev_pos,
+            output_classes, output_coords, all_past_traj_preds,
+            last_ref_pts, query_feats
+        )
+
+    def track2mop_trt(self, track_instances, output_classes, 
+                      output_coords, last_ref_pts, query_feats, max_obj_id):
+        track_instances[7] = output_classes[-1, 0, :].sigmoid().max(dim=-1).values
+        track_instances[-4] = output_classes[-1, 0]
+        track_instances[-5] = output_coords[-1, 0]
+        track_instances[2] = query_feats[-1][0]
+        track_instances[1] = last_ref_pts[0]
+        track_instances[3][900] = -2
+        track_instances[3], track_instances[5], max_obj_id = \
+            track_base_update_trt_script_v4(
+            track_instances[3], track_instances[5],track_instances[7], 
+            track_instances[-5].shape[0], self.track_base.score_thresh, 
+            self.track_base.filter_score_thresh, 
+            max_obj_id, self.track_base.miss_tolerance)        
+        
+        # filter out sleep objects
+        active_index = (track_instances[3]>=0) & (track_instances[7] >= 
+                                                  self.track_base.filter_score_thresh)    
+        
+        active_index = self.index_bool2long_trt(active_index)
+
+        (
+        track_query_embeddings, 
+        track_query_matched_idxes, 
+        bboxes_dict_bboxes,
+        bboxes_gravity_center,
+        bboxes_yaw,
+        scores,
+        labels,
+        track_scores,
+        bbox_index,
+        obj_idxes,
+        mask,
+        track_bbox_results1,
+        track_bbox_results2,
+        ) = self.select_active_track_query_trt(
+            track_instances, active_index)
+        
+        sdc_active_index = track_instances[3]==-2
+        sdc_active_index = self.index_bool2long_trt(sdc_active_index)
+        sdc_track_instances = []
+        for item in track_instances:
+            sdc_track_instances.append(item[sdc_active_index])
+
+        (sdc_bboxes_dict_bboxes,
+        sdc_boxes_3d_gravity_center,
+        sdc_boxes_3d_yaw, 
+        sdc_scores_3d,
+        sdc_track_scores,
+        sdc_track_bbox_results1,
+        sdc_track_bbox_results2,
+        sdc_embedding)=self.select_sdc_track_query_trt(sdc_track_instances)
+        if self.memory_bank is not None:
+            track_instances[2], track_instances[-2], track_instances[-1], track_instances[-3] = self.memory_bank.forward_trt(
+                    track_instances[-2], 
+                    track_instances[2], 
+                    track_instances[-3], 
+                    track_instances[7], 
+                    track_instances[-1],
+                    )
+        query_interact_track_instances = self.query_interact.forward_trt(self._generate_empty_tracks_trt(), 
+                                                                         track_instances)
+
+        return (
+                track_query_embeddings, 
+                track_query_matched_idxes, 
+                bboxes_dict_bboxes,
+                bboxes_gravity_center,
+                bboxes_yaw,
+                scores,
+                labels,
+                track_scores,
+                bbox_index,
+                obj_idxes,
+                mask,
+                track_bbox_results1,
+                track_bbox_results2,
+
+                sdc_bboxes_dict_bboxes,
+                sdc_boxes_3d_gravity_center,
+                sdc_boxes_3d_yaw,  
+                sdc_scores_3d,
+                sdc_track_scores,
+                sdc_track_bbox_results1,
+                sdc_track_bbox_results2,
+                sdc_embedding,
+                max_obj_id,
+
+                track_instances,
+                query_interact_track_instances, # final track_instances
+                )
+    
+    @staticmethod
+    def index_bool2long_trt(bool_index):
+        # Convert Boolean Tensor to Long Tensor:
+        long_index = bool_index.long()
+        # Create a Range Tensor and Multiply by the Long Tensor
+        long_index = torch.arange(1, long_index.shape[-1]+1
+                                            , device=long_index.device)*long_index
+        # Extract Non-Zero Indices
+        long_index = long_index[long_index.nonzero(as_tuple=True)[0]]
+        # Adjust Indices to Be Zero-Based
+        long_index = long_index - torch.ones_like(long_index)
+        return long_index
+
+    def select_active_track_query_trt(self, track_instances, active_index, with_mask=True):
+        activate_track_instances = []
+        for item in track_instances:
+            activate_track_instances.append(item[active_index])
+        (
+        bboxes_dict_bboxes,
+        bboxes_gravity_center,
+        bboxes_yaw,
+        scores,
+        labels,
+        track_scores,
+        bbox_index,
+        obj_idxes,
+        mask,
+        track_bbox_results1,
+        track_bbox_results2,
+        ) = self._track_instances2results_trt(activate_track_instances,
+                                                              with_mask=with_mask)
+        mask = self.index_bool2long_trt(mask.bool())
+        track_query_embeddings = track_instances[2][active_index][bbox_index][mask]
+        track_query_matched_idxes = track_instances[4][active_index][bbox_index][mask]
+        return (track_query_embeddings, 
+                track_query_matched_idxes, 
+                bboxes_dict_bboxes,
+                bboxes_gravity_center,
+                bboxes_yaw,
+                scores,
+                labels,
+                track_scores,
+                bbox_index,
+                obj_idxes,
+                mask,
+                track_bbox_results1,
+                track_bbox_results2,)
+    
+    def select_sdc_track_query_trt(self, sdc_instance):
+        (
+         sdc_bboxes_dict_bboxes,
+        sdc_boxes_3d_gravity_center,
+        sdc_boxes_3d_yaw,
+        sdc_scores_3d,
+        labels,
+        sdc_track_scores,
+        bbox_index,
+        obj_idxes,
+        bboxes_dict_mask,
+        sdc_track_bbox_results1,
+        sdc_track_bbox_results2,) = self._track_instances2results_trt(sdc_instance, 
+                                                                with_mask=False)
+        sdc_embedding = sdc_instance[2][0]
+        return (sdc_bboxes_dict_bboxes,
+            sdc_boxes_3d_gravity_center,
+                sdc_boxes_3d_yaw, 
+                sdc_scores_3d,
+                sdc_track_scores,
+                sdc_track_bbox_results1,
+                sdc_track_bbox_results2,
+                sdc_embedding)
+    
+    def _track_instances2results_trt(self, track_instances, with_mask=True):
+        bbox_list = [
+            track_instances[-4],
+            track_instances[-5],
+            track_instances[7],
+            track_instances[3],
+        ]
+        
+        (bboxes_dict_bboxes,
+         bboxes_dict_scores,
+         bboxes_dict_labels,
+         bboxes_dict_track_scores,
+         bboxes_dict_obj_idxes,
+         bboxes_dict_bbox_index,
+         bboxes_dict_mask) = self.bbox_coder_decode_trt(bbox_list, 
+                                                    with_mask=with_mask)
+        bottom_center = bboxes_dict_bboxes[:, :3]
+        bboxes_gravity_center = torch.zeros_like(bottom_center)
+        mask = torch.zeros_like(bottom_center)
+        mask[:, 0:2]=torch.ones_like(mask[:, 0:2])
+        bboxes_gravity_center = bboxes_gravity_center*(1-mask) + bottom_center*mask
+        mask = torch.zeros_like(bottom_center)
+        mask[:, 2]=torch.ones_like(mask[:, 2])
+        bboxes_gravity_center = bboxes_gravity_center*(1-mask)+bottom_center*mask + bboxes_dict_bboxes[:, 3:6]*mask * 0.5
+        bboxes_yaw = bboxes_dict_bboxes[:, 6]
+        labels = bboxes_dict_labels
+        scores = bboxes_dict_scores
+        bbox_index = bboxes_dict_bbox_index
+
+        track_scores = bboxes_dict_track_scores
+        obj_idxes = bboxes_dict_obj_idxes
+        return (
+            bboxes_dict_bboxes,
+            bboxes_gravity_center,
+            bboxes_yaw,
+            scores,
+            labels,
+            track_scores,
+            bbox_index,
+            obj_idxes,
+            bboxes_dict_mask,
+            scores,
+            labels,
+        )
+
+    def bbox_coder_decode_trt(self, 
+                              bbox_list, 
+                              with_mask=True):
+        all_cls_scores = bbox_list[0]
+        all_bbox_preds = bbox_list[1]
+        track_scores = bbox_list[2]
+        obj_idxes = bbox_list[3]
+        return self.decode_single_trt(
+            all_cls_scores, all_bbox_preds,
+            track_scores, obj_idxes, with_mask)
+    
+    def decode_single_trt(self, cls_scores, bbox_preds, 
+                      track_scores, obj_idxes, 
+                      with_mask=True):
+
+        # method4: original one, not work, onnx export will only consider cls_scores.size(0)
+        # max_num = min(cls_scores.size(0), self.bbox_coder.max_num)
+        # max_num = min(max_num, track_scores.shape[-1])
+
+        # method5: works, topk all and slice 300 later, no need if-else in pytorch, onnx and tensorrt.
+        # but maybe it has higher time complexity because topk all => sort all.
+        # max_num = cls_scores.size(0)
+
+        # # method6 works, two single tensors first and then torch.minimum
+        max_num = torch.minimum(torch.tensor(cls_scores.size(0)), 
+                            torch.tensor(self.bbox_coder.max_num)).int()
+
+        cls_scores = cls_scores.sigmoid()
+        _, indexs = cls_scores.max(dim=-1)
+        labels = indexs % self.bbox_coder.num_classes
+
+        _, bbox_index = track_scores.topk(max_num)
+
+        # part of method5
+        # bbox_index = bbox_index[:self.bbox_coder.max_num]
+        
+        labels = labels[bbox_index]
+        bbox_preds = bbox_preds[bbox_index]
+        track_scores = track_scores[bbox_index]
+        obj_idxes = obj_idxes[bbox_index]
+
+        scores = track_scores
+        
+        final_box_preds = denormalize_bbox_trt(bbox_preds, 
+                                           self.bbox_coder.pc_range)   
+        final_scores = track_scores
+        final_preds = labels
+
+        if self.bbox_coder.score_threshold is not None:
+            thresh_mask = final_scores > self.bbox_coder.score_threshold
+
+        self.bbox_coder.post_center_range = torch.tensor(
+            self.bbox_coder.post_center_range, device=scores.device)
+        mask = (final_box_preds[..., :3] >=
+                self.bbox_coder.post_center_range[:3]).all(1)
+
+        mask = mask & ((final_box_preds[..., :3] <=
+                    self.bbox_coder.post_center_range[3:]).all(1))
+
+        if self.bbox_coder.score_threshold:
+            mask = mask & thresh_mask
+        if not with_mask:
+            mask = torch.ones_like(mask) > 0
+
+        boxes3d = final_box_preds[self.index_bool2long_trt(mask)] 
+        scores = final_scores[self.index_bool2long_trt(mask)] 
+        labels = final_preds[self.index_bool2long_trt(mask)]
+        track_scores = track_scores[self.index_bool2long_trt(mask)]
+        obj_idxes = obj_idxes[self.index_bool2long_trt(mask)]
+
+        return (boxes3d,
+                scores,
+                labels,
+                track_scores,
+                obj_idxes,
+                bbox_index,
+                mask.int())
+    
+    def _generate_empty_tracks_trt(self):
+        num_queries, dim = self.query_embedding.weight.shape  # (300, 256 * 2)
+        device = self.query_embedding.weight.device
+        query = self.query_embedding.weight
+        query.requires_grad = False
+        ref_pts = self.reference_points(query[..., : dim // 2]).to(device)
+
+        # init boxes: xy, wl, z, h, sin, cos, vx, vy, vz
+        pred_boxes_init = torch.zeros(
+            (ref_pts.shape[0], 10), dtype=torch.float, device=device
+        )
+
+        output_embedding = torch.zeros(
+            (num_queries, dim >> 1), device=device
+        )
+
+        obj_idxes = torch.full(
+            (ref_pts.shape[0],), -1, dtype=torch.int, device=device  #long
+        )
+        matched_gt_idxes = torch.full(
+            (ref_pts.shape[0],), -1, dtype=torch.int, device=device  # long
+        )
+        disappear_time = torch.zeros(
+            (ref_pts.shape[0],), dtype=torch.int, device=device # long
+        )
+
+        iou = torch.zeros(
+            (ref_pts.shape[0],), dtype=torch.float, device=device
+        )
+        scores = torch.zeros(
+            (ref_pts.shape[0],), dtype=torch.float, device=device
+        )
+        track_scores = torch.zeros(
+            (ref_pts.shape[0],), dtype=torch.float, device=device
+        )
+        # xy, wl, z, h, sin, cos, vx, vy, vz
+        pred_boxes = pred_boxes_init
+
+        pred_logits = torch.zeros(
+            (ref_pts.shape[0], self.num_classes), dtype=torch.float, device=device
+        )
+
+        mem_bank_len = self.mem_bank_len
+        mem_bank = torch.zeros(
+            (ref_pts.shape[0], mem_bank_len, dim // 2),
+            dtype=torch.float32,
+            device=device,
+        )
+        mem_padding_mask = torch.ones(
+            (ref_pts.shape[0], mem_bank_len), dtype=torch.int, device=device  #bool
+        )
+        save_period = torch.zeros(
+            (ref_pts.shape[0],), dtype=torch.float32, device=device
+        )
+
+        return [
+            query,
+            ref_pts,
+            output_embedding,
+            obj_idxes,
+            matched_gt_idxes,
+            disappear_time,
+            iou,
+            scores,
+            track_scores,
+            pred_boxes,
+            pred_logits,
+            mem_bank,
+            mem_padding_mask,
+            save_period,
+        ]
+                
+
+    def simple_test_track_preprocess_trt_v2(
+        self,
+        prev_track_instances,
+        prev_timestamp,
+        prev_l2g_r_mat,
+        prev_l2g_t,
+        l2g_t=None,
+        l2g_r_mat=None,
+        # img_metas_scene_token=None,
+        scene_token_changed=None,
+        timestamp=None,
+    ):
+        empty_track_instances = self._generate_empty_tracks_trt()
+        
+        # WARNing Hardcode for different shape length
+        def shape_len_1(test_track_instances_i, empty_track_instances_i, scene_token_changed):
+            len_test_track_instances = test_track_instances_i.shape[0]
+            len_empty_track_instances = empty_track_instances_i.shape[0]
+            padded_empty_track_instances_i = torch.cat([empty_track_instances_i, 
+                                                  torch.ones(len_test_track_instances-len_empty_track_instances,
+                                                             ).to(
+                                                                 empty_track_instances_i)*(-10**4)], 
+                                                  dim=0)
+            track_instances_i = padded_empty_track_instances_i*scene_token_changed+\
+                                test_track_instances_i*(1-scene_token_changed)
+            index = self.index_bool2long_trt((track_instances_i!=(-10**4)))
+            return track_instances_i[index]
+        def shape_len_2(test_track_instances_i, empty_track_instances_i, scene_token_changed):
+            len_test_track_instances = test_track_instances_i.shape[0]
+            len_empty_track_instances, empty_track_instances_dim1 = empty_track_instances_i.shape
+            padded_empty_track_instances_i = torch.cat([empty_track_instances_i, 
+                                                  torch.ones(len_test_track_instances-len_empty_track_instances,
+                                                             empty_track_instances_dim1).to(
+                                                                 empty_track_instances_i)*(-10**4)], 
+                                                  dim=0)
+            track_instances_i = padded_empty_track_instances_i*scene_token_changed+\
+                                test_track_instances_i*(1-scene_token_changed)
+            index = self.index_bool2long_trt((track_instances_i!=(-10**4))[:, 0])
+            return track_instances_i[index]
+        def shape_len_3(test_track_instances_i, empty_track_instances_i, scene_token_changed):
+            len_test_track_instances = test_track_instances_i.shape[0]
+            len_empty_track_instances, empty_track_instances_dim1, empty_track_instances_dim2 = \
+                empty_track_instances_i.shape
+            padded_empty_track_instances_i = torch.cat([empty_track_instances_i, 
+                                                  torch.ones(len_test_track_instances-len_empty_track_instances,
+                                                             empty_track_instances_dim1, 
+                                                             empty_track_instances_dim2).to(
+                                                                 empty_track_instances_i)*(-10**4)], 
+                                                  dim=0)
+            track_instances_i = padded_empty_track_instances_i*scene_token_changed+\
+                                test_track_instances_i*(1-scene_token_changed)
+            index = self.index_bool2long_trt((track_instances_i!=(-10**4))[:, 0, 0])
+            return track_instances_i[index]
+        # WARNing Hardcode for different shape length
+        track_instances = []
+        for i in range(0, 3):   
+            track_instances.append(shape_len_2(prev_track_instances[i],
+                                               empty_track_instances[i],
+                                               scene_token_changed))
+        for i in range(3, 9):
+            track_instances.append(shape_len_1(prev_track_instances[i],
+                                               empty_track_instances[i],
+                                               scene_token_changed))
+        for i in range(9, 11):
+            track_instances.append(shape_len_2(prev_track_instances[i],
+                                               empty_track_instances[i],
+                                               scene_token_changed))
+        for i in range(11, 12):
+            track_instances.append(shape_len_3(prev_track_instances[i],
+                                               empty_track_instances[i],
+                                               scene_token_changed))
+        for i in range(12, 13):
+            track_instances.append(shape_len_2(prev_track_instances[i],
+                                               empty_track_instances[i],
+                                               scene_token_changed))
+        for i in range(13, 14):
+            track_instances.append(shape_len_1(prev_track_instances[i],
+                                               empty_track_instances[i],
+                                               scene_token_changed))
+
+        time_delta = torch.zeros_like(timestamp)[0]*scene_token_changed\
+                    + (timestamp[0] - prev_timestamp[0])*(1-scene_token_changed)
+
+
+        l2g_r1 = torch.zeros_like(l2g_r_mat)*scene_token_changed\
+                +prev_l2g_r_mat*(1-scene_token_changed)
+    
+        l2g_t1 = torch.zeros_like(l2g_t)*scene_token_changed\
+                +prev_l2g_t*(1-scene_token_changed)
+        
+        l2g_r2 = torch.rand_like(l2g_r_mat)*scene_token_changed\
+                +l2g_r_mat*(1-scene_token_changed)
+        
+        l2g_t2 = torch.zeros_like(l2g_t)*scene_token_changed\
+                +l2g_t*(1-scene_token_changed)
+        
+        return time_delta, l2g_r1, l2g_t1, l2g_r2, l2g_t2, track_instances, timestamp, l2g_t, l2g_r_mat
+
+
+    def simple_test_track_trt(
+        self,
+        prev_track_intances,
+        prev_timestamp,
+        prev_l2g_r_mat,
+        prev_l2g_t, 
+        can_bus, 
+        lidar2img, 
+        # img_metas_scene_token,#
+        scene_token_changed,
+        timestamp,
+        l2g_r_mat, 
+        l2g_t, 
+        image_shape,
+        prev_bev,
+        max_obj_id,
+        img=None,
+        use_prev_bev=None,
+    ):
+
+        (time_delta, 
+        l2g_r1, 
+        l2g_t1, 
+        l2g_r2, 
+        l2g_t2, 
+        track_instances,
+        prev_timestamp,
+        prev_l2g_t, 
+        prev_l2g_r_mat)=self.simple_test_track_preprocess_trt_v2(
+                        prev_track_intances,
+                        prev_timestamp,
+                        prev_l2g_r_mat,
+                        prev_l2g_t, 
+                        l2g_t,
+                        l2g_r_mat,
+                        # img_metas_scene_token,
+                        scene_token_changed,
+                        timestamp,
+                        )
+        
+        track_instances = [item.detach() for item in track_instances]
+        
+        (
+        track_instances,
+        bev_embed, 
+        bev_pos,
+        output_classes, 
+        output_coords, 
+        all_past_traj_preds,
+        last_ref_pts, 
+        query_feats,
+        )= self._forward_single_frame_inference_trt(
+            img,
+            can_bus, 
+            lidar2img,
+            image_shape,
+            track_instances,
+            prev_bev,
+            l2g_r1, 
+            l2g_t1, 
+            l2g_r2, 
+            l2g_t2, 
+            time_delta, 
+            use_prev_bev,
+        )
+        output_classes = output_classes.float().detach()
+        output_coords = output_coords.float().detach()
+        last_ref_pts = last_ref_pts.float().detach()
+        query_feats = query_feats.float().detach()
+        
+        (
+        track_query_embeddings, 
+        track_query_matched_idxes, 
+        bboxes_dict_bboxes,
+        bboxes_gravity_center,
+        bboxes_yaw,
+        scores,
+        labels,
+        track_scores,
+        bbox_index,
+        obj_idxes,
+        mask,
+        track_bbox_results1,
+        track_bbox_results2,
+
+        sdc_bboxes_dict_bboxes,
+        sdc_boxes_3d_gravity_center,
+        sdc_boxes_3d_yaw,  
+        sdc_scores_3d,
+        sdc_track_scores,
+        sdc_track_bbox_results1,
+        sdc_track_bbox_results2,
+        sdc_embedding,
+        max_obj_id_out,
+
+        track_instances_fordet,
+        track_instances, # final track_instances
+        )=self.track2mop_trt(track_instances,
+                            output_classes, 
+                            output_coords, 
+                            last_ref_pts, 
+                            query_feats,
+                            max_obj_id,)
+        
+        return (
+                track_instances,
+                prev_timestamp,
+                prev_l2g_t, 
+                prev_l2g_r_mat,
+                bev_embed, 
+                bev_pos, 
+                output_classes, 
+                output_coords, 
+                all_past_traj_preds, 
+                last_ref_pts, 
+                query_feats,
+
+                track_query_embeddings, 
+                track_query_matched_idxes, 
+                bboxes_dict_bboxes,
+                bboxes_gravity_center,
+                bboxes_yaw,
+                scores,
+                labels,
+                track_scores,
+                bbox_index,
+                obj_idxes,
+                mask,
+                track_bbox_results1,
+                track_bbox_results2,
+
+                sdc_bboxes_dict_bboxes,
+                sdc_boxes_3d_gravity_center,
+                sdc_boxes_3d_yaw,  
+                sdc_scores_3d,
+                sdc_track_scores,
+                sdc_track_bbox_results1,
+                sdc_track_bbox_results2,
+                sdc_embedding,
+                max_obj_id_out,
+
+                track_instances_fordet,
+                )
+    
+
+    def upsample_bev_if_tiny_trt(self, bev_embed, bev_pos, prev_bev=None):
+        if bev_embed.size(0) == 50 * 50:
+            # For tiny model
+            dim, _, _ = bev_embed.size()
+            w = h = int(math.sqrt(dim))
+            assert h == w == 50
+
+            bev_embed = rearrange(bev_embed, '(h w) b c -> b c h w', h=h, w=w)  
+            bev_embed = nn.Upsample(scale_factor=4)(bev_embed)  
+            bev_embed = rearrange(bev_embed, 'b c h w -> (h w) b c')
+            if prev_bev is not None:
+                prev_bev = rearrange(prev_bev, '(h w) b c -> b c h w', h=h, w=w)
+                prev_bev = nn.Upsample(scale_factor=4)(prev_bev) 
+                prev_bev = rearrange(prev_bev, 'b c h w -> (h w) b c')
+            bev_pos = nn.Upsample(scale_factor=4)(bev_pos)  
+        return bev_embed, bev_pos, prev_bev
+
+
+def index_bool2long_trt(bool_index):
+    # Convert Boolean Tensor to Long Tensor:
+    long_index = bool_index.long()
+    # Create a Range Tensor and Multiply by the Long Tensor
+    long_index = torch.arange(1, long_index.shape[-1]+1
+                                        , device=long_index.device)*long_index
+    # Extract Non-Zero Indices
+    long_index = long_index[long_index.nonzero(as_tuple=True)[0]]
+    # Adjust Indices to Be Zero-Based
+    long_index = long_index - torch.ones_like(long_index)
+    return long_index
+
+@torch.jit.script
+def track_base_update_trt_script_v4(track_instances3, track_instances5, track_instances7, length,
+                                 score_thresh, filter_score_thresh, max_obj_id, miss_tolerance):
+    track_instances3_c = track_instances3.int().clone()
+    track_instances5_c = track_instances5.clone()
+    track_instances5_c[track_instances7 >= score_thresh] = 0
+    cond1 = ((track_instances3_c == -1) & (track_instances7 >= score_thresh))
+    cond2=((track_instances3_c >= 0) & (track_instances7 < filter_score_thresh))&(~cond1)
+    track_instances5_c = (track_instances5_c + 1)*cond2+track_instances5_c*~cond2
+    cond3=cond2&(track_instances5_c >= miss_tolerance)
+    track_instances3_c = ((-1)*cond3+(track_instances3_c)*~cond3).int()
+    cond1 = cond1.int()
+    for i in range(track_instances3_c.shape[0]):
+        track_instances3_c[i]=(max_obj_id[0].int())*cond1[i]+track_instances3_c[i]*(1-cond1[i])
+        max_obj_id = (max_obj_id + 1)*cond1[i]+max_obj_id*(1-cond1[i])
+
+    return track_instances3_c, track_instances5_c, max_obj_id
+
+
+# the vectorized version of track_base_update_trt_script_v4, can save 20-25ms TRT engine inference time on Orin.
+# TODO: waiting for bug fix of TensorRT-8.6.13.3
+def track_base_update_trt_script_v5(track_instances3, track_instances5, track_instances7, length,
+                                 score_thresh, filter_score_thresh, max_obj_id, miss_tolerance):
+    track_instances3_c = track_instances3.int().clone()
+    track_instances5_c = track_instances5.clone()
+    track_instances5_c[track_instances7 >= score_thresh] = 0
+    cond1 = ((track_instances3_c == -1) & (track_instances7 >= score_thresh))
+    cond2=((track_instances3_c >= 0) & (track_instances7 < filter_score_thresh))&(~cond1)
+    track_instances5_c = (track_instances5_c + 1)*cond2+track_instances5_c*~cond2
+    cond3=cond2&(track_instances5_c >= miss_tolerance)
+    track_instances3_c = ((-1)*cond3+(track_instances3_c)*~cond3).int()
+    diff = cond1.int().sum().int()
+    data = torch.arange(max_obj_id[0].int(), diff+max_obj_id[0].int(), device=max_obj_id.device, dtype=torch.int32)
+    track_instances3_c[index_bool2long_trt(cond1)] = data 
+    max_obj_id_out = max_obj_id + diff
+
+    return track_instances3_c, track_instances5_c, max_obj_id_out
+
+
+
+
diff --git a/projects/mmdet3d_plugin/uniad/modules/__init__.py b/projects/mmdet3d_plugin/uniad/modules/__init__.py
index 1bb5e04..87da8ed 100644
--- a/projects/mmdet3d_plugin/uniad/modules/__init__.py
+++ b/projects/mmdet3d_plugin/uniad/modules/__init__.py
@@ -1,6 +1,16 @@
-from .transformer import PerceptionTransformer
-from .spatial_cross_attention import SpatialCrossAttention, MSDeformableAttention3D
-from .temporal_self_attention import TemporalSelfAttention
-from .encoder import BEVFormerEncoder, BEVFormerLayer
-from .decoder import DetectionTransformerDecoder
-
+# from .transformer import PerceptionTransformer
+# from .spatial_cross_attention import SpatialCrossAttention, MSDeformableAttention3D
+# from .temporal_self_attention import TemporalSelfAttention
+# from .encoder import BEVFormerEncoder, BEVFormerLayer
+# from .decoder import DetectionTransformerDecoder
+from .cnn import *
+# from .multi_head_attention import MultiheadAttentionTRT
+from .encoder import *
+from .decoder import *
+from .multi_scale_deformable_attn_function import *
+from .spatial_cross_attention import *
+from .temporal_self_attention import *
+from .transformer import *
+from .custom_base_transformer_layer import *
+# from .feedforward_network import FFNTRT
+from .multi_scale_deformable_attn_function import *
\ No newline at end of file
diff --git a/projects/mmdet3d_plugin/uniad/modules/decoder.py b/projects/mmdet3d_plugin/uniad/modules/decoder.py
index 33024f8..0186ef3 100644
--- a/projects/mmdet3d_plugin/uniad/modules/decoder.py
+++ b/projects/mmdet3d_plugin/uniad/modules/decoder.py
@@ -30,6 +30,13 @@ from .multi_scale_deformable_attn_function import MultiScaleDeformableAttnFuncti
 ext_module = ext_loader.load_ext(
     '_ext', ['ms_deform_attn_backward', 'ms_deform_attn_forward'])
 
+import sys
+sys.path.insert(1, './projects/mmdet3d_plugin/uniad/functions')
+from multi_scale_deformable_attn import multi_scale_deformable_attn, MSDAPlugin
+
+from mmcv.cnn.bricks.transformer import MultiScaleDeformableAttention
+from torch.autograd.function import Function, once_differentiable
+
 
 def inverse_sigmoid(x, eps=1e-5):
     """Inverse function of sigmoid.
@@ -49,7 +56,7 @@ def inverse_sigmoid(x, eps=1e-5):
     return torch.log(x1 / x2)
 
 
-@TRANSFORMER_LAYER_SEQUENCE.register_module()
+@TRANSFORMER_LAYER_SEQUENCE.register_module(force=True)
 class DetectionTransformerDecoder(TransformerLayerSequence):
     """Implements the decoder in DETR3D transformer.
     Args:
@@ -129,7 +136,7 @@ class DetectionTransformerDecoder(TransformerLayerSequence):
         return output, reference_points
 
 
-@ATTENTION.register_module()
+@ATTENTION.register_module(force=True)
 class CustomMSDeformableAttention(BaseModule):
     """An attention module used in Deformable-Detr.
 
@@ -343,3 +350,585 @@ class CustomMSDeformableAttention(BaseModule):
             output = output.permute(1, 0, 2)
 
         return self.dropout(output) + identity
+
+
+
+
+@TRANSFORMER_LAYER_SEQUENCE.register_module()
+class DetectionTransformerDecoderTRTP(DetectionTransformerDecoder):
+    """Implements the decoder in DETR3D transformer.
+    Args:
+        return_intermediate (bool): Whether to return intermediate outputs.
+        coder_norm_cfg (dict): Config of last normalization layer. Default：
+            `LN`.
+    """
+
+    def __init__(self, *args, **kwargs):
+        super(DetectionTransformerDecoderTRTP, self).__init__(*args, **kwargs)
+
+    def forward(
+        self,
+        query,
+        *args,
+        reference_points=None,
+        reg_branches=None,
+        key_padding_mask=None,
+        **kwargs,
+    ):
+        """Forward function for `Detr3DTransformerDecoder`.
+        Args:
+            query (Tensor): Input query with shape
+                `(num_query, bs, embed_dims)`.
+            reference_points (Tensor): The reference
+                points of offset. has shape
+                (bs, num_query, 4) when as_two_stage,
+                otherwise has shape ((bs, num_query, 2).
+            reg_branch: (obj:`nn.ModuleList`): Used for
+                refining the regression results. Only would
+                be passed when with_box_refine is True,
+                otherwise would be passed a `None`.
+        Returns:
+            Tensor: Results with shape [1, num_query, bs, embed_dims] when
+                return_intermediate is `False`, otherwise it has shape
+                [num_layers, num_query, bs, embed_dims].
+        """
+        output = query
+        intermediate = []
+        intermediate_reference_points = []
+        for lid, layer in enumerate(self.layers):
+            reference_points_input = reference_points[..., :2][:,:,None, ...]
+            output = layer(
+                output,
+                *args,
+                reference_points=reference_points_input,
+                key_padding_mask=key_padding_mask,
+                **kwargs,
+            )
+
+            if reg_branches is not None:
+                tmp = reg_branches[lid](output).view(1, -1, 10)
+
+                assert reference_points.shape[-1] == 3
+                reference_points = torch.cat(
+                    [
+                        tmp[..., :2] + inverse_sigmoid(reference_points[..., :2]),
+                        tmp[..., 4:5] + inverse_sigmoid(reference_points[..., 2:3]),
+                    ],
+                    dim=-1,
+                ).sigmoid()
+
+            if self.return_intermediate:
+                intermediate.append(output)
+                intermediate_reference_points.append(reference_points)
+
+        if self.return_intermediate:
+            return torch.stack(intermediate), torch.stack(intermediate_reference_points)
+
+        return output, reference_points
+
+
+@ATTENTION.register_module()
+class CustomMSDeformableAttentionTRT(CustomMSDeformableAttention):
+    """An attention module used in Deformable-Detr.
+
+    `Deformable DETR: Deformable Transformers for End-to-End Object Detection.
+    <https://arxiv.org/pdf/2010.04159.pdf>`_.
+
+    Args:
+        embed_dims (int): The embedding dimension of Attention.
+            Default: 256.
+        num_heads (int): Parallel attention heads. Default: 64.
+        num_levels (int): The number of feature map used in
+            Attention. Default: 4.
+        num_points (int): The number of sampling points for
+            each query in each head. Default: 4.
+        im2col_step (int): The step used in image_to_column.
+            Default: 64.
+        dropout (float): A Dropout layer on `inp_identity`.
+            Default: 0.1.
+        batch_first (bool): Key, Query and Value are shape of
+            (batch, n, embed_dim)
+            or (n, batch, embed_dim). Default to False.
+        norm_cfg (dict): Config dict for normalization layer.
+            Default: None.
+        init_cfg (obj:`mmcv.ConfigDict`): The Config for initialization.
+            Default: None.
+    """
+
+    def __init__(
+        self,
+        embed_dims=256,
+        num_heads=8,
+        num_levels=4,
+        num_points=4,
+        im2col_step=64,
+        dropout=0.1,
+        batch_first=False,
+        norm_cfg=None,
+        init_cfg=None,
+    ):
+        super(CustomMSDeformableAttention, self).__init__(init_cfg)
+        if embed_dims % num_heads != 0:
+            raise ValueError(
+                f"embed_dims must be divisible by num_heads, "
+                f"but got {embed_dims} and {num_heads}"
+            )
+        dim_per_head = embed_dims // num_heads
+        self.norm_cfg = norm_cfg
+        self.dropout = nn.Dropout(dropout)
+        self.batch_first = batch_first
+        self.fp16_enabled = False
+
+        # you'd better set dim_per_head to a power of 2
+        # which is more efficient in the CUDA implementation
+        def _is_power_of_2(n):
+            if (not isinstance(n, int)) or (n < 0):
+                raise ValueError(
+                    "invalid input for _is_power_of_2: {} (type: {})".format(n, type(n))
+                )
+            return (n & (n - 1) == 0) and n != 0
+
+        if not _is_power_of_2(dim_per_head):
+            warnings.warn(
+                "You'd better set embed_dims in "
+                "MultiScaleDeformAttention to make "
+                "the dimension of each attention head a power of 2 "
+                "which is more efficient in our CUDA implementation."
+            )
+
+        self.im2col_step = im2col_step
+        self.embed_dims = embed_dims
+        self.num_levels = num_levels
+        self.num_heads = num_heads
+        self.num_points = num_points
+        self.sampling_offsets = nn.Linear(
+            embed_dims, num_heads * num_levels * num_points * 2
+        )
+        self.attention_weights = nn.Linear(embed_dims, num_heads * num_levels * num_points)
+        self.value_proj = nn.Linear(embed_dims, embed_dims)
+        self.output_proj = nn.Linear(embed_dims, embed_dims)
+        self.init_weights()
+        self.forward = self.forward_trt
+
+    @deprecated_api_warning(
+        {"residual": "identity"}, cls_name="MultiScaleDeformableAttention"
+    )
+    def forward_trt(
+        self,
+        query,
+        key=None,
+        value=None,
+        identity=None,
+        query_pos=None,
+        key_padding_mask=None,
+        reference_points=None,
+        spatial_shapes=None,
+        level_start_index=None,
+        flag="decoder",
+        **kwargs,
+    ):
+        """Forward Function of MultiScaleDeformAttention.
+
+        Args:
+            query (Tensor): Query of Transformer with shape
+                (num_query, bs, embed_dims).
+            key (Tensor): The key tensor with shape
+                `(num_key, bs, embed_dims)`.
+            value (Tensor): The value tensor with shape
+                `(num_key, bs, embed_dims)`.
+            identity (Tensor): The tensor used for addition, with the
+                same shape as `query`. Default None. If None,
+                `query` will be used.
+            query_pos (Tensor): The positional encoding for `query`.
+                Default: None.
+            key_pos (Tensor): The positional encoding for `key`. Default
+                None.
+            reference_points (Tensor):  The normalized reference
+                points with shape (bs, num_query, num_levels, 2),
+                all elements is range in [0, 1], top-left (0,0),
+                bottom-right (1, 1), including padding area.
+                or (N, Length_{query}, num_levels, 4), add
+                additional two dimensions is (w, h) to
+                form reference boxes.
+            key_padding_mask (Tensor): ByteTensor for `query`, with
+                shape [bs, num_key].
+            spatial_shapes (Tensor): Spatial shape of features in
+                different levels. With shape (num_levels, 2),
+                last dimension represents (h, w).
+            level_start_index (Tensor): The start index of each level.
+                A tensor has shape ``(num_levels, )`` and can be represented
+                as [0, h_0*w_0, h_0*w_0+h_1*w_1, ...].
+
+        Returns:
+             Tensor: forwarded results with shape [num_query, bs, embed_dims].
+        """
+        if value is None:
+            value = query
+
+        if identity is None:
+            identity = query
+        if query_pos is not None:
+            query = query + query_pos
+        if not self.batch_first:
+            # change to (bs, num_query ,embed_dims)
+            query = query.permute(1, 0, 2)
+            value = value.permute(1, 0, 2)
+
+        bs, num_query, _ = query.shape
+        bs, num_value, _ = value.shape
+        assert (spatial_shapes[:, 0] * spatial_shapes[:, 1]).sum() == num_value
+
+        value = self.value_proj(value)
+        if key_padding_mask is not None:
+            value = value.masked_fill(key_padding_mask[..., None], 0.0)
+        value = value.view(bs, num_value, self.num_heads, -1)
+
+        sampling_offsets = self.sampling_offsets(query).view(
+            bs, num_query, self.num_heads, self.num_levels, self.num_points, 2
+        )
+        attention_weights = self.attention_weights(query).view(
+            bs, num_query, self.num_heads, self.num_levels * self.num_points
+        )
+        attention_weights = attention_weights.softmax(-1)
+
+        attention_weights = attention_weights.view(
+            bs, num_query, self.num_heads, self.num_levels, self.num_points
+        )
+        if reference_points.shape[-1] == 2:
+            offset_normalizer = torch.stack(
+                [spatial_shapes[..., 1], spatial_shapes[..., 0]], -1
+            )
+            sampling_locations = (
+                reference_points[:, :, None, :, None, :]
+                + sampling_offsets / offset_normalizer[None, None, None, :, None, :]
+            )
+        elif reference_points.shape[-1] == 4:
+            sampling_locations = (
+                reference_points[:, :, None, :, None, :2]
+                + sampling_offsets
+                / self.num_points
+                * reference_points[:, :, None, :, None, 2:]
+                * 0.5
+            )
+        else:
+            raise ValueError(
+                f"Last dim of reference_points must be"
+                f" 2 or 4, but get {reference_points.shape[-1]} instead."
+            )
+        if (
+            torch.cuda.is_available()
+            and value.is_cuda
+            and not torch.onnx.is_in_onnx_export()
+        ):
+            MultiScaleDeformableAttnFunction = MultiScaleDeformableAttnFunction_fp32
+            output = MultiScaleDeformableAttnFunction.apply(
+                value,
+                spatial_shapes,
+                level_start_index,
+                sampling_locations,
+                attention_weights,
+                self.im2col_step,
+            )
+        else:
+            output = multi_scale_deformable_attn_pytorch(#msda_torch(
+                value,
+                spatial_shapes,
+                sampling_locations,
+                attention_weights,
+                num_heads=self.num_heads,
+                embed_dims=self.embed_dims,
+                num_levels=self.num_levels,
+                num_points=self.num_points,
+                num_queries=int(num_query),
+                bs=1,
+            )
+
+        output = self.output_proj(output)
+
+        if not self.batch_first:
+            # (num_query, bs ,embed_dims)
+            output = output.permute(1, 0, 2)
+
+        return self.dropout(output) + identity
+
+
+@ATTENTION.register_module()
+class CustomMSDeformableAttentionTRTP(CustomMSDeformableAttentionTRT):
+    """An attention module used in Deformable-Detr.
+
+    `Deformable DETR: Deformable Transformers for End-to-End Object Detection.
+    <https://arxiv.org/pdf/2010.04159.pdf>`_.
+
+    Args:
+        embed_dims (int): The embedding dimension of Attention.
+            Default: 256.
+        num_heads (int): Parallel attention heads. Default: 64.
+        num_levels (int): The number of feature map used in
+            Attention. Default: 4.
+        num_points (int): The number of sampling points for
+            each query in each head. Default: 4.
+        im2col_step (int): The step used in image_to_column.
+            Default: 64.
+        dropout (float): A Dropout layer on `inp_identity`.
+            Default: 0.1.
+        batch_first (bool): Key, Query and Value are shape of
+            (batch, n, embed_dim)
+            or (n, batch, embed_dim). Default to False.
+        norm_cfg (dict): Config dict for normalization layer.
+            Default: None.
+        init_cfg (obj:`mmcv.ConfigDict`): The Config for initialization.
+            Default: None.
+    """
+
+    def __init__(self, *args, **kwargs):
+        super(CustomMSDeformableAttentionTRTP, self).__init__(*args, **kwargs)
+        self.multi_scale_deformable_attn = multi_scale_deformable_attn
+
+    @deprecated_api_warning(
+        {"residual": "identity"}, cls_name="MultiScaleDeformableAttention"
+    )
+    def forward_trt(
+        self,
+        query,
+        key=None,
+        value=None,
+        identity=None,
+        query_pos=None,
+        key_padding_mask=None,
+        reference_points=None,
+        spatial_shapes=None,
+        level_start_index=None,
+        flag="decoder",
+        **kwargs,
+    ):
+        """Forward Function of MultiScaleDeformAttention.
+
+        Args:
+            query (Tensor): Query of Transformer with shape
+                (num_query, bs, embed_dims).
+            key (Tensor): The key tensor with shape
+                `(num_key, bs, embed_dims)`.
+            value (Tensor): The value tensor with shape
+                `(num_key, bs, embed_dims)`.
+            identity (Tensor): The tensor used for addition, with the
+                same shape as `query`. Default None. If None,
+                `query` will be used.
+            query_pos (Tensor): The positional encoding for `query`.
+                Default: None.
+            key_pos (Tensor): The positional encoding for `key`. Default
+                None.
+            reference_points (Tensor):  The normalized reference
+                points with shape (bs, num_query, num_levels, 2),
+                all elements is range in [0, 1], top-left (0,0),
+                bottom-right (1, 1), including padding area.
+                or (N, Length_{query}, num_levels, 4), add
+                additional two dimensions is (w, h) to
+                form reference boxes.
+            key_padding_mask (Tensor): ByteTensor for `query`, with
+                shape [bs, num_key].
+            spatial_shapes (Tensor): Spatial shape of features in
+                different levels. With shape (num_levels, 2),
+                last dimension represents (h, w).
+            level_start_index (Tensor): The start index of each level.
+                A tensor has shape ``(num_levels, )`` and can be represented
+                as [0, h_0*w_0, h_0*w_0+h_1*w_1, ...].
+
+        Returns:
+             Tensor: forwarded results with shape [num_query, bs, embed_dims].
+        """
+        if value is None:
+            value = query
+
+        if identity is None:
+            identity = query
+        if query_pos is not None:
+            query = query + query_pos
+        assert not self.batch_first
+        # change to (bs, num_query ,embed_dims)
+        query = query.view(1, -1, self.embed_dims)
+        value = value.view(1, -1, self.embed_dims)
+
+        assert (spatial_shapes[:, 0] * spatial_shapes[:, 1]).sum() == value.shape[1]
+
+        value = self.value_proj(value)
+        if key_padding_mask is not None:
+            value = value.masked_fill(key_padding_mask[..., None], 0.0)
+        value = value.view(1, -1, self.num_heads, self.embed_dims // self.num_heads)
+
+        sampling_offsets = self.sampling_offsets(query)
+        attention_weights = self.attention_weights(query)
+
+        if torch.onnx.is_in_onnx_export():
+            assert value.is_cuda
+        sampling_offsets = sampling_offsets.view(
+            *sampling_offsets.shape[:2], self.num_heads, -1
+        )
+        attention_weights = attention_weights.view(
+            *attention_weights.shape[:2], self.num_heads, -1
+        )
+        output = self.multi_scale_deformable_attn(
+            value,
+            spatial_shapes,
+            reference_points,
+            sampling_offsets,
+            attention_weights,
+        ).flatten(2)
+
+        output = self.output_proj(output)
+
+        output = output.permute(1, 0, 2)
+        return self.dropout(output) + identity
+    
+
+@ATTENTION.register_module(force=True)
+class MultiScaleDeformableAttentionTRTP(CustomMSDeformableAttentionTRT):
+    """An attention module used in Deformable-Detr.

+    `Deformable DETR: Deformable Transformers for End-to-End Object Detection.
+    <https://arxiv.org/pdf/2010.04159.pdf>`_.

+    Args:
+        embed_dims (int): The embedding dimension of Attention.
+            Default: 256.
+        num_heads (int): Parallel attention heads. Default: 64.
+        num_levels (int): The number of feature map used in
+            Attention. Default: 4.
+        num_points (int): The number of sampling points for
+            each query in each head. Default: 4.
+        im2col_step (int): The step used in image_to_column.
+            Default: 64.
+        dropout (float): A Dropout layer on `inp_identity`.
+            Default: 0.1.
+        batch_first (bool): Key, Query and Value are shape of
+            (batch, n, embed_dim)
+            or (n, batch, embed_dim). Default to False.
+        norm_cfg (dict): Config dict for normalization layer.
+            Default: None.
+        init_cfg (obj:`mmcv.ConfigDict`): The Config for initialization.
+            Default: None.
+    """

+    def __init__(self, *args, **kwargs):
+        super(MultiScaleDeformableAttentionTRTP, self).__init__(*args, **kwargs)
+        self.multi_scale_deformable_attn_standard = multi_scale_deformable_attn_standard
+        self.multi_scale_deformable_attn = multi_scale_deformable_attn

+    @deprecated_api_warning(
+        {"residual": "identity"}, cls_name="MultiScaleDeformableAttention"
+    )
+    def forward_trt(
+        self,
+        query,
+        key=None,
+        value=None,
+        identity=None,
+        query_pos=None,
+        key_padding_mask=None,
+        reference_points=None,
+        spatial_shapes=None,
+        level_start_index=None,
+        flag="decoder",
+        **kwargs,
+    ):
+        """Forward Function of MultiScaleDeformAttention.

+        Args:
+            query (Tensor): Query of Transformer with shape
+                (num_query, bs, embed_dims).
+            key (Tensor): The key tensor with shape
+                `(num_key, bs, embed_dims)`.
+            value (Tensor): The value tensor with shape
+                `(num_key, bs, embed_dims)`.
+            identity (Tensor): The tensor used for addition, with the
+                same shape as `query`. Default None. If None,
+                `query` will be used.
+            query_pos (Tensor): The positional encoding for `query`.
+                Default: None.
+            key_pos (Tensor): The positional encoding for `key`. Default
+                None.
+            reference_points (Tensor):  The normalized reference
+                points with shape (bs, num_query, num_levels, 2),
+                all elements is range in [0, 1], top-left (0,0),
+                bottom-right (1, 1), including padding area.
+                or (N, Length_{query}, num_levels, 4), add
+                additional two dimensions is (w, h) to
+                form reference boxes.
+            key_padding_mask (Tensor): ByteTensor for `query`, with
+                shape [bs, num_key].
+            spatial_shapes (Tensor): Spatial shape of features in
+                different levels. With shape (num_levels, 2),
+                last dimension represents (h, w).
+            level_start_index (Tensor): The start index of each level.
+                A tensor has shape ``(num_levels, )`` and can be represented
+                as [0, h_0*w_0, h_0*w_0+h_1*w_1, ...].

+        Returns:
+             Tensor: forwarded results with shape [num_query, bs, embed_dims].
+        """
+        if value is None:
+            value = query

+        if identity is None:
+            identity = query
+        if query_pos is not None:
+            query = query + query_pos
+        assert not self.batch_first
+        +change to (bs, num_query ,embed_dims)
+        query = query.view(1, -1, self.embed_dims)
+        value = value.view(1, -1, self.embed_dims)

+        assert (spatial_shapes[:, 0] * spatial_shapes[:, 1]).sum() == value.shape[1]

+        value = self.value_proj(value)
+        if key_padding_mask is not None:
+            value = value.masked_fill(key_padding_mask[..., None], 0.0)
+        value = value.view(1, -1, self.num_heads, self.embed_dims // self.num_heads)

+        sampling_offsets = self.sampling_offsets(query)
+        attention_weights = self.attention_weights(query)

+        if torch.onnx.is_in_onnx_export():
+            assert value.is_cuda
+        sampling_offsets = sampling_offsets.view(
+            *sampling_offsets.shape[:2], self.num_heads, -1
+        )
+        attention_weights = attention_weights.view(
+            *attention_weights.shape[:2], self.num_heads, -1
+        )
+        if reference_points.shape[-1]==2:
+            output = self.multi_scale_deformable_attn(
+                value,
+                spatial_shapes,
+                reference_points,
+                sampling_offsets,
+                attention_weights,
+            ).flatten(2)
+        elif reference_points.shape[-1] == 4:
+            num_heads, channel = value.shape[2:]
+            num_level = spatial_shapes.shape[0]
+            bs, num_queries = reference_points.shape[:2]
+            offset_normalizer = torch.stack([spatial_shapes[..., 1], 
+                                            spatial_shapes[..., 0]], -1).reshape(1, 1, 1, -1, 1, 1, 2)
+            points_per_group = torch.div(reference_points.shape[-1], 2, rounding_mode="floor")
+            original_sampling_offsets_shape = sampling_offsets.shape
+            sampling_offsets = sampling_offsets.reshape(
+                    bs, num_queries, num_heads, num_level, -1, points_per_group, 2,
+                    ) / 4 \
+                    * reference_points[:, :, None, :, None, None, 2:] \
+                    * 0.5 * offset_normalizer
+            output = self.multi_scale_deformable_attn(
+                value,
+                spatial_shapes,
+                reference_points[..., :2],
+                sampling_offsets.reshape(original_sampling_offsets_shape),
+                attention_weights,
+            ).flatten(2)
+        else:
+            raise ValueError(
+                f'Last dim of reference_points must be'
+                f' 2 or 4, but get {reference_points.shape[-1]} instead.')

+        output = self.output_proj(output)

+        output = output.permute(1, 0, 2)
+        return self.dropout(output) + identity
diff --git a/projects/mmdet3d_plugin/uniad/modules/encoder.py b/projects/mmdet3d_plugin/uniad/modules/encoder.py
index 6875233..99bbdc8 100644
--- a/projects/mmdet3d_plugin/uniad/modules/encoder.py
+++ b/projects/mmdet3d_plugin/uniad/modules/encoder.py
@@ -23,7 +23,7 @@ ext_module = ext_loader.load_ext(
     '_ext', ['ms_deform_attn_backward', 'ms_deform_attn_forward'])
 
 
-@TRANSFORMER_LAYER_SEQUENCE.register_module()
+@TRANSFORMER_LAYER_SEQUENCE.register_module(force=True)
 class BEVFormerEncoder(TransformerLayerSequence):
 
     """
@@ -235,7 +235,7 @@ class BEVFormerEncoder(TransformerLayerSequence):
         return output
 
 
-@TRANSFORMER_LAYER.register_module()
+@TRANSFORMER_LAYER.register_module(force=True)
 class BEVFormerLayer(MyCustomBaseTransformerLayer):
     """Implements decoder layer in DETR transformer.
     Args:
@@ -352,7 +352,7 @@ class BEVFormerLayer(MyCustomBaseTransformerLayer):
         for layer in self.operation_order:
             # temporal self attention
             if layer == 'self_attn':
-
+                
                 query = self.attentions[attn_index](
                     query,
                     prev_bev,
@@ -400,3 +400,674 @@ class BEVFormerLayer(MyCustomBaseTransformerLayer):
                 ffn_index += 1
 
         return query
+
+
+@TRANSFORMER_LAYER_SEQUENCE.register_module(force=True)
+class BEVFormerEncoderTRT(BEVFormerEncoder):
+    def __init__(self, *args, **kwargs):
+        super(BEVFormerEncoderTRT, self).__init__(*args, **kwargs)
+
+    def point_sampling_trt(self, reference_points, pc_range, lidar2img, image_shape):
+        reference_points = reference_points.clone()
+
+        reference_points[..., 0:1] = (
+            reference_points[..., 0:1] * (pc_range[3] - pc_range[0]) + pc_range[0]
+        )
+        reference_points[..., 1:2] = (
+            reference_points[..., 1:2] * (pc_range[4] - pc_range[1]) + pc_range[1]
+        )
+        reference_points[..., 2:3] = (
+            reference_points[..., 2:3] * (pc_range[5] - pc_range[2]) + pc_range[2]
+        )
+
+        reference_points = torch.cat(
+            (reference_points, torch.ones_like(reference_points[..., :1])), -1
+        )
+
+        reference_points = reference_points.permute(1, 0, 2, 3)
+        D, B, num_query = reference_points.size()[:3]
+        num_cam = lidar2img.size(1)
+
+        reference_points = (
+            reference_points.view(D, B, 1, num_query, 4)
+            .repeat(1, 1, num_cam, 1, 1)
+            .unsqueeze(-1)
+        )
+
+        lidar2img = lidar2img.view(1, B, num_cam, 1, 4, 4).repeat(
+            D, 1, 1, num_query, 1, 1
+        )
+
+        reference_points_cam = torch.matmul(
+            lidar2img.to(torch.float32), reference_points.to(torch.float32)
+        ).squeeze(-1)
+        eps = 1e-5
+
+        bev_mask = reference_points_cam[..., 2:3] > eps
+        # reference_points_cam = reference_points_cam[..., 0:2] / torch.maximum(
+        #     reference_points_cam[..., 2:3], torch.ones_like(reference_points_cam[..., 2:3]) * eps)
+        reference_points_cam = reference_points_cam[..., 0:2] / torch.max(
+            reference_points_cam[..., 2:3],
+            torch.ones_like(reference_points_cam[..., 2:3]) * eps,
+        )
+
+        reference_points_cam[..., 0] /= image_shape[1]
+        reference_points_cam[..., 1] /= image_shape[0]
+
+        bev_mask = (
+            bev_mask
+            & (reference_points_cam[..., 1:2] > 0.0)
+            & (reference_points_cam[..., 1:2] < 1.0)
+            & (reference_points_cam[..., 0:1] < 1.0)
+            & (reference_points_cam[..., 0:1] > 0.0)
+        )
+
+        bev_mask = torch.nan_to_num(bev_mask).int()
+
+        reference_points_cam = reference_points_cam.permute(2, 1, 3, 0, 4)
+        bev_mask = bev_mask.permute(2, 1, 3, 0, 4)[..., 0]
+        return reference_points_cam, bev_mask
+
+    def forward_trt(
+        self,
+        bev_query,
+        key,
+        value,
+        *args,
+        lidar2img=None,
+        bev_h=None,
+        bev_w=None,
+        bev_pos=None,
+        spatial_shapes=None,
+        level_start_index=None,
+        valid_ratios=None,
+        prev_bev=None,
+        shift=0.0,
+        image_shape=None,
+        use_prev_bev=None,
+    ):
+        output = bev_query
+        intermediate = []
+
+        ref_3d = self.get_reference_points(
+            bev_h,
+            bev_w,
+            self.pc_range[5] - self.pc_range[2],
+            self.num_points_in_pillar,
+            dim="3d",
+            bs=bev_query.size(1),
+            device=bev_query.device,
+            dtype=bev_query.dtype,
+        )
+        ref_2d = self.get_reference_points(
+            bev_h,
+            bev_w,
+            dim="2d",
+            bs=bev_query.size(1),
+            device=bev_query.device,
+            dtype=bev_query.dtype,
+        )
+
+        reference_points_cam, bev_mask = self.point_sampling_trt(
+            ref_3d, self.pc_range, lidar2img, image_shape
+        )
+
+        # bug: this code should be 'shift_ref_2d = ref_2d.clone()', we keep this bug for reproducing our results in paper.
+        shift_ref_2d = ref_2d.clone()
+        shift_ref_2d = shift_ref_2d + shift[:, None, None, :] * use_prev_bev
+
+        # (num_query, bs, embed_dims) -> (bs, num_query, embed_dims)
+        bev_query = bev_query.permute(1, 0, 2)
+        bev_pos = bev_pos.permute(1, 0, 2)
+        bs, len_bev, num_bev_level, _ = ref_2d.shape
+
+        prev_bev = prev_bev.permute(1, 0, 2)
+        prev_bev = torch.stack([prev_bev, bev_query], 1).reshape(bs * 2, len_bev, -1)
+        hybird_ref_2d = torch.stack([shift_ref_2d, ref_2d], 1).reshape(
+            bs * 2, len_bev, num_bev_level, 2
+        )
+
+        for lid, layer in enumerate(self.layers):
+            output = layer.forward_trt(
+                bev_query,
+                key,
+                value,
+                *args,
+                bev_pos=bev_pos,
+                ref_2d=hybird_ref_2d,
+                ref_3d=ref_3d,
+                bev_h=bev_h,
+                bev_w=bev_w,
+                spatial_shapes=spatial_shapes,
+                level_start_index=level_start_index,
+                reference_points_cam=reference_points_cam,
+                bev_mask=bev_mask,
+                prev_bev=prev_bev,
+                use_prev_bev=use_prev_bev,
+            )
+
+            bev_query = output
+            if self.return_intermediate:
+                intermediate.append(output)
+
+        if self.return_intermediate:
+            return torch.stack(intermediate)
+
+        return output
+
+
+@TRANSFORMER_LAYER_SEQUENCE.register_module(force=True)
+class BEVFormerEncoderTRTP(BEVFormerEncoderTRT):
+    def __init__(self, *args, **kwargs):
+        super(BEVFormerEncoderTRTP, self).__init__(*args, **kwargs)
+
+    @staticmethod
+    def get_reference_points_3d(
+        H, W, Z=8, num_points_in_pillar=4, bs=1, device="cuda", dtype=torch.float,
+    ):
+        # H, W = H[0], W[0]
+        H, W = H.item(), W.item()
+        zs = (
+            torch.linspace(
+                0.5, Z - 0.5, num_points_in_pillar, dtype=dtype, device=device
+            )
+            .view(-1, 1, 1)
+            .repeat(1, H, W)
+            / Z
+        )
+        xs = (
+            torch.linspace(0.5, W - 0.5, W, dtype=dtype, device=device)
+            .view(1, 1, W)
+            .repeat(num_points_in_pillar, H, 1)
+            / W
+        )
+        ys = (
+            torch.linspace(0.5, H - 0.5, H, dtype=dtype, device=device)
+            .view(1, H, 1)
+            .repeat(num_points_in_pillar, 1, W)
+            / H
+        )
+
+        ref_3d = torch.stack((xs, ys, zs), -1).view(1, 4, -1, 3)
+        return ref_3d
+    
+    @staticmethod
+    def get_reference_points_2d(H, W, Z=8, num_points_in_pillar=4, bs=1, device='cuda', dtype=torch.float):
+        """Get the reference points used in SCA and TSA.
+        Args:
+            H, W: spatial shape of bev.
+            Z: hight of pillar.
+            D: sample D points uniformly from each pillar.
+            device (obj:`device`): The device where
+                reference_points should be.
+        Returns:
+            Tensor: reference points used in decoder, has \
+                shape (bs, num_keys, num_levels, 2).
+        """
+        ref_y, ref_x = torch.meshgrid(
+            torch.linspace(
+                0.5, H - 0.5, H, dtype=dtype, device=device),
+            torch.linspace(
+                0.5, W - 0.5, W, dtype=dtype, device=device)
+        )
+        ref_y = ref_y.reshape(-1)[None] / H
+        ref_x = ref_x.reshape(-1)[None] / W
+        ref_2d = torch.stack((ref_x, ref_y), -1)
+        ref_2d = ref_2d.repeat(bs, 1, 1)[:,:,None,...]
+        return ref_2d
+
+    def point_sampling_trt(self, reference_points, pc_range, lidar2img, image_shape):
+        reference_points = reference_points * torch.tensor(
+            [
+                pc_range[3] - pc_range[0],
+                pc_range[4] - pc_range[1],
+                pc_range[5] - pc_range[2],
+            ],
+            dtype=reference_points.dtype,
+            device=reference_points.device,
+        ).view(1, 1, 1, 3) + torch.tensor(
+            pc_range[:3], dtype=reference_points.dtype, device=reference_points.device
+        )
+
+        reference_points = torch.cat(
+            (reference_points, torch.ones_like(reference_points[..., :1])), -1
+        )
+
+        reference_points = reference_points.view(
+            self.num_points_in_pillar, 1, 1, -1, 4, 1
+        )
+        lidar2img = lidar2img.view(1, 1, 6, 1, 4, 4)
+        reference_points_cam = torch.matmul(lidar2img, reference_points)[...,0]
+
+        eps = 1e-5
+        mask_zeros = reference_points_cam.new_zeros(
+            self.num_points_in_pillar,
+            1,
+            6,
+            reference_points_cam.shape[3],
+            1,
+            dtype=torch.float32,
+        )
+        mask_ones = mask_zeros + 1
+        bev_mask = torch.where(
+            reference_points_cam[..., 2:3] > eps, mask_ones, mask_zeros
+        )
+        reference_points_cam = reference_points_cam[..., 0:2] / torch.max(
+            reference_points_cam[..., 2:3],
+            torch.ones_like(reference_points_cam[..., 2:3]) * eps,
+        )
+
+        # reference_points_cam[..., 0] /= image_shape[1]
+        # reference_points_cam[..., 1] /= image_shape[0]
+        reference_points_cam[..., 0] = reference_points_cam[..., 0] / image_shape[1]
+        reference_points_cam[..., 1] = reference_points_cam[..., 1] / image_shape[0]
+
+        # bev_mask *= torch.where(
+        #     reference_points_cam[..., 1:2] > 0.0, mask_ones, mask_zeros
+        # )
+        # bev_mask *= torch.where(
+        #     reference_points_cam[..., 1:2] < 1.0, mask_ones, mask_zeros
+        # )
+        # bev_mask *= torch.where(
+        #     reference_points_cam[..., 0:1] < 1.0, mask_ones, mask_zeros
+        # )
+        # bev_mask *= torch.where(
+        #     reference_points_cam[..., 0:1] > 0.0, mask_ones, mask_zeros
+        # )
+        bev_mask = bev_mask * torch.where(
+            reference_points_cam[..., 1:2] > 0.0, mask_ones, mask_zeros
+        )
+        bev_mask = bev_mask * torch.where(
+            reference_points_cam[..., 1:2] < 1.0, mask_ones, mask_zeros
+        )
+        bev_mask = bev_mask * torch.where(
+            reference_points_cam[..., 0:1] < 1.0, mask_ones, mask_zeros
+        )
+        bev_mask = bev_mask * torch.where(
+            reference_points_cam[..., 0:1] > 0.0, mask_ones, mask_zeros
+        )
+
+        reference_points_cam = reference_points_cam.permute(2, 1, 3, 0, 4)
+        bev_mask = (1 - (1 - bev_mask).prod(0)).view(6, -1, 1)
+        bev_mask = bev_mask / torch.clamp(bev_mask.sum(0, keepdims=True), min=1e-4)
+        return reference_points_cam, bev_mask
+
+    def forward_trt(
+        self,
+        bev_query,
+        key,
+        value,
+        *args,
+        lidar2img=None,
+        bev_h=None,
+        bev_w=None,
+        bev_pos=None,
+        spatial_shapes=None,
+        level_start_index=None,
+        valid_ratios=None,
+        prev_bev=None,
+        shift=None,
+        image_shape=None,
+        use_prev_bev=None,
+    ):
+        output = bev_query
+        intermediate = []
+
+        ref_3d = self.get_reference_points_3d(
+            bev_h,
+            bev_w,
+            self.pc_range[5] - self.pc_range[2],
+            self.num_points_in_pillar,
+            bs=1,
+            device=bev_query.device,
+            dtype=bev_query.dtype,
+        )
+        ref_2d = ref_3d[0, 0, :, :2].view(1, -1, 1, 2).clone()
+
+        reference_points_cam, bev_mask = self.point_sampling_trt(
+            ref_3d, self.pc_range, lidar2img, image_shape
+        )
+
+        # ref_2d += shift.view(1, 1, 1, 2)# * use_prev_bev
+        ref_2d = ref_2d + shift.view(1, 1, 1, 2)
+
+        # (num_query, bs, embed_dims) -> (bs, num_query, embed_dims)
+        bev_query = bev_query.view(1, -1, self.embed_dims)
+        bev_pos = bev_pos.view(1, -1, self.embed_dims)
+        prev_bev = prev_bev.view(1, -1, self.embed_dims)
+
+        prev_bev = torch.cat([prev_bev, bev_query], dim=0)
+        hybird_ref_2d = torch.cat([ref_2d, ref_2d], dim=0)
+        for lid, layer in enumerate(self.layers):
+            output = layer.forward_trt(
+                bev_query,
+                key,
+                value,
+                *args,
+                bev_pos=bev_pos,
+                ref_2d=hybird_ref_2d, 
+                ref_3d=ref_3d,
+                bev_h=bev_h,
+                bev_w=bev_w,
+                spatial_shapes=spatial_shapes,
+                level_start_index=level_start_index, 
+                reference_points_cam=reference_points_cam,
+                bev_mask=bev_mask, 
+                prev_bev=prev_bev,  
+                use_prev_bev=use_prev_bev,
+            )
+
+            bev_query = output
+            if self.return_intermediate:
+                intermediate.append(output)
+        if self.return_intermediate:
+            return torch.stack(intermediate)
+        
+        return output
+
+
+
+@TRANSFORMER_LAYER.register_module(force=True)
+class BEVFormerLayerTRT(BEVFormerLayer):
+    def __init__(
+        self,
+        attn_cfgs,
+        feedforward_channels,
+        ffn_dropout=0.0,
+        operation_order=None,
+        act_cfg=dict(type="ReLU", inplace=True),
+        norm_cfg=dict(type="LN"),
+        ffn_num_fcs=2,
+        # ffn_cfgs=dict(
+        #     type="FFN",
+        #     embed_dims=256,
+        #     feedforward_channels=1024,
+        #     num_fcs=2,
+        #     ffn_drop=0.0,
+        #     act_cfg=dict(type="ReLU", inplace=True),
+        # ),
+        **kwargs,
+    ):
+
+        super(BEVFormerLayer, self).__init__(
+            attn_cfgs=attn_cfgs,
+            feedforward_channels=feedforward_channels,
+            ffn_dropout=ffn_dropout,
+            operation_order=operation_order,
+            act_cfg=act_cfg,
+            norm_cfg=norm_cfg,
+            ffn_num_fcs=ffn_num_fcs,
+            # ffn_cfgs=ffn_cfgs,
+            **kwargs,
+        )
+        self.fp16_enabled = False
+        assert len(operation_order) == 6
+        assert set(operation_order) == set(["self_attn", "norm", "cross_attn", "ffn"])
+
+    def forward_trt(
+        self,
+        query,
+        key=None,
+        value=None,
+        bev_pos=None,
+        query_pos=None,
+        key_pos=None,
+        attn_masks=None,
+        query_key_padding_mask=None,
+        key_padding_mask=None,
+        ref_2d=None,
+        ref_3d=None,
+        bev_h=None,
+        bev_w=None,
+        reference_points_cam=None,
+        mask=None,
+        spatial_shapes=None,
+        level_start_index=None,
+        prev_bev=None,
+        use_prev_bev=None,
+        **kwargs,
+    ):
+        """Forward function for `TransformerDecoderLayer`.
+
+        **kwargs contains some specific arguments of attentions.
+
+        Args:
+            query (Tensor): The input query with shape
+                [num_queries, bs, embed_dims] if
+                self.batch_first is False, else
+                [bs, num_queries embed_dims].
+            key (Tensor): The key tensor with shape [num_keys, bs,
+                embed_dims] if self.batch_first is False, else
+                [bs, num_keys, embed_dims] .
+            value (Tensor): The value tensor with same shape as `key`.
+            query_pos (Tensor): The positional encoding for `query`.
+                Default: None.
+            key_pos (Tensor): The positional encoding for `key`.
+                Default: None.
+            attn_masks (List[Tensor] | None): 2D Tensor used in
+                calculation of corresponding attention. The length of
+                it should equal to the number of `attention` in
+                `operation_order`. Default: None.
+            query_key_padding_mask (Tensor): ByteTensor for `query`, with
+                shape [bs, num_queries]. Only used in `self_attn` layer.
+                Defaults to None.
+            key_padding_mask (Tensor): ByteTensor for `query`, with
+                shape [bs, num_keys]. Default: None.
+
+        Returns:
+            Tensor: forwarded results with shape [num_queries, bs, embed_dims].
+        """
+
+        norm_index = 0
+        attn_index = 0
+        ffn_index = 0
+        identity = query
+        if attn_masks is None:
+            attn_masks = [None for _ in range(self.num_attn)]
+        elif isinstance(attn_masks, torch.Tensor):
+            attn_masks = [copy.deepcopy(attn_masks) for _ in range(self.num_attn)]
+            warnings.warn(
+                f"Use same attn_mask in all attentions in "
+                f"{self.__class__.__name__} "
+            )
+        else:
+            assert len(attn_masks) == self.num_attn, (
+                f"The length of "
+                f"attn_masks {len(attn_masks)} must be equal "
+                f"to the number of attention in "
+                f"operation_order {self.num_attn}"
+            )
+
+        for layer in self.operation_order:
+            if layer == "self_attn":
+                q_bs, q_len_bev, q_c = query.shape
+                prev_bev = use_prev_bev * prev_bev + (1 - use_prev_bev) * torch.stack(
+                    [query, query], 1
+                ).reshape(q_bs * 2, q_len_bev, q_c)
+                query = self.attentions[attn_index].forward_trt(
+                    query,
+                    prev_bev,
+                    prev_bev,
+                    identity if self.pre_norm else None,
+                    query_pos=bev_pos,
+                    key_pos=bev_pos,
+                    attn_mask=attn_masks[attn_index],
+                    key_padding_mask=query_key_padding_mask,
+                    reference_points=ref_2d,
+                    spatial_shapes=torch.tensor([[bev_h, bev_w]], device=query.device),
+                    level_start_index=torch.tensor([0], device=query.device),
+                    **kwargs,
+                )
+                attn_index += 1
+                identity = query
+
+            elif layer == "norm":
+                query = self.norms[norm_index](query)
+                norm_index += 1
+
+            # spaital cross attention
+            elif layer == "cross_attn":
+                query = self.attentions[attn_index].forward_trt(
+                    query,
+                    key,
+                    value,
+                    identity if self.pre_norm else None,
+                    query_pos=query_pos,
+                    key_pos=key_pos,
+                    reference_points=ref_3d,
+                    reference_points_cam=reference_points_cam,
+                    mask=mask,
+                    attn_mask=attn_masks[attn_index],
+                    key_padding_mask=key_padding_mask,
+                    spatial_shapes=spatial_shapes,
+                    level_start_index=level_start_index,
+                    **kwargs,
+                )
+                attn_index += 1
+                identity = query
+
+            elif layer == "ffn":
+                query = self.ffns[ffn_index](query, identity if self.pre_norm else None)
+                ffn_index += 1
+
+        return query
+
+
+@TRANSFORMER_LAYER.register_module(force=True)
+class BEVFormerLayerTRTP(BEVFormerLayerTRT):
+    def __init__(
+        self, *args, **kwargs,
+    ):
+        super(BEVFormerLayerTRTP, self).__init__(*args, **kwargs)
+
+    def forward_trt(
+        self,
+        query,
+        key=None,
+        value=None,
+        bev_pos=None,
+        query_pos=None,
+        key_pos=None,
+        attn_masks=None,
+        query_key_padding_mask=None,
+        key_padding_mask=None,
+        ref_2d=None,
+        ref_3d=None,
+        bev_h=None,
+        bev_w=None,
+        reference_points_cam=None,
+        mask=None,
+        spatial_shapes=None,
+        level_start_index=None,
+        prev_bev=None,
+        use_prev_bev=None,
+        **kwargs,
+    ):
+        """Forward function for `TransformerDecoderLayer`.
+
+        **kwargs contains some specific arguments of attentions.
+
+        Args:
+            query (Tensor): The input query with shape
+                [num_queries, bs, embed_dims] if
+                self.batch_first is False, else
+                [bs, num_queries embed_dims].
+            key (Tensor): The key tensor with shape [num_keys, bs,
+                embed_dims] if self.batch_first is False, else
+                [bs, num_keys, embed_dims] .
+            value (Tensor): The value tensor with same shape as `key`.
+            query_pos (Tensor): The positional encoding for `query`.
+                Default: None.
+            key_pos (Tensor): The positional encoding for `key`.
+                Default: None.
+            attn_masks (List[Tensor] | None): 2D Tensor used in
+                calculation of corresponding attention. The length of
+                it should equal to the number of `attention` in
+                `operation_order`. Default: None.
+            query_key_padding_mask (Tensor): ByteTensor for `query`, with
+                shape [bs, num_queries]. Only used in `self_attn` layer.
+                Defaults to None.
+            key_padding_mask (Tensor): ByteTensor for `query`, with
+                shape [bs, num_keys]. Default: None.
+
+        Returns:
+            Tensor: forwarded results with shape [num_queries, bs, embed_dims].
+        """
+
+        norm_index = 0
+        attn_index = 0
+        ffn_index = 0
+        identity = query
+
+        if attn_masks is None:
+            attn_masks = [None] * self.num_attn
+        elif isinstance(attn_masks, torch.Tensor):
+            attn_masks = [copy.deepcopy(attn_masks) for _ in range(self.num_attn)]
+            warnings.warn(
+                f"Use same attn_mask in all attentions in "
+                f"{self.__class__.__name__} "
+            )
+        else:
+            assert len(attn_masks) == self.num_attn, (
+                f"The length of "
+                f"attn_masks {len(attn_masks)} must be equal "
+                f"to the number of attention in "
+                f"operation_order {self.num_attn}"
+            )
+
+        for layer in self.operation_order:
+            if layer == "self_attn":
+                prev_bev = use_prev_bev * prev_bev + (1 - use_prev_bev) * query.repeat(
+                    2, 1, 1
+                )
+                query = self.attentions[attn_index].forward_trt(
+                    query,
+                    prev_bev,
+                    prev_bev,
+                    identity if self.pre_norm else None,
+                    query_pos=bev_pos,
+                    key_pos=bev_pos,
+                    attn_mask=attn_masks[attn_index],
+                    key_padding_mask=query_key_padding_mask,
+                    reference_points=ref_2d,
+                    spatial_shapes=torch.tensor([[bev_h, bev_w]], device=query.device),
+                    level_start_index=torch.tensor([0], device=query.device),
+                    **kwargs,
+                )
+                # attn_index += 1
+                attn_index = attn_index + 1
+                identity = query
+
+            elif layer == "norm":
+                query = self.norms[norm_index](query)
+                # norm_index += 1
+                norm_index = norm_index + 1
+
+            # spaital cross attention
+            elif layer == "cross_attn":
+                query = self.attentions[attn_index].forward_trt(
+                    query,
+                    key,
+                    value,
+                    identity if self.pre_norm else None,
+                    query_pos=query_pos,
+                    key_pos=key_pos,
+                    reference_points=ref_3d,
+                    reference_points_cam=reference_points_cam,
+                    mask=mask,
+                    attn_mask=attn_masks[attn_index],
+                    key_padding_mask=key_padding_mask,
+                    spatial_shapes=spatial_shapes,
+                    level_start_index=level_start_index,
+                    **kwargs,
+                )
+                # attn_index += 1
+                attn_index = attn_index + 1
+                identity = query
+
+            elif layer == "ffn":
+                query = self.ffns[ffn_index](query, identity if self.pre_norm else None)
+                # ffn_index += 1
+                ffn_index = ffn_index + 1
+
+        return query
\ No newline at end of file
diff --git a/projects/mmdet3d_plugin/uniad/modules/spatial_cross_attention.py b/projects/mmdet3d_plugin/uniad/modules/spatial_cross_attention.py
index 77dfa91..67bd28c 100644
--- a/projects/mmdet3d_plugin/uniad/modules/spatial_cross_attention.py
+++ b/projects/mmdet3d_plugin/uniad/modules/spatial_cross_attention.py
@@ -23,11 +23,14 @@ from mmcv.runner.base_module import BaseModule, ModuleList, Sequential
 from mmcv.utils import ext_loader
 from .multi_scale_deformable_attn_function import MultiScaleDeformableAttnFunction_fp32, \
     MultiScaleDeformableAttnFunction_fp16
+import sys
+sys.path.insert(1, './projects/mmdet3d_plugin/uniad/functions')
+from multi_scale_deformable_attn import multi_scale_deformable_attn
 ext_module = ext_loader.load_ext(
     '_ext', ['ms_deform_attn_backward', 'ms_deform_attn_forward'])
 
 
-@ATTENTION.register_module()
+@ATTENTION.register_module(force=True)
 class SpatialCrossAttention(BaseModule):
     """An attention module used in BEVFormer.
     Args:
@@ -174,7 +177,7 @@ class SpatialCrossAttention(BaseModule):
         return self.dropout(slots) + inp_residual
 
 
-@ATTENTION.register_module()
+@ATTENTION.register_module(force=True)
 class MSDeformableAttention3D(BaseModule):
     """An attention module used in BEVFormer based on Deformable-Detr.
     `Deformable DETR: Deformable Transformers for End-to-End Object Detection.
@@ -396,3 +399,755 @@ class MSDeformableAttention3D(BaseModule):
             output = output.permute(1, 0, 2)
 
         return output
+
+
+@ATTENTION.register_module(force=True)
+class SpatialCrossAttentionTRT(SpatialCrossAttention):
+    """An attention module used in BEVFormer.
+    Args:
+        embed_dims (int): The embedding dimension of Attention.
+            Default: 256.
+        num_cams (int): The number of cameras
+        dropout (float): A Dropout layer on `inp_residual`.
+            Default: 0..
+        init_cfg (obj:`mmcv.ConfigDict`): The Config for initialization.
+            Default: None.
+        deformable_attention: (dict): The config for the deformable attention used in SCA.
+    """
+
+    def __init__(
+        self,
+        embed_dims=256,
+        num_cams=6,
+        pc_range=None,
+        dropout=0.1,
+        init_cfg=None,
+        batch_first=False,
+        deformable_attention=dict(
+            type="MSDeformableAttention3D", embed_dims=256, num_levels=4
+        ),
+        linear_cfg=None,
+        **kwargs,
+    ):
+        super(SpatialCrossAttention, self).__init__(init_cfg)
+
+        # if linear_cfg is None:
+        #     linear_cfg = dict(type="Linear")
+        # linear = LINEAR_LAYERS.get(linear_cfg["type"])
+
+        self.init_cfg = init_cfg
+        self.dropout = nn.Dropout(dropout)
+        self.pc_range = pc_range
+        self.fp16_enabled = False
+        self.deformable_attention = build_attention(deformable_attention)
+        self.embed_dims = embed_dims
+        self.num_cams = num_cams
+        self.output_proj = nn.Linear(embed_dims, embed_dims)
+        self.batch_first = batch_first
+        self.init_weight()
+
+    @force_fp32(apply_to=("query", "key", "value", "query_pos", "reference_points_cam"))
+    def forward_trt(
+        self,
+        query,
+        key,
+        value,
+        residual=None,
+        query_pos=None,
+        key_padding_mask=None,
+        reference_points=None,
+        spatial_shapes=None,
+        reference_points_cam=None,
+        bev_mask=None,
+        level_start_index=None,
+        flag="encoder",
+        **kwargs,
+    ):
+        """Forward Function of Detr3DCrossAtten.
+        Args:
+            query (Tensor): Query of Transformer with shape
+                (num_query, bs, embed_dims).
+            key (Tensor): The key tensor with shape
+                `(num_key, bs, embed_dims)`.
+            value (Tensor): The value tensor with shape
+                `(num_key, bs, embed_dims)`. (B, N, C, H, W)
+            residual (Tensor): The tensor used for addition, with the
+                same shape as `x`. Default None. If None, `x` will be used.
+            query_pos (Tensor): The positional encoding for `query`.
+                Default: None.
+            key_pos (Tensor): The positional encoding for  `key`. Default
+                None.
+            reference_points (Tensor):  The normalized reference
+                points with shape (bs, num_query, 4),
+                all elements is range in [0, 1], top-left (0,0),
+                bottom-right (1, 1), including padding area.
+                or (N, Length_{query}, num_levels, 4), add
+                additional two dimensions is (w, h) to
+                form reference boxes.
+            key_padding_mask (Tensor): ByteTensor for `query`, with
+                shape [bs, num_key].
+            spatial_shapes (Tensor): Spatial shape of features in
+                different level. With shape  (num_levels, 2),
+                last dimension represent (h, w).
+            level_start_index (Tensor): The start index of each level.
+                A tensor has shape (num_levels) and can be represented
+                as [0, h_0*w_0, h_0*w_0+h_1*w_1, ...].
+        Returns:
+             Tensor: forwarded results with shape [num_query, bs, embed_dims].
+        """
+
+        if residual is None:
+            inp_residual = query
+        if query_pos is not None:
+            query = query + query_pos
+
+        bs, num_query, _ = query.size()
+
+        D = reference_points_cam.size(3)
+
+        indexes = (bev_mask.sum(-1) > 0).permute(1, 0, 2)[...,None]
+
+        max_len = bev_mask.shape[2]
+
+        # each camera only interacts with its corresponding BEV queries. This step can  greatly save GPU memory.
+        queries_rebatch = query.new_zeros([bs, self.num_cams, max_len, self.embed_dims])
+        reference_points_rebatch = reference_points_cam.clone().view(
+            bs, self.num_cams, max_len, D, 2
+        )
+
+        for j in range(bs):
+            for i, reference_points_per_img in enumerate(reference_points_cam):
+                queries_rebatch[j, i, :] = query[j, :]
+
+        value = [
+            value[i]
+            .permute(2, 0, 1, 3)
+            .reshape(
+                value[i].shape[2] * self.num_cams, value[i].shape[1], self.embed_dims
+            )
+            for i in range(len(value))
+        ]
+
+        queries = self.deformable_attention.forward_trt(
+            query=queries_rebatch.view(bs * self.num_cams, max_len, self.embed_dims),
+            # key=key,
+            value=value,
+            reference_points=reference_points_rebatch.view(
+                bs * self.num_cams, max_len, D, 2
+            ),
+            spatial_shapes=spatial_shapes,
+            level_start_index=level_start_index,
+        ).view(bs, self.num_cams, max_len, self.embed_dims)
+
+        slots = (queries * indexes).sum(1)
+
+        count = bev_mask.sum(-1) > 0
+        count = count.sum(0)
+        count = torch.clamp(count, min=1.0)
+        slots = slots / count[..., None]
+        slots = self.output_proj(slots)
+
+        return self.dropout(slots) + inp_residual
+
+
+@ATTENTION.register_module()
+class SpatialCrossAttentionTRTP(SpatialCrossAttentionTRT):
+    """An attention module used in BEVFormer.
+    Args:
+        embed_dims (int): The embedding dimension of Attention.
+            Default: 256.
+        num_cams (int): The number of cameras
+        dropout (float): A Dropout layer on `inp_residual`.
+            Default: 0..
+        init_cfg (obj:`mmcv.ConfigDict`): The Config for initialization.
+            Default: None.
+        deformable_attention: (dict): The config for the deformable attention used in SCA.
+    """
+
+    def __init__(
+        self, *args, **kwargs,
+    ):
+        super(SpatialCrossAttentionTRTP, self).__init__(*args, **kwargs)
+
+    @force_fp32(apply_to=("query", "key", "value", "query_pos", "reference_points_cam"))
+    def forward_trt(
+        self,
+        query,
+        key,
+        value,
+        residual=None,
+        query_pos=None,
+        key_padding_mask=None,
+        reference_points=None,
+        spatial_shapes=None,
+        reference_points_cam=None,
+        bev_mask=None,
+        level_start_index=None,
+        flag="encoder",
+        **kwargs,
+    ):
+        """Forward Function of Detr3DCrossAtten.
+        Args:
+            query (Tensor): Query of Transformer with shape
+                (num_query, bs, embed_dims).
+            key (Tensor): The key tensor with shape
+                `(num_key, bs, embed_dims)`.
+            value (Tensor): The value tensor with shape
+                `(num_key, bs, embed_dims)`. (B, N, C, H, W)
+            residual (Tensor): The tensor used for addition, with the
+                same shape as `x`. Default None. If None, `x` will be used.
+            query_pos (Tensor): The positional encoding for `query`.
+                Default: None.
+            key_pos (Tensor): The positional encoding for  `key`. Default
+                None.
+            reference_points (Tensor):  The normalized reference
+                points with shape (bs, num_query, 4),
+                all elements is range in [0, 1], top-left (0,0),
+                bottom-right (1, 1), including padding area.
+                or (N, Length_{query}, num_levels, 4), add
+                additional two dimensions is (w, h) to
+                form reference boxes.
+            key_padding_mask (Tensor): ByteTensor for `query`, with
+                shape [bs, num_key].
+            spatial_shapes (Tensor): Spatial shape of features in
+                different level. With shape  (num_levels, 2),
+                last dimension represent (h, w).
+            level_start_index (Tensor): The start index of each level.
+                A tensor has shape (num_levels) and can be represented
+                as [0, h_0*w_0, h_0*w_0+h_1*w_1, ...].
+        Returns:
+             Tensor: forwarded results with shape [num_query, bs, embed_dims].
+        """
+        if residual is None:
+            inp_residual = query
+        if query_pos is not None:
+            query = query + query_pos
+
+        # each camera only interacts with its corresponding BEV queries. This step can  greatly save GPU memory.
+        query = query.repeat(self.num_cams, 1, 1)
+        reference_points_cam = reference_points_cam.view(
+            self.num_cams, -1, reference_points_cam.size(3), 2
+        )
+
+        value = value.view(self.num_cams, -1, self.embed_dims)
+
+        queries = self.deformable_attention.forward_trt(
+            query=query,
+            # key=key,
+            value=value,
+            reference_points=reference_points_cam,
+            spatial_shapes=spatial_shapes,
+            level_start_index=level_start_index,
+        )
+
+        slots = (queries * bev_mask).sum(0, keepdims=True)
+        slots = self.output_proj(slots)
+
+        return self.dropout(slots) + inp_residual
+
+
+@ATTENTION.register_module()
+class MSDeformableAttention3DTRT(MSDeformableAttention3D):
+    """An attention module used in BEVFormer based on Deformable-Detr.
+    `Deformable DETR: Deformable Transformers for End-to-End Object Detection.
+    <https://arxiv.org/pdf/2010.04159.pdf>`_.
+    Args:
+        embed_dims (int): The embedding dimension of Attention.
+            Default: 256.
+        num_heads (int): Parallel attention heads. Default: 64.
+        num_levels (int): The number of feature map used in
+            Attention. Default: 4.
+        num_points (int): The number of sampling points for
+            each query in each head. Default: 4.
+        im2col_step (int): The step used in image_to_column.
+            Default: 64.
+        dropout (float): A Dropout layer on `inp_identity`.
+            Default: 0.1.
+        batch_first (bool): Key, Query and Value are shape of
+            (batch, n, embed_dim)
+            or (n, batch, embed_dim). Default to False.
+        norm_cfg (dict): Config dict for normalization layer.
+            Default: None.
+        init_cfg (obj:`mmcv.ConfigDict`): The Config for initialization.
+            Default: None.
+    """
+
+    def __init__(
+        self,
+        embed_dims=256,
+        num_heads=8,
+        num_levels=4,
+        num_points=8,
+        im2col_step=64,
+        dropout=0.1,
+        batch_first=True,
+        norm_cfg=None,
+        init_cfg=None,
+        linear_cfg=None,
+    ):
+        super(MSDeformableAttention3D, self).__init__(init_cfg)
+        if embed_dims % num_heads != 0:
+            raise ValueError(
+                f"embed_dims must be divisible by num_heads, "
+                f"but got {embed_dims} and {num_heads}"
+            )
+        dim_per_head = embed_dims // num_heads
+        self.norm_cfg = norm_cfg
+        self.batch_first = batch_first
+        self.output_proj = None
+        self.fp16_enabled = False
+
+        # if linear_cfg is None:
+        #     linear_cfg = dict(type="Linear")
+        # linear = LINEAR_LAYERS.get(linear_cfg["type"])
+
+        # you'd better set dim_per_head to a power of 2
+        # which is more efficient in the CUDA implementation
+        def _is_power_of_2(n):
+            if (not isinstance(n, int)) or (n < 0):
+                raise ValueError(
+                    "invalid input for _is_power_of_2: {} (type: {})".format(n, type(n))
+                )
+            return (n & (n - 1) == 0) and n != 0
+
+        if not _is_power_of_2(dim_per_head):
+            warnings.warn(
+                "You'd better set embed_dims in "
+                "MultiScaleDeformAttention to make "
+                "the dimension of each attention head a power of 2 "
+                "which is more efficient in our CUDA implementation."
+            )
+
+        self.im2col_step = im2col_step
+        self.embed_dims = embed_dims
+        self.num_levels = num_levels
+        self.num_heads = num_heads
+        self.num_points = num_points
+        self.sampling_offsets = nn.Linear(
+            embed_dims, num_heads * num_levels * num_points * 2
+        )
+        self.attention_weights = nn.Linear(embed_dims, num_heads * num_levels * num_points)
+        self.value_proj = nn.Linear(embed_dims, embed_dims)
+
+        self.init_weights()
+
+    def forward_trt(
+        self,
+        query,
+        key=None,
+        value=None,
+        identity=None,
+        query_pos=None,
+        key_padding_mask=None,
+        reference_points=None,
+        spatial_shapes=None,
+        level_start_index=None,
+        **kwargs,
+    ):
+        """Forward Function of MultiScaleDeformAttention.
+        Args:
+            query (Tensor): Query of Transformer with shape
+                ( bs, num_query, embed_dims).
+            key (Tensor): The key tensor with shape
+                `(bs, num_key,  embed_dims)`.
+            value (Tensor): The value tensor with shape
+                `(bs, num_key,  embed_dims)`.
+            identity (Tensor): The tensor used for addition, with the
+                same shape as `query`. Default None. If None,
+                `query` will be used.
+            query_pos (Tensor): The positional encoding for `query`.
+                Default: None.
+            key_pos (Tensor): The positional encoding for `key`. Default
+                None.
+            reference_points (Tensor):  The normalized reference
+                points with shape (bs, num_query, num_levels, 2),
+                all elements is range in [0, 1], top-left (0,0),
+                bottom-right (1, 1), including padding area.
+                or (N, Length_{query}, num_levels, 4), add
+                additional two dimensions is (w, h) to
+                form reference boxes.
+            key_padding_mask (Tensor): ByteTensor for `query`, with
+                shape [bs, num_key].
+            spatial_shapes (Tensor): Spatial shape of features in
+                different levels. With shape (num_levels, 2),
+                last dimension represents (h, w).
+            level_start_index (Tensor): The start index of each level.
+                A tensor has shape ``(num_levels, )`` and can be represented
+                as [0, h_0*w_0, h_0*w_0+h_1*w_1, ...].
+        Returns:
+             Tensor: forwarded results with shape [num_query, bs, embed_dims].
+        """
+        assert isinstance(value, list)
+
+        if identity is None:
+            identity = query
+        if query_pos is not None:
+            query = query + query_pos
+
+        if not self.batch_first:
+            # change to (bs, num_query ,embed_dims)
+            query = query.permute(1, 0, 2)
+            value = [value[i].permute(1, 0, 2) for i in range(len(value))]
+
+        bs, num_query, _ = query.shape
+        num_value = 0
+        for i in range(len(value)):
+            num_value += value[i].shape[1]
+
+        if len(value) == 1:
+            value[0] = self.value_proj(value[0]).view(
+                bs, value[0].shape[1], self.num_heads, -1
+            )
+        else:
+            value = torch.cat(value, dim=1)
+            value = self.value_proj(value)
+            value = [
+                *torch.split(
+                    value,
+                    [
+                        spatial_shapes[i, 0] * spatial_shapes[i, 1]
+                        for i in range(len(spatial_shapes))
+                    ],
+                    dim=1,
+                )
+            ]
+            value = [
+                value[i].view(bs, value[i].shape[1], self.num_heads, -1)
+                for i in range(len(value))
+            ]
+
+        assert (spatial_shapes[:, 0] * spatial_shapes[:, 1]).sum() == num_value
+        assert key_padding_mask is None
+
+        sampling_offsets = self.sampling_offsets(query).view(
+            bs, num_query, self.num_heads, self.num_levels, self.num_points, 2
+        )
+        attention_weights = self.attention_weights(query).view(
+            bs, num_query, self.num_heads, self.num_levels * self.num_points
+        )
+
+        attention_weights = attention_weights.softmax(-1)
+
+        attention_weights = attention_weights.view(
+            bs, num_query, self.num_heads, self.num_levels, self.num_points
+        )
+
+        if reference_points.shape[-1] == 2:
+            """
+            For each BEV query, it owns `num_Z_anchors` in 3D space that having different heights.
+            After proejcting, each BEV query has `num_Z_anchors` reference points in each 2D image.
+            For each referent point, we sample `num_points` sampling points.
+            For `num_Z_anchors` reference points,  it has overall `num_points * num_Z_anchors` sampling points.
+            """
+            offset_normalizer = torch.stack(
+                [spatial_shapes[..., 1], spatial_shapes[..., 0]], -1
+            )
+
+            bs, num_query, num_Z_anchors, xy = reference_points.shape
+            reference_points = reference_points[:, :, None, None, None, :, :]
+            sampling_offsets = (
+                sampling_offsets / offset_normalizer[None, None, None, :, None, :]
+            )
+            (
+                bs,
+                num_query,
+                num_heads,
+                num_levels,
+                num_all_points,
+                xy,
+            ) = sampling_offsets.shape
+            sampling_offsets = sampling_offsets.view(
+                bs,
+                num_query,
+                num_heads,
+                num_levels,
+                num_all_points // num_Z_anchors,
+                num_Z_anchors,
+                xy,
+            )
+            sampling_locations = reference_points + sampling_offsets
+            (
+                bs,
+                num_query,
+                num_heads,
+                num_levels,
+                num_points,
+                num_Z_anchors,
+                xy,
+            ) = sampling_locations.shape
+            assert num_all_points == num_points * num_Z_anchors
+
+            sampling_locations = sampling_locations.view(
+                bs, num_query, num_heads, num_levels, num_all_points, xy
+            )
+
+        elif reference_points.shape[-1] == 4:
+            assert False
+        else:
+            raise ValueError(
+                f"Last dim of reference_points must be"
+                f" 2 or 4, but get {reference_points.shape[-1]} instead."
+            )
+
+        #  sampling_locations.shape: bs, num_query, num_heads, num_levels, num_all_points, 2
+        #  attention_weights.shape: bs, num_query, num_heads, num_levels, num_all_points
+
+        if (
+            torch.cuda.is_available()
+            and value[0].is_cuda
+            and not torch.onnx.is_in_onnx_export()
+        ):
+            value = torch.cat(value, 1)
+            MultiScaleDeformableAttnFunction = MultiScaleDeformableAttnFunction_fp32
+            output = MultiScaleDeformableAttnFunction.apply(
+                value,
+                spatial_shapes,
+                level_start_index,
+                sampling_locations,
+                attention_weights,
+                self.im2col_step,
+            )
+        else:
+            if self.num_levels == 1:
+                output = multi_scale_deformable_attn_pytorch(
+                    value,
+                    spatial_shapes,
+                    sampling_locations,
+                    attention_weights,
+                    num_heads=self.num_heads,
+                    embed_dims=self.embed_dims,
+                    num_levels=self.num_levels,
+                    num_points=self.num_points,
+                    num_queries=int(num_query),
+                    bs=6,
+                )
+            elif self.num_levels == 4:
+                embed_dims_ = self.embed_dims // self.num_heads
+
+                attention_weights_ = attention_weights.transpose(1, 2).reshape(
+                    6 * self.num_heads, 1, num_query, self.num_levels * self.num_points
+                )
+
+                sampling_grids_ = 2 * sampling_locations - 1
+
+                output = sampling_locations.new_zeros(
+                    6 * self.num_heads, embed_dims_, int(num_query)
+                )
+
+                H_0, W_0 = spatial_shapes[0]
+                value_l_0 = (
+                    value[0]
+                    .permute(0, 2, 3, 1)
+                    .reshape(6 * self.num_heads, embed_dims_, H_0, W_0)
+                )
+                sampling_grids_l_0 = (
+                    sampling_grids_[:, :, :, 0].transpose(1, 2).flatten(0, 1)
+                )
+                sampling_value_l_0 = F.grid_sample(
+                    value_l_0,
+                    sampling_grids_l_0,
+                    mode="bilinear",
+                    padding_mode="zeros",
+                    align_corners=False,
+                )
+                # sampling_value_l_0 *= attention_weights_[
+                #     ..., 0 * self.num_points : (0 + 1) * self.num_points
+                # ]
+                sampling_value_l_0 = sampling_value_l_0 * attention_weights_[
+                    ..., 0 * self.num_points : (0 + 1) * self.num_points
+                ]
+                output += sampling_value_l_0.sum(-1)
+
+                H_1, W_1 = spatial_shapes[1]
+                value_l_1 = (
+                    value[1]
+                    .permute(0, 2, 3, 1)
+                    .reshape(6 * self.num_heads, embed_dims_, H_1, W_1)
+                )
+                sampling_grids_l_1 = (
+                    sampling_grids_[:, :, :, 1].transpose(1, 2).flatten(0, 1)
+                )
+                sampling_value_l_1 = F.grid_sample(
+                    value_l_1,
+                    sampling_grids_l_1,
+                    mode="bilinear",
+                    padding_mode="zeros",
+                    align_corners=False,
+                )
+                # sampling_value_l_1 *= attention_weights_[
+                #     ..., 1 * self.num_points : (1 + 1) * self.num_points
+                # ]
+                sampling_value_l_1 = sampling_value_l_1 * attention_weights_[
+                    ..., 1 * self.num_points : (1 + 1) * self.num_points
+                ]
+                output += sampling_value_l_1.sum(-1)
+
+                H_2, W_2 = spatial_shapes[2]
+                value_l_2 = (
+                    value[2]
+                    .permute(0, 2, 3, 1)
+                    .reshape(6 * self.num_heads, embed_dims_, H_2, W_2)
+                )
+                sampling_grids_l_2 = (
+                    sampling_grids_[:, :, :, 2].transpose(1, 2).flatten(0, 1)
+                )
+                sampling_value_l_2 = F.grid_sample(
+                    value_l_2,
+                    sampling_grids_l_2,
+                    mode="bilinear",
+                    padding_mode="zeros",
+                    align_corners=False,
+                )
+                # sampling_value_l_2 *= attention_weights_[
+                #     ..., 2 * self.num_points : (2 + 1) * self.num_points
+                # ]
+                sampling_value_l_2 = sampling_value_l_2 * attention_weights_[
+                    ..., 2 * self.num_points : (2 + 1) * self.num_points
+                ]
+                output += sampling_value_l_2.sum(-1)
+
+                H_3, W_3 = spatial_shapes[3]
+                value_l_3 = (
+                    value[3]
+                    .permute(0, 2, 3, 1)
+                    .reshape(6 * self.num_heads, embed_dims_, H_3, W_3)
+                )
+                sampling_grids_l_3 = (
+                    sampling_grids_[:, :, :, 3].transpose(1, 2).flatten(0, 1)
+                )
+                sampling_value_l_3 = F.grid_sample(
+                    value_l_3,
+                    sampling_grids_l_3,
+                    mode="bilinear",
+                    padding_mode="zeros",
+                    align_corners=False,
+                )
+                # sampling_value_l_3 *= attention_weights_[
+                #     ..., 3 * self.num_points : (3 + 1) * self.num_points
+                # ]
+                sampling_value_l_3 = sampling_value_l_3 * attention_weights_[
+                    ..., 3 * self.num_points : (3 + 1) * self.num_points
+                ]
+                output += sampling_value_l_3.sum(-1)
+
+                output = (
+                    output.view(6, self.num_heads * embed_dims_, num_query)
+                    .transpose(1, 2)
+                    .contiguous()
+                )
+            else:
+                raise RuntimeError
+
+        if not self.batch_first:
+            output = output.permute(1, 0, 2)
+
+        return output
+
+
+@ATTENTION.register_module()
+class MSDeformableAttention3DTRTP(MSDeformableAttention3DTRT):
+    """An attention module used in BEVFormer based on Deformable-Detr.
+    `Deformable DETR: Deformable Transformers for End-to-End Object Detection.
+    <https://arxiv.org/pdf/2010.04159.pdf>`_.
+    Args:
+        embed_dims (int): The embedding dimension of Attention.
+            Default: 256.
+        num_heads (int): Parallel attention heads. Default: 64.
+        num_levels (int): The number of feature map used in
+            Attention. Default: 4.
+        num_points (int): The number of sampling points for
+            each query in each head. Default: 4.
+        im2col_step (int): The step used in image_to_column.
+            Default: 64.
+        dropout (float): A Dropout layer on `inp_identity`.
+            Default: 0.1.
+        batch_first (bool): Key, Query and Value are shape of
+            (batch, n, embed_dim)
+            or (n, batch, embed_dim). Default to False.
+        norm_cfg (dict): Config dict for normalization layer.
+            Default: None.
+        init_cfg (obj:`mmcv.ConfigDict`): The Config for initialization.
+            Default: None.
+    """
+
+    def __init__(self, *args, **kwargs):
+        super(MSDeformableAttention3DTRTP, self).__init__(*args, **kwargs)
+        self.multi_scale_deformable_attn = multi_scale_deformable_attn
+
+    def forward_trt(
+        self,
+        query,
+        key=None,
+        value=None,
+        identity=None,
+        query_pos=None,
+        key_padding_mask=None,
+        reference_points=None,
+        spatial_shapes=None,
+        level_start_index=None,
+        **kwargs,
+    ):
+        """Forward Function of MultiScaleDeformAttention.
+        Args:
+            query (Tensor): Query of Transformer with shape
+                ( bs, num_query, embed_dims).
+            key (Tensor): The key tensor with shape
+                `(bs, num_key,  embed_dims)`.
+            value (Tensor): The value tensor with shape
+                `(bs, num_key,  embed_dims)`.
+            identity (Tensor): The tensor used for addition, with the
+                same shape as `query`. Default None. If None,
+                `query` will be used.
+            query_pos (Tensor): The positional encoding for `query`.
+                Default: None.
+            key_pos (Tensor): The positional encoding for `key`. Default
+                None.
+            reference_points (Tensor):  The normalized reference
+                points with shape (bs, num_query, num_levels, 2),
+                all elements is range in [0, 1], top-left (0,0),
+                bottom-right (1, 1), including padding area.
+                or (N, Length_{query}, num_levels, 4), add
+                additional two dimensions is (w, h) to
+                form reference boxes.
+            key_padding_mask (Tensor): ByteTensor for `query`, with
+                shape [bs, num_key].
+            spatial_shapes (Tensor): Spatial shape of features in
+                different levels. With shape (num_levels, 2),
+                last dimension represents (h, w).
+            level_start_index (Tensor): The start index of each level.
+                A tensor has shape ``(num_levels, )`` and can be represented
+                as [0, h_0*w_0, h_0*w_0+h_1*w_1, ...].
+        Returns:
+             Tensor: forwarded results with shape [num_query, bs, embed_dims].
+        """
+        if identity is None:
+            identity = query
+        if query_pos is not None:
+            query = query + query_pos
+
+        assert self.batch_first
+
+        value = self.value_proj(value).view(
+            6, -1, self.num_heads, self.embed_dims // self.num_heads
+        )
+
+        assert (spatial_shapes[:, 0] * spatial_shapes[:, 1]).sum() == value.shape[1]
+        assert key_padding_mask is None
+
+        sampling_offsets = self.sampling_offsets(query)
+        attention_weights = self.attention_weights(query)
+
+        reference_points = reference_points.reshape(6, -1, 1, 8)
+        sampling_offsets = sampling_offsets.view(
+            *sampling_offsets.shape[:2], self.num_heads, -1
+        )
+
+        attention_weights = attention_weights.view(
+            *attention_weights.shape[:2], self.num_heads, -1
+        )
+
+        output = self.multi_scale_deformable_attn(
+            value, spatial_shapes, reference_points, sampling_offsets, attention_weights
+        ).flatten(2)
+
+        return output
diff --git a/projects/mmdet3d_plugin/uniad/modules/temporal_self_attention.py b/projects/mmdet3d_plugin/uniad/modules/temporal_self_attention.py
index f846b4b..e7582fd 100644
--- a/projects/mmdet3d_plugin/uniad/modules/temporal_self_attention.py
+++ b/projects/mmdet3d_plugin/uniad/modules/temporal_self_attention.py
@@ -19,9 +19,13 @@ from mmcv.utils import (ConfigDict, build_from_cfg, deprecated_api_warning,
 from mmcv.utils import ext_loader
 ext_module = ext_loader.load_ext(
     '_ext', ['ms_deform_attn_backward', 'ms_deform_attn_forward'])
+import sys
+sys.path.insert(1, './projects/mmdet3d_plugin/uniad/functions')
+from multi_scale_deformable_attn import multi_scale_deformable_attn
+
 
 
-@ATTENTION.register_module()
+@ATTENTION.register_module(force=True)
 class TemporalSelfAttention(BaseModule):
     """An attention module used in BEVFormer based on Deformable-Detr.
 
@@ -267,3 +271,439 @@ class TemporalSelfAttention(BaseModule):
             output = output.permute(1, 0, 2)
 
         return self.dropout(output) + identity
+
+
+@ATTENTION.register_module(force=True)
+class TemporalSelfAttentionTRT(TemporalSelfAttention):
+    """An attention module used in BEVFormer based on Deformable-Detr.
+
+    `Deformable DETR: Deformable Transformers for End-to-End Object Detection.
+    <https://arxiv.org/pdf/2010.04159.pdf>`_.
+
+    Args:
+        embed_dims (int): The embedding dimension of Attention.
+            Default: 256.
+        num_heads (int): Parallel attention heads. Default: 64.
+        num_levels (int): The number of feature map used in
+            Attention. Default: 4.
+        num_points (int): The number of sampling points for
+            each query in each head. Default: 4.
+        im2col_step (int): The step used in image_to_column.
+            Default: 64.
+        dropout (float): A Dropout layer on `inp_identity`.
+            Default: 0.1.
+        batch_first (bool): Key, Query and Value are shape of
+            (batch, n, embed_dim)
+            or (n, batch, embed_dim). Default to True.
+        norm_cfg (dict): Config dict for normalization layer.
+            Default: None.
+        init_cfg (obj:`mmcv.ConfigDict`): The Config for initialization.
+            Default: None.
+        num_bev_queue (int): In this version, we only use one history BEV and one currenct BEV.
+         the length of BEV queue is 2.
+    """
+
+    def __init__(
+        self,
+        embed_dims=256,
+        num_heads=8,
+        num_levels=4,
+        num_points=4,
+        num_bev_queue=2,
+        im2col_step=64,
+        dropout=0.1,
+        batch_first=True,
+        norm_cfg=None,
+        init_cfg=None,
+        linear_cfg=None,
+    ):
+
+        super(TemporalSelfAttention, self).__init__(init_cfg)
+        if embed_dims % num_heads != 0:
+            raise ValueError(
+                f"embed_dims must be divisible by num_heads, "
+                f"but got {embed_dims} and {num_heads}"
+            )
+        dim_per_head = embed_dims // num_heads
+        self.norm_cfg = norm_cfg
+        self.dropout = nn.Dropout(dropout)
+        self.batch_first = batch_first
+        self.fp16_enabled = False
+
+        # if linear_cfg is None:
+        #     linear_cfg = dict(type="Linear")
+        # linear = LINEAR_LAYERS.get(linear_cfg["type"])
+
+        # you'd better set dim_per_head to a power of 2
+        # which is more efficient in the CUDA implementation
+        def _is_power_of_2(n):
+            if (not isinstance(n, int)) or (n < 0):
+                raise ValueError(
+                    "invalid input for _is_power_of_2: {} (type: {})".format(n, type(n))
+                )
+            return (n & (n - 1) == 0) and n != 0
+
+        if not _is_power_of_2(dim_per_head):
+            warnings.warn(
+                "You'd better set embed_dims in "
+                "MultiScaleDeformAttention to make "
+                "the dimension of each attention head a power of 2 "
+                "which is more efficient in our CUDA implementation."
+            )
+
+        self.im2col_step = im2col_step
+        self.embed_dims = embed_dims
+        self.num_levels = num_levels
+        self.num_heads = num_heads
+        self.num_points = num_points
+        self.num_bev_queue = num_bev_queue
+        self.sampling_offsets = nn.Linear(
+            embed_dims * self.num_bev_queue,
+            num_bev_queue * num_heads * num_levels * num_points * 2,
+        )
+        self.attention_weights = nn.Linear(
+            embed_dims * self.num_bev_queue,
+            num_bev_queue * num_heads * num_levels * num_points,
+        )
+        self.value_proj = nn.Linear(embed_dims, embed_dims)
+        self.output_proj = nn.Linear(embed_dims, embed_dims)
+        self.init_weights()
+
+    def forward_trt(
+        self,
+        query,
+        key=None,
+        value=None,
+        identity=None,
+        query_pos=None,
+        key_padding_mask=None,
+        reference_points=None,
+        spatial_shapes=None,
+        level_start_index=None,
+        flag="decoder",
+        **kwargs,
+    ):
+        """Forward Function of MultiScaleDeformAttention.
+
+        Args:
+            query (Tensor): Query of Transformer with shape
+                (num_query, bs, embed_dims).
+            key (Tensor): The key tensor with shape
+                `(num_key, bs, embed_dims)`.
+            value (Tensor): The value tensor with shape
+                `(num_key, bs, embed_dims)`.
+            identity (Tensor): The tensor used for addition, with the
+                same shape as `query`. Default None. If None,
+                `query` will be used.
+            query_pos (Tensor): The positional encoding for `query`.
+                Default: None.
+            key_pos (Tensor): The positional encoding for `key`. Default
+                None.
+            reference_points (Tensor):  The normalized reference
+                points with shape (bs, num_query, num_levels, 2),
+                all elements is range in [0, 1], top-left (0,0),
+                bottom-right (1, 1), including padding area.
+                or (N, Length_{query}, num_levels, 4), add
+                additional two dimensions is (w, h) to
+                form reference boxes.
+            key_padding_mask (Tensor): ByteTensor for `query`, with
+                shape [bs, num_key].
+            spatial_shapes (Tensor): Spatial shape of features in
+                different levels. With shape (num_levels, 2),
+                last dimension represents (h, w).
+            level_start_index (Tensor): The start index of each level.
+                A tensor has shape ``(num_levels, )`` and can be represented
+                as [0, h_0*w_0, h_0*w_0+h_1*w_1, ...].
+
+        Returns:
+             Tensor: forwarded results with shape [num_query, bs, embed_dims].
+        """
+
+        if value is None:
+            assert self.batch_first
+            bs, len_bev, c = query.shape
+            value = torch.stack([query, query], 1).reshape(bs * 2, len_bev, c)
+
+        if identity is None:
+            identity = query
+        if query_pos is not None:
+            query = query + query_pos
+        if not self.batch_first:
+            # change to (bs, num_query ,embed_dims)
+            query = query.permute(1, 0, 2)
+            value = value.permute(1, 0, 2)
+        bs, num_query, embed_dims = query.shape
+        _, num_value, _ = value.shape
+        assert (spatial_shapes[:, 0] * spatial_shapes[:, 1]).sum() == num_value
+        assert self.num_bev_queue == 2
+
+        query = torch.cat([value[:1], query], -1)
+
+        value = self.value_proj(value)
+
+        if key_padding_mask is not None:
+            value = value.masked_fill(key_padding_mask[..., None], 0.0)
+
+        value = value.reshape(bs * self.num_bev_queue, num_value, self.num_heads, -1)
+
+        sampling_offsets = self.sampling_offsets(query)
+        sampling_offsets = sampling_offsets.view(
+            bs,
+            num_query,
+            self.num_heads,
+            self.num_bev_queue,
+            self.num_levels,
+            self.num_points,
+            2,
+        )
+        attention_weights = self.attention_weights(query).view(
+            bs,
+            num_query,
+            self.num_heads,
+            self.num_bev_queue,
+            self.num_levels * self.num_points,
+        )
+        attention_weights = attention_weights.softmax(-1)
+
+        attention_weights = attention_weights.view(
+            bs,
+            num_query,
+            self.num_heads,
+            self.num_bev_queue,
+            self.num_levels,
+            self.num_points,
+        )
+
+        attention_weights = (
+            attention_weights.permute(0, 3, 1, 2, 4, 5)
+            .reshape(
+                bs * self.num_bev_queue,
+                num_query,
+                self.num_heads,
+                self.num_levels,
+                self.num_points,
+            )
+            .contiguous()
+        )
+        sampling_offsets = sampling_offsets.permute(0, 3, 1, 2, 4, 5, 6).reshape(
+            bs * self.num_bev_queue,
+            num_query,
+            self.num_heads,
+            self.num_levels,
+            self.num_points,
+            2,
+        )
+
+        if reference_points.shape[-1] == 2:
+            offset_normalizer = torch.stack(
+                [spatial_shapes[..., 1], spatial_shapes[..., 0]], -1
+            )
+            sampling_locations = (
+                reference_points[:, :, None, :, None, :]
+                + sampling_offsets / offset_normalizer[None, None, None, :, None, :]
+            )
+
+        elif reference_points.shape[-1] == 4:
+            sampling_locations = (
+                reference_points[:, :, None, :, None, :2]
+                + sampling_offsets
+                / self.num_points
+                * reference_points[:, :, None, :, None, 2:]
+                * 0.5
+            )
+        else:
+            raise ValueError(
+                f"Last dim of reference_points must be"
+                f" 2 or 4, but get {reference_points.shape[-1]} instead."
+            )
+
+        if (
+            torch.cuda.is_available()
+            and value.is_cuda
+            and not torch.onnx.is_in_onnx_export()
+        ):
+            MultiScaleDeformableAttnFunction = MultiScaleDeformableAttnFunction_fp32
+            output = MultiScaleDeformableAttnFunction.apply(
+                value,
+                spatial_shapes,
+                level_start_index,
+                sampling_locations,
+                attention_weights,
+                self.im2col_step,
+            )
+        else:
+            output = multi_scale_deformable_attn_pytorch(
+                value,
+                spatial_shapes,
+                sampling_locations,
+                attention_weights,
+                num_heads=self.num_heads,
+                embed_dims=self.embed_dims,
+                num_levels=self.num_levels,
+                num_points=self.num_points,
+                num_queries=int(num_query),
+                bs=2,
+            )
+
+        # output shape (bs*num_bev_queue, num_query, embed_dims)
+        # (bs*num_bev_queue, num_query, embed_dims)-> (num_query, embed_dims, bs*num_bev_queue)
+        output = output.permute(1, 2, 0)
+
+        # fuse history value and current value
+        # (num_query, embed_dims, bs*num_bev_queue)-> (num_query, embed_dims, bs, num_bev_queue)
+        output = output.view(num_query, embed_dims, bs, self.num_bev_queue)
+        output = output.mean(-1)
+
+        # (num_query, embed_dims, bs)-> (bs, num_query, embed_dims)
+        output = output.permute(2, 0, 1)
+
+        output = self.output_proj(output)
+
+        if not self.batch_first:
+            output = output.permute(1, 0, 2)
+
+        return self.dropout(output) + identity
+
+
+@ATTENTION.register_module(force=True)
+class TemporalSelfAttentionTRTP(TemporalSelfAttentionTRT):
+    """An attention module used in BEVFormer based on Deformable-Detr.
+
+    `Deformable DETR: Deformable Transformers for End-to-End Object Detection.
+    <https://arxiv.org/pdf/2010.04159.pdf>`_.
+
+    Args:
+        embed_dims (int): The embedding dimension of Attention.
+            Default: 256.
+        num_heads (int): Parallel attention heads. Default: 64.
+        num_levels (int): The number of feature map used in
+            Attention. Default: 4.
+        num_points (int): The number of sampling points for
+            each query in each head. Default: 4.
+        im2col_step (int): The step used in image_to_column.
+            Default: 64.
+        dropout (float): A Dropout layer on `inp_identity`.
+            Default: 0.1.
+        batch_first (bool): Key, Query and Value are shape of
+            (batch, n, embed_dim)
+            or (n, batch, embed_dim). Default to True.
+        norm_cfg (dict): Config dict for normalization layer.
+            Default: None.
+        init_cfg (obj:`mmcv.ConfigDict`): The Config for initialization.
+            Default: None.
+        num_bev_queue (int): In this version, we only use one history BEV and one currenct BEV.
+         the length of BEV queue is 2.
+    """
+
+    def __init__(self, *args, **kwargs):
+        super(TemporalSelfAttentionTRTP, self).__init__(*args, **kwargs)
+        self.multi_scale_deformable_attn = multi_scale_deformable_attn
+
+    def forward_trt(
+        self,
+        query,
+        key=None,
+        value=None,
+        identity=None,
+        query_pos=None,
+        key_padding_mask=None,
+        reference_points=None,
+        spatial_shapes=None,
+        level_start_index=None,
+        flag="decoder",
+        **kwargs,
+    ):
+        """Forward Function of MultiScaleDeformAttention.
+
+        Args:
+            query (Tensor): Query of Transformer with shape
+                (num_query, bs, embed_dims).
+            key (Tensor): The key tensor with shape
+                `(num_key, bs, embed_dims)`.
+            value (Tensor): The value tensor with shape
+                `(num_key, bs, embed_dims)`.
+            identity (Tensor): The tensor used for addition, with the
+                same shape as `query`. Default None. If None,
+                `query` will be used.
+            query_pos (Tensor): The positional encoding for `query`.
+                Default: None.
+            key_pos (Tensor): The positional encoding for `key`. Default
+                None.
+            reference_points (Tensor):  The normalized reference
+                points with shape (bs, num_query, num_levels, 2),
+                all elements is range in [0, 1], top-left (0,0),
+                bottom-right (1, 1), including padding area.
+                or (N, Length_{query}, num_levels, 4), add
+                additional two dimensions is (w, h) to
+                form reference boxes.
+            key_padding_mask (Tensor): ByteTensor for `query`, with
+                shape [bs, num_key].
+            spatial_shapes (Tensor): Spatial shape of features in
+                different levels. With shape (num_levels, 2),
+                last dimension represents (h, w).
+            level_start_index (Tensor): The start index of each level.
+                A tensor has shape ``(num_levels, )`` and can be represented
+                as [0, h_0*w_0, h_0*w_0+h_1*w_1, ...].
+
+        Returns:
+             Tensor: forwarded results with shape [num_query, bs, embed_dims].
+        """
+
+        assert self.batch_first
+        if value is None:
+            value = query.repeat(2, 1, 1)
+
+        if identity is None:
+            identity = query
+        if query_pos is not None:
+            query = query + query_pos
+
+        assert (spatial_shapes[:, 0] * spatial_shapes[:, 1]).sum() == value.shape[1]
+        assert self.num_bev_queue == 2
+
+        query = torch.cat([value[:1], query], -1)
+
+        value = self.value_proj(value)
+
+        if key_padding_mask is not None:
+            value = value.masked_fill(key_padding_mask[..., None], 0.0)
+
+        value = value.view(
+            self.num_bev_queue, -1, self.num_heads, self.embed_dims // self.num_heads
+        )
+
+        sampling_offsets = self.sampling_offsets(query).view(
+            1,
+            -1,
+            self.num_heads,
+            self.num_bev_queue,
+            self.num_levels,
+            self.num_points,
+            2,
+        )
+        attention_weights = self.attention_weights(query).view(
+            1, -1, self.num_heads, self.num_bev_queue, self.num_levels, self.num_points,
+        )
+
+        attention_weights = attention_weights.permute(0, 3, 1, 2, 4, 5).contiguous()
+        sampling_offsets = sampling_offsets.permute(0, 3, 1, 2, 4, 5, 6).contiguous()
+
+        if torch.onnx.is_in_onnx_export():
+            assert value.is_cuda
+        attention_weights = attention_weights.view(
+            *attention_weights.shape[1:3], self.num_heads, -1
+        )
+        sampling_offsets = sampling_offsets.view(
+            *sampling_offsets.shape[1:3], self.num_heads, -1
+        )
+        output = self.multi_scale_deformable_attn(
+            value, spatial_shapes, reference_points, sampling_offsets, attention_weights
+        ).flatten(2)
+
+        # output shape (bs*num_bev_queue, num_query, embed_dims)
+        # (bs*num_bev_queue, num_query, embed_dims)-> (num_query, embed_dims, bs*num_bev_queue)
+        output = torch.mean(output, keepdim=True, dim=0)
+
+        output = self.output_proj(output)
+
+        return self.dropout(output) + identity
\ No newline at end of file
diff --git a/projects/mmdet3d_plugin/uniad/modules/transformer.py b/projects/mmdet3d_plugin/uniad/modules/transformer.py
index bb5fae8..fe9bf7c 100644
--- a/projects/mmdet3d_plugin/uniad/modules/transformer.py
+++ b/projects/mmdet3d_plugin/uniad/modules/transformer.py
@@ -19,8 +16,11 @@ from .temporal_self_attention import TemporalSelfAttention
 from .spatial_cross_attention import MSDeformableAttention3D
 from .decoder import CustomMSDeformableAttention
 from mmcv.runner import force_fp32, auto_fp16
+import sys
+sys.path.insert(1, './projects/mmdet3d_plugin/uniad/functions')
+from rotate import rotate as rotate_trt
 
-@TRANSFORMER.register_module()
+@TRANSFORMER.register_module(force=True)
 class PerceptionTransformer(BaseModule):
     """Implements the Detr3D transformer.
     Args:
@@ -150,7 +153,7 @@ class PerceptionTransformer(BaseModule):
 
         # add can bus signals
         can_bus = bev_queries.new_tensor(
-            [each['can_bus'] for each in img_metas])  # [:, :]
+            [each['can_bus'] for each in img_metas])  
         can_bus = self.can_bus_mlp(can_bus)[None, :, :]
         bev_queries = bev_queries + can_bus * self.use_can_bus
 
@@ -229,4 +232,357 @@ class PerceptionTransformer(BaseModule):
         )
         inter_references_out = inter_references
 
-        return inter_states, init_reference_out, inter_references_out
\ No newline at end of file
+        return inter_states, init_reference_out, inter_references_out
+
+
+@TRANSFORMER.register_module(force=True)
+class PerceptionTransformerUniADTRT(PerceptionTransformer):
+    def __init__(
+        self,
+        num_feature_levels=4,
+        num_cams=6,
+        two_stage_num_proposals=300,
+        encoder=None,
+        decoder=None,
+        embed_dims=256,
+        rotate_prev_bev=True,
+        use_shift=True,
+        use_can_bus=True,
+        can_bus_norm=True,
+        use_cams_embeds=True,
+        rotate_center=[100, 100],
+        linear_cfg=None,
+        **kwargs
+    ):
+        super(PerceptionTransformer, self).__init__(**kwargs)
+        self.encoder = build_transformer_layer_sequence(encoder)
+        self.decoder = build_transformer_layer_sequence(decoder)
+        self.embed_dims = embed_dims
+        self.num_feature_levels = num_feature_levels
+        self.num_cams = num_cams
+        self.fp16_enabled = False
+
+        self.rotate_prev_bev = rotate_prev_bev
+        self.use_shift = use_shift
+        self.use_can_bus = use_can_bus
+        self.can_bus_norm = can_bus_norm
+        self.use_cams_embeds = use_cams_embeds
+
+        self.two_stage_num_proposals = two_stage_num_proposals
+
+        # if linear_cfg is None:
+        #     linear_cfg = dict(type="Linear")
+        # self.linear = LINEAR_LAYERS.get(linear_cfg["type"])
+
+        self.init_layers()
+        self.rotate_center = rotate_center
+
+    def init_layers(self):
+        """Initialize layers of the Detr3DTransformer."""
+        self.level_embeds = nn.Parameter(
+            torch.Tensor(self.num_feature_levels, self.embed_dims)
+        )
+        self.cams_embeds = nn.Parameter(torch.Tensor(self.num_cams, self.embed_dims))
+        self.can_bus_mlp = nn.Sequential(
+            nn.Linear(18, self.embed_dims // 2),
+            nn.ReLU(inplace=True),
+            nn.Linear(self.embed_dims // 2, self.embed_dims),
+            nn.ReLU(inplace=True),
+        )
+        if self.can_bus_norm:
+            self.can_bus_mlp.add_module("norm", nn.LayerNorm(self.embed_dims))
+
+    def get_bev_features_trt(
+        self,
+        mlvl_feats,
+        bev_queries,
+        bev_h,
+        bev_w,
+        can_bus,
+        lidar2img,
+        grid_length=[0.512, 0.512],
+        bev_pos=None,
+        prev_bev=None,
+        image_shape=None,
+        use_prev_bev=None,
+    ):  
+        bev_h, bev_w = bev_h[0].int(), bev_w[0].int()
+        bs = mlvl_feats[0].size(0)
+        bev_queries = bev_queries[:,None,...].repeat(1, bs, 1)
+        bev_pos = bev_pos.flatten(2).permute(2, 0, 1)
+
+        # obtain rotation angle and shift with ego motion
+        delta_x = can_bus[0:1]
+        delta_y = can_bus[1:2]
+        ego_angle = can_bus[-2:-1] / np.pi * 180
+
+        grid_length_y = grid_length[0].item()
+        grid_length_x = grid_length[1].item()
+        translation_length = torch.sqrt(delta_x ** 2 + delta_y ** 2)
+        # translation_angle = torch.atan2(delta_y, delta_x) / np.pi * 180
+        translation_angle = (
+            (
+                torch.atan(delta_y / (delta_x + 1e-8))
+                + ((1 - torch.sign(delta_x)) / 2) * torch.sign(delta_y) * np.pi
+            )
+            / np.pi
+            * 180
+        )
+        bev_angle = ego_angle - translation_angle
+        shift_y = (
+            translation_length
+            * torch.cos(bev_angle / 180 * np.pi)
+            / grid_length_y
+            / bev_h
+        )
+        shift_x = (
+            translation_length
+            * torch.sin(bev_angle / 180 * np.pi)
+            / grid_length_x
+            / bev_w
+        )
+        # shift_y = shift_y * int(self.use_shift)
+        # shift_x = shift_x * int(self.use_shift)
+        shift_y = shift_y * torch.tensor(self.use_shift, device=shift_y.device).int()
+        shift_x = shift_x * torch.tensor(self.use_shift, device=shift_x.device).int()
+        shift = torch.stack([shift_x, shift_y]).permute(1, 0)
+
+        if self.rotate_prev_bev:
+            for i in range(bs):
+                rotation_angle = can_bus[-1]
+                tmp_prev_bev = prev_bev[:, i].reshape(bev_h, bev_w, -1).permute(2, 0, 1)
+                tmp_prev_bev = rotate(tmp_prev_bev, rotation_angle.item(), center=self.rotate_center)
+                tmp_prev_bev = tmp_prev_bev.permute(1, 2, 0).reshape(
+                    bev_h * bev_w, 1, -1
+                )
+                prev_bev[:, i] = tmp_prev_bev[:, 0]
+
+        # add can bus signals
+        can_bus = can_bus[None, ...]
+        can_bus = self.can_bus_mlp(can_bus)[None, :, :]
+        bev_queries = bev_queries + can_bus * torch.tensor(self.use_can_bus, device=can_bus.device).int()
+
+        feat_flatten = []
+        spatial_shapes = []
+        for lvl, feat in enumerate(mlvl_feats):
+            bs, num_cam, c, h, w = feat.shape
+            spatial_shape = (h, w)
+            feat = feat.flatten(3).permute(1, 0, 3, 2)
+            if self.use_cams_embeds:
+                feat = feat + self.cams_embeds[:, None, None, :].to(feat.dtype)
+            feat = feat + self.level_embeds[None, None, lvl : lvl + 1, :].to(feat.dtype)
+            spatial_shapes.append(spatial_shape)
+            feat_flatten.append(feat)
+
+        spatial_shapes = torch.as_tensor(
+            spatial_shapes, dtype=torch.long, device=bev_pos.device
+        )
+        level_start_index = torch.cat(
+            (spatial_shapes.new_zeros((1,)), spatial_shapes.prod(1).cumsum(0)[:-1])
+        )
+
+        for i in range(len(feat_flatten)):
+            feat_flatten[i] = feat_flatten[i].permute(0, 2, 1, 3)
+        bev_embed = self.encoder.forward_trt(
+            bev_queries,
+            feat_flatten,
+            feat_flatten,
+            bev_h=torch.tensor([bev_h], device=bev_pos.device),
+            bev_w=torch.tensor([bev_w], device=bev_pos.device),
+            bev_pos=bev_pos,
+            spatial_shapes=spatial_shapes,
+            level_start_index=level_start_index,
+            prev_bev=prev_bev,
+            shift=shift,
+            lidar2img=lidar2img,
+            image_shape=image_shape,
+            use_prev_bev=use_prev_bev,
+        )
+
+        return bev_embed
+
+
+    def get_states_and_refs_trt(
+        self,
+        bev_embed,
+        object_query_embed,
+        bev_h,
+        bev_w,
+        reference_points,
+        reg_branches=None,
+        cls_branches=None,
+    ):
+        bs = bev_embed.shape[1]
+        query_pos, query = torch.split(
+            object_query_embed, self.embed_dims, dim=1)
+        query_pos = query_pos[None,...].expand(bs, -1, -1)
+        query = query[None,...].expand(bs, -1, -1)
+
+        reference_points = reference_points[None,...].expand(bs, -1, -1)
+        reference_points = reference_points.sigmoid()
+
+        init_reference_out = reference_points
+        query = query.permute(1, 0, 2)
+        query_pos = query_pos.permute(1, 0, 2)
+        inter_states, inter_references = self.decoder(
+            query=query,
+            key=None,
+            value=bev_embed,
+            query_pos=query_pos,
+            reference_points=reference_points,
+            reg_branches=reg_branches,
+            cls_branches=cls_branches,
+            spatial_shapes=torch.tensor([[bev_h, bev_w]], device=query.device),
+            level_start_index=torch.tensor([0], device=query.device),
+        )
+        inter_references_out = inter_references
+
+        return inter_states, init_reference_out, inter_references_out
+    
+
+@TRANSFORMER.register_module(force=True)
+class PerceptionTransformerUniADTRTP(PerceptionTransformerUniADTRT):
+    def __init__(
+        self,
+        *args,
+        **kwargs
+    ):
+        super(PerceptionTransformerUniADTRTP, self).__init__(*args, **kwargs)
+        self.rotate = rotate_trt
+
+
+    def get_bev_features_trt(
+        self,
+        mlvl_feats,
+        bev_queries,
+        bev_h,
+        bev_w,
+        can_bus,
+        lidar2img,
+        grid_length=[0.512, 0.512],
+        bev_pos=None,
+        prev_bev=None,
+        image_shape=None,
+        use_prev_bev=None,
+    ):
+        bev_queries = bev_queries[:, None, ...]
+        bev_pos = bev_pos.flatten(2).permute(2, 0, 1)
+
+        # obtain rotation angle and shift with ego motion
+        delta_x = can_bus[0:1]
+        delta_y = can_bus[1:2]
+        ego_angle = can_bus[-2:-1] / np.pi * 180
+
+        grid_length_y = grid_length[0]
+        grid_length_x = grid_length[1]
+        translation_length = torch.sqrt(delta_x ** 2 + delta_y ** 2)
+        # translation_angle = torch.atan2(delta_y, delta_x) / np.pi * 180
+        translation_angle = (
+            (
+                torch.atan(delta_y / (delta_x + 1e-8))
+                + ((1 - torch.sign(delta_x)) / 2) * torch.sign(delta_y) * np.pi
+            )
+            / np.pi
+            * 180
+        )
+        bev_angle = ego_angle - translation_angle
+        shift_y = (
+            translation_length
+            * torch.cos(bev_angle / 180 * np.pi)
+            / grid_length_y
+            / bev_h
+        )
+        shift_x = (
+            translation_length
+            * torch.sin(bev_angle / 180 * np.pi)
+            / grid_length_x
+            / bev_w
+        )
+        # shift_y = shift_y * int(self.use_shift)
+        # shift_x = shift_x * int(self.use_shift)
+        shift_y = shift_y * torch.tensor(self.use_shift, device=shift_y.device).int()
+        shift_x = shift_x * torch.tensor(self.use_shift, device=shift_x.device).int()
+        shift = torch.cat([shift_x, shift_y])[None, ...]
+
+        if self.rotate_prev_bev:
+            rotation_angle = can_bus[-1]
+            prev_bev = self.rotate(
+                prev_bev.view(bev_h, bev_w, -1).permute(2, 0, 1),
+                rotation_angle,
+                center=prev_bev.new_tensor(self.rotate_center),
+            )
+            prev_bev = prev_bev.permute(1, 2, 0).reshape(bev_h * bev_w, 1, -1)
+
+        # add can bus signals
+        can_bus = can_bus.view(1, -1)
+        can_bus = self.can_bus_mlp(can_bus).view(1, 1, -1)
+        # bev_queries = bev_queries + can_bus * int(self.use_can_bus)
+        bev_queries = bev_queries + can_bus * torch.tensor(self.use_can_bus, device=can_bus.device).int()
+
+        feat_flatten = mlvl_feats[0].new_zeros(
+            6, 1, 0, self.embed_dims, dtype=torch.float32
+        )
+        spatial_shapes = mlvl_feats[0].new_zeros(len(mlvl_feats), 2, dtype=torch.long)
+        for lvl, feat in enumerate(mlvl_feats):
+            _, _, _, h, w = feat.shape
+            feat = feat.flatten(3).permute(1, 0, 3, 2)
+            if self.use_cams_embeds:
+                feat = feat + self.cams_embeds.view(6, 1, 1, self.embed_dims)
+            feat = feat + self.level_embeds[lvl].view(1, 1, 1, -1)
+            spatial_shapes[lvl, 0] = h
+            spatial_shapes[lvl, 1] = w
+            feat_flatten = torch.cat([feat_flatten, feat], dim=2)
+        feat_flatten = feat_flatten.view(6, -1, 1, self.embed_dims)
+
+        bev_embed = self.encoder.forward_trt(
+            bev_queries,
+            feat_flatten,
+            feat_flatten,
+            lidar2img=lidar2img,
+            bev_h=bev_h,
+            bev_w=bev_w,
+            bev_pos=bev_pos,
+            spatial_shapes=spatial_shapes,
+            level_start_index=None,
+            prev_bev=prev_bev,
+            shift=shift,
+            image_shape=image_shape,
+            use_prev_bev=use_prev_bev,
+        )
+
+        return bev_embed
+
+
+    def get_states_and_refs_trt(
+        self,
+        bev_embed,
+        object_query_embed,
+        bev_h,
+        bev_w,
+        reference_points,
+        reg_branches=None,
+        cls_branches=None,
+    ):
+        query_pos, query = torch.split(
+            object_query_embed[:,None,...], self.embed_dims, dim=2
+        )
+
+        reference_points = reference_points[None,...]
+        reference_points = reference_points.sigmoid()
+
+        init_reference_out = reference_points
+        inter_states, inter_references = self.decoder(
+            query=query,
+            key=None,
+            value=bev_embed,
+            query_pos=query_pos,
+            reference_points=reference_points,
+            reg_branches=reg_branches,
+            cls_branches=cls_branches,
+            spatial_shapes=torch.tensor([[bev_h, bev_w]], device=query.device),
+            level_start_index=torch.tensor([0], device=query.device),
+        )
+        inter_references_out = inter_references
+
+        return inter_states, init_reference_out, inter_references_out
+    
\ No newline at end of file
-- 
2.39.3 (Apple Git-146)

