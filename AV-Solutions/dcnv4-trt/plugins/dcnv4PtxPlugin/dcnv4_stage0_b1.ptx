/*
 * SPDX-FileCopyrightText: Copyright (c) 2023-2024 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
 * SPDX-License-Identifier: Apache-2.0
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
 
//
// Generated by LLVM NVPTX Back-End
//

.version 7.1
.target sm_60
.address_size 64

// .globl	dcn_kernel
// __shared_memory__ has been demoted

.visible .entry dcn_kernel(
	.param .u64 dcn_kernel_param_1,
	.param .u64 dcn_kernel_param_12,
	.param .u64 dcn_kernel_param_21
)
{
	.reg .pred 	%p<8>;
	.reg .b16 	%rs<12>;
	.reg .b32 	%r<154>;
	.reg .f32 	%f<14>;
	.reg .b64 	%rd<130>;
	// demoted variable
	.shared .align 64 .b8 __shared_memory__[3584];
	ld.param.u64 	%rd39, [dcn_kernel_param_21];
	ld.param.u64 	%rd40, [dcn_kernel_param_1];
	cvta.to.global.u64 	%rd3, %rd40;
	mov.u32 	%r1, %ctaid.x;
	mov.u32 	%r53, %ctaid.y;
	cvt.s64.s32 	%rd4, %r53;
	shr.s32 	%r54, %r1, 31;
	xor.b32  	%r55, %r54, %r1;
	mul.hi.s32 	%r56, %r55, -1840700269;
	mad.lo.s32 	%r57, %r55, 1, %r56;
	shr.u32 	%r58, %r57, 31;
	shr.s32 	%r59, %r57, 4;
	add.s32 	%r60, %r59, %r58;
	xor.b32  	%r61, %r60, %r54;
	cvt.s64.s32 	%rd5, %r61;
	mov.u32 	%r62, %tid.x;
	cvt.s64.s32 	%rd6, %r62;
	shr.s32 	%r63, %r62, 31;
	xor.b32  	%r64, %r63, %r62;
	cvt.s64.s32 	%rd7, %r64;
	shr.u64 	%rd42, %rd7, 58;
	add.s64 	%rd43, %rd7, %rd42;
	shr.s64 	%rd44, %rd43, 6;
	cvt.s64.s32 	%rd8, %r63;
	xor.b64  	%rd9, %rd44, %rd8;
	mul.wide.s32 	%rd45, %r1, 2;
	add.s64 	%rd10, %rd9, %rd45;
	and.b64  	%rd46, %rd6, 14;
	setp.eq.s64 	%p1, %rd46, 14;
	shl.b64 	%rd120, %rd6, 3;
	shl.b64 	%rd121, %rd4, 3;
	mov.u64 	%rd122, __shared_memory__;
	mul.lo.s64 	%rd123, %rd9, 1792;
	@%p1 bra 	$L__BB0_2;
	ld.param.u64 	%rd41, [dcn_kernel_param_12];
	cvta.to.global.u64 	%rd2, %rd41;
	shr.u64 	%rd48, %rd7, 60;
	add.s64 	%rd49, %rd7, %rd48;
	shr.u64 	%rd50, %rd49, 4;
	xor.b64  	%rd51, %rd50, %rd8;
	shl.b64 	%rd52, %rd51, 7;
	sub.s64 	%rd53, %rd120, %rd52;
	bfe.u64 	%rd54, %rd6, 4, 2;
	or.b64  	%rd56, %rd54, %rd121;
	mul.lo.s64 	%rd57, %rd10, 12544;
	add.s64 	%rd58, %rd2, %rd57;
	mul.lo.s64 	%rd59, %rd56, 224;
	add.s64 	%rd60, %rd58, %rd59;
	shl.b64 	%rd61, %rd53, 1;
	add.s64 	%rd62, %rd60, %rd61;
	ld.global.v4.b32 	{%r65, %r66, %r67, %r68}, [%rd62];
	add.s64 	%rd65, %rd122, %rd123;
	mul.lo.s64 	%rd66, %rd54, 224;
	add.s64 	%rd67, %rd65, %rd66;
	add.s64 	%rd68, %rd67, %rd61;
	st.shared.v4.b32 	[%rd68], {%r65, %r66, %r67, %r68};
	ld.global.v4.b32 	{%r69, %r70, %r71, %r72}, [%rd62+896];
	st.shared.v4.b32 	[%rd68+896], {%r69, %r70, %r71, %r72};
$L__BB0_2:
	cvta.to.global.u64 	%rd1, %rd39;
	cvt.u32.u64 	%r77, %rd5;
	bar.sync 	0;
	shr.u64 	%rd70, %rd7, 61;
	add.s64 	%rd71, %rd7, %rd70;
	shr.s64 	%rd72, %rd71, 3;
	xor.b64  	%rd73, %rd72, %rd8;
	shl.b64 	%rd74, %rd73, 6;
	sub.s64 	%rd12, %rd120, %rd74;
	bfe.u64 	%rd76, %rd6, 3, 3;
	or.b64  	%rd13, %rd76, %rd121;
	shr.u64 	%rd77, %rd7, 63;
	add.s64 	%rd78, %rd7, %rd77;
	shr.s64 	%rd79, %rd78, 1;
	xor.b64  	%rd80, %rd79, %rd8;
	mul.lo.s64 	%rd83, %rd76, 224;
	add.s64 	%rd84, %rd123, %rd83;
	mul.lo.s64 	%rd85, %rd80, 54;
	add.s64 	%rd86, %rd84, %rd85;
	add.s64 	%rd87, %rd86, %rd122;
	mul.lo.s64 	%rd88, %rd73, -216;
	add.s64 	%rd89, %rd87, %rd88;
	add.s64 	%rd125, %rd89, 36;
	mul.lo.s64 	%rd90, %rd5, 401408;
	add.s64 	%rd15, %rd3, %rd90;
	mul.lo.s64 	%rd91, %rd73, 216;
	sub.s64 	%rd92, %rd86, %rd91;
	add.s64 	%rd93, %rd92, %rd122;
	add.s64 	%rd124, %rd93, 2;
	shl.b32 	%r78, %r1, 1;
	cvt.u32.u64 	%r79, %rd9;
	add.s32 	%r80, %r78, %r79;
	mul.lo.s32 	%r81, %r77, 56;
	not.b32 	%r82, %r81;
	add.s32 	%r83, %r82, %r80;
	cvt.u64.u32 	%rd17, %r83;
	mov.b32 	%r134, 0;
	mov.u64 	%rd69, 0;
	mov.u32 	%r135, %r134;
	mov.u32 	%r136, %r134;
	mov.u32 	%r137, %r134;
	mov.u64 	%rd126, %rd69;
	bra.uni 	$L__BB0_3;
$L__BB0_13:
	add.s64 	%rd126, %rd126, 1;
	add.s64 	%rd125, %rd125, 6;
	add.s64 	%rd124, %rd124, 12;
	setp.ne.s64 	%p7, %rd126, 3;
	@%p7 bra 	$L__BB0_3;
	bra.uni 	$L__BB0_14;
$L__BB0_3:
	add.s64 	%rd95, %rd13, %rd126;
	cvt.u32.u64 	%r84, %rd95;
	add.s32 	%r85, %r84, -1;
	cvt.rn.f32.s32 	%f1, %r85;
	mov.u64 	%rd127, %rd124;
	mov.u64 	%rd128, %rd125;
	mov.u64 	%rd129, %rd69;
	bra.uni 	$L__BB0_4;
$L__BB0_12:
	mov.f32 	%f11, 0f3F800000;
	sub.rn.f32 	%f12, %f11, %f2;
	sub.rn.f32 	%f13, %f11, %f3;
	cvt.rn.f16.f32 	%rs4, %f2;
	cvt.rn.f16.f32 	%rs5, %f3;
	cvt.rn.f16.f32 	%rs6, %f12;
	cvt.rn.f16.f32 	%rs7, %f13;
	mul.rn.f16 	%rs8, %rs7, %rs6;
	mov.b32 	%r110, {%rs8, %rs8};
	mul.rn.f16x2 	%r111, %r110, %r141;
	mul.rn.f16x2 	%r112, %r110, %r140;
	mul.rn.f16x2 	%r113, %r110, %r139;
	mul.rn.f16x2 	%r114, %r110, %r138;
	mul.rn.f16 	%rs9, %rs5, %rs6;
	mov.b32 	%r115, {%rs9, %rs9};
	fma.rn.f16x2 	%r116, %r115, %r142, %r114;
	fma.rn.f16x2 	%r117, %r115, %r143, %r113;
	fma.rn.f16x2 	%r118, %r115, %r144, %r112;
	fma.rn.f16x2 	%r119, %r115, %r145, %r111;
	mul.rn.f16 	%rs10, %rs4, %rs7;
	mov.b32 	%r120, {%rs10, %rs10};
	fma.rn.f16x2 	%r121, %r120, %r149, %r119;
	fma.rn.f16x2 	%r122, %r120, %r148, %r118;
	fma.rn.f16x2 	%r123, %r120, %r147, %r117;
	fma.rn.f16x2 	%r124, %r120, %r146, %r116;
	mul.rn.f16 	%rs11, %rs5, %rs4;
	mov.b32 	%r125, {%rs11, %rs11};
	fma.rn.f16x2 	%r126, %r125, %r150, %r124;
	fma.rn.f16x2 	%r127, %r125, %r151, %r123;
	fma.rn.f16x2 	%r128, %r125, %r152, %r122;
	fma.rn.f16x2 	%r129, %r125, %r153, %r121;
	fma.rn.f16x2 	%r137, %r10, %r129, %r137;
	fma.rn.f16x2 	%r136, %r10, %r128, %r136;
	fma.rn.f16x2 	%r135, %r10, %r127, %r135;
	fma.rn.f16x2 	%r134, %r10, %r126, %r134;
	add.s64 	%rd129, %rd129, 1;
	add.s64 	%rd128, %rd128, 2;
	add.s64 	%rd127, %rd127, 4;
	setp.ne.s64 	%p6, %rd129, 3;
	@%p6 bra 	$L__BB0_4;
	bra.uni 	$L__BB0_13;
$L__BB0_4:
	ld.shared.b16 	%rs1, [%rd127+-2];
	cvt.f32.f16 	%f4, %rs1;
	add.rn.f32 	%f5, %f1, %f4;
	add.s64 	%rd96, %rd17, %rd129;
	cvt.u32.u64 	%r90, %rd96;
	cvt.rn.f32.s32 	%f6, %r90;
	ld.shared.b16 	%rs2, [%rd127];
	cvt.f32.f16 	%f7, %rs2;
	add.rn.f32 	%f8, %f6, %f7;
	cvt.rmi.f32.f32 	%f9, %f8;
	cvt.rmi.f32.f32 	%f10, %f5;
	cvt.rzi.s32.f32 	%r14, %f9;
	cvt.rzi.s32.f32 	%r91, %f10;
	cvt.s64.s32 	%rd24, %r91;
	mul.wide.s32 	%rd25, %r91, 64;
	mul.wide.s32 	%rd97, %r14, 7168;
	add.s64 	%rd26, %rd15, %rd97;
	shl.b64 	%rd98, %rd25, 1;
	shl.b64 	%rd100, %rd12, 1;
	mov.b32 	%r150, 0;
	max.u32 	%r92, %r91, %r14;
	setp.gt.u32 	%p2, %r92, 55;
	mov.u32 	%r138, %r150;
	mov.u32 	%r139, %r150;
	mov.u32 	%r140, %r150;
	mov.u32 	%r141, %r150;
	@%p2 bra 	$L__BB0_6;
	add.s64 	%rd99, %rd26, %rd98;
	add.s64 	%rd27, %rd99, %rd100;
	ld.global.v4.b32 	{%r138, %r139, %r140, %r141}, [%rd27];
$L__BB0_6:
	cvt.u32.u64 	%r97, %rd24;
	shl.b64 	%rd101, %rd24, 6;
	add.s32 	%r23, %r97, 1;
	add.s64 	%rd28, %rd101, 64;
	shl.b64 	%rd102, %rd28, 1;
	max.u32 	%r98, %r14, %r23;
	setp.gt.u32 	%p3, %r98, 55;
	mov.u32 	%r142, %r150;
	mov.u32 	%r143, %r150;
	mov.u32 	%r144, %r150;
	mov.u32 	%r145, %r150;
	@%p3 bra 	$L__BB0_8;
	add.s64 	%rd103, %rd26, %rd102;
	add.s64 	%rd29, %rd103, %rd100;
	ld.global.v4.b32 	{%r142, %r143, %r144, %r145}, [%rd29];
$L__BB0_8:
	add.s32 	%r32, %r14, 1;
	add.s64 	%rd30, %rd26, 7168;
	max.u32 	%r104, %r97, %r32;
	setp.gt.u32 	%p4, %r104, 55;
	mov.u32 	%r146, %r150;
	mov.u32 	%r147, %r150;
	mov.u32 	%r148, %r150;
	mov.u32 	%r149, %r150;
	@%p4 bra 	$L__BB0_10;
	add.s64 	%rd106, %rd30, %rd98;
	add.s64 	%rd31, %rd106, %rd100;
	ld.global.v4.b32 	{%r146, %r147, %r148, %r149}, [%rd31];
$L__BB0_10:
	ld.shared.b16 	%rs3, [%rd128];
	mov.b32 	%r10, {%rs3, %rs3};
	sub.rn.f32 	%f2, %f8, %f9;
	sub.rn.f32 	%f3, %f5, %f10;
	max.u32 	%r109, %r23, %r32;
	setp.gt.u32 	%p5, %r109, 55;
	mov.u32 	%r151, %r150;
	mov.u32 	%r152, %r150;
	mov.u32 	%r153, %r150;
	@%p5 bra 	$L__BB0_12;
	add.s64 	%rd109, %rd30, %rd102;
	add.s64 	%rd32, %rd109, %rd100;
	ld.global.v4.b32 	{%r150, %r151, %r152, %r153}, [%rd32];
	bra.uni 	$L__BB0_12;
$L__BB0_14:
	and.b64  	%rd111, %rd120, 448;
	mul.lo.s64 	%rd112, %rd10, 7168;
	add.s64 	%rd113, %rd1, %rd112;
	shl.b64 	%rd114, %rd4, 10;
	shl.b64 	%rd115, %rd111, 1;
	or.b64  	%rd116, %rd114, %rd115;
	add.s64 	%rd117, %rd113, %rd116;
	add.s64 	%rd119, %rd117, %rd100;
	st.global.v4.b32 	[%rd119], {%r134, %r135, %r136, %r137};
	ret;

}