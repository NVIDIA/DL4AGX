/*
 * SPDX-FileCopyrightText: Copyright (c) 2023-2024 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
 * SPDX-License-Identifier: Apache-2.0
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
 
//
// Generated by LLVM NVPTX Back-End
//

.version 7.1
.target sm_60
.address_size 64

// .globl	dcn_kernel
// __shared_memory__ has been demoted

.visible .entry dcn_kernel(
	.param .u64 dcn_kernel_param_1,
	.param .u64 dcn_kernel_param_12,
	.param .u64 dcn_kernel_param_21
)
{
	.reg .pred 	%p<8>;
	.reg .b16 	%rs<12>;
	.reg .b32 	%r<149>;
	.reg .f32 	%f<14>;
	.reg .b64 	%rd<104>;
	// demoted variable
	.shared .align 64 .b8 __shared_memory__[1728];
	ld.param.u64 	%rd37, [dcn_kernel_param_21];
	ld.param.u64 	%rd38, [dcn_kernel_param_1];
	cvta.to.global.u64 	%rd3, %rd38;
	mov.u32 	%r1, %ctaid.x;
	cvt.s64.s32 	%rd40, %r1;
	mov.u32 	%r30, %ctaid.y;
	cvt.s64.s32 	%rd4, %r30;
	shr.s32 	%r31, %r1, 31;
	xor.b32  	%r32, %r31, %r1;
	mul.hi.s32 	%r33, %r32, -1840700269;
	mad.lo.s32 	%r34, %r32, 1, %r33;
	shr.u32 	%r35, %r34, 31;
	shr.s32 	%r36, %r34, 2;
	add.s32 	%r37, %r36, %r35;
	xor.b32  	%r38, %r37, %r31;
	cvt.s64.s32 	%rd5, %r38;
	mov.u32 	%r39, %tid.x;
	cvt.s64.s32 	%rd6, %r39;
	shr.s32 	%r40, %r39, 31;
	xor.b32  	%r41, %r40, %r39;
	cvt.s64.s32 	%rd7, %r41;
	shr.u64 	%rd41, %rd7, 57;
	add.s64 	%rd42, %rd7, %rd41;
	shr.s64 	%rd43, %rd42, 7;
	cvt.s64.s32 	%rd8, %r40;
	xor.b64  	%rd9, %rd43, %rd8;
	add.s64 	%rd10, %rd9, %rd40;
	and.b64  	%rd44, %rd6, 124;
	setp.gt.u64 	%p1, %rd44, 107;
	mul.lo.s64 	%rd96, %rd9, 1728;
	mov.u64 	%rd97, __shared_memory__;
	@%p1 bra 	$L__BB0_2;
	ld.param.u64 	%rd39, [dcn_kernel_param_12];
	cvta.to.global.u64 	%rd2, %rd39;
	shl.b64 	%rd45, %rd6, 3;
	shl.b64 	%rd46, %rd9, 10;
	sub.s64 	%rd47, %rd45, %rd46;
	mul.lo.s64 	%rd48, %rd10, 12096;
	add.s64 	%rd49, %rd2, %rd48;
	mul.lo.s64 	%rd50, %rd4, 1728;
	add.s64 	%rd51, %rd49, %rd50;
	shl.b64 	%rd52, %rd47, 1;
	add.s64 	%rd53, %rd51, %rd52;
	ld.global.u16 	%r42, [%rd53+12];
	ld.global.u16 	%r43, [%rd53+14];
	shl.b32 	%r44, %r43, 16;
	or.b32  	%r45, %r44, %r42;
	ld.global.u16 	%r47, [%rd53+8];
	ld.global.u16 	%r48, [%rd53+10];
	shl.b32 	%r49, %r48, 16;
	or.b32  	%r50, %r49, %r47;
	ld.global.u16 	%r52, [%rd53+4];
	ld.global.u16 	%r53, [%rd53+6];
	shl.b32 	%r54, %r53, 16;
	or.b32  	%r55, %r54, %r52;
	ld.global.u16 	%r57, [%rd53];
	ld.global.u16 	%r58, [%rd53+2];
	shl.b32 	%r59, %r58, 16;
	or.b32  	%r60, %r59, %r57;
	add.s64 	%rd56, %rd97, %rd96;
	add.s64 	%rd57, %rd56, %rd52;
	st.shared.v4.b32 	[%rd57], {%r60, %r55, %r50, %r45};
$L__BB0_2:
	cvta.to.global.u64 	%rd1, %rd37;
	cvt.u32.u64 	%r64, %rd5;
	cvt.u32.u64 	%r65, %rd4;
	bar.sync 	0;
	shl.b64 	%rd59, %rd6, 2;
	shl.b64 	%rd60, %rd9, 9;
	sub.s64 	%rd11, %rd59, %rd60;
	shr.u64 	%rd61, %rd7, 62;
	add.s64 	%rd62, %rd7, %rd61;
	shr.s64 	%rd63, %rd62, 2;
	xor.b64  	%rd64, %rd63, %rd8;
	add.s32 	%r2, %r65, -1;
	add.s64 	%rd67, %rd97, %rd96;
	mul.lo.s64 	%rd68, %rd64, 54;
	add.s64 	%rd69, %rd67, %rd68;
	mul.lo.s64 	%rd70, %rd9, -1728;
	add.s64 	%rd71, %rd69, %rd70;
	add.s64 	%rd99, %rd71, 36;
	mul.lo.s64 	%rd72, %rd5, 50176;
	add.s64 	%rd13, %rd3, %rd72;
	add.s64 	%rd73, %rd68, %rd97;
	add.s64 	%rd98, %rd73, 2;
	cvt.u32.u64 	%r66, %rd9;
	add.s32 	%r67, %r1, %r66;
	mul.lo.s32 	%r68, %r64, 7;
	not.b32 	%r69, %r68;
	add.s32 	%r70, %r69, %r67;
	cvt.u64.u32 	%rd15, %r70;
	mov.b32 	%r62, 0;
	mov.u64 	%rd58, 0;
	mov.u32 	%r139, %r62;
	mov.u32 	%r140, %r62;
	mov.u64 	%rd100, %rd58;
	bra.uni 	$L__BB0_3;
$L__BB0_13:
	add.s64 	%rd100, %rd100, 1;
	add.s64 	%rd99, %rd99, 6;
	add.s64 	%rd98, %rd98, 12;
	setp.ne.s64 	%p7, %rd100, 3;
	@%p7 bra 	$L__BB0_3;
	bra.uni 	$L__BB0_14;
$L__BB0_3:
	cvt.u32.u64 	%r71, %rd100;
	add.s32 	%r72, %r2, %r71;
	cvt.rn.f32.s32 	%f1, %r72;
	mov.u64 	%rd101, %rd98;
	mov.u64 	%rd102, %rd99;
	mov.u64 	%rd103, %rd58;
	bra.uni 	$L__BB0_4;
$L__BB0_12:
	sub.rn.f32 	%f9, %f3, %f4;
	sub.rn.f32 	%f10, %f2, %f5;
	mov.f32 	%f11, 0f3F800000;
	sub.rn.f32 	%f12, %f11, %f9;
	sub.rn.f32 	%f13, %f11, %f10;
	cvt.rn.f16.f32 	%rs4, %f9;
	cvt.rn.f16.f32 	%rs5, %f10;
	cvt.rn.f16.f32 	%rs6, %f12;
	cvt.rn.f16.f32 	%rs7, %f13;
	mul.rn.f16 	%rs8, %rs7, %rs6;
	mul.rn.f16 	%rs9, %rs5, %rs6;
	mul.rn.f16 	%rs10, %rs4, %rs7;
	mul.rn.f16 	%rs11, %rs5, %rs4;
	mov.b32 	%r121, {%rs8, %rs8};
	mov.b32 	%r122, {%rs9, %rs9};
	mov.b32 	%r123, {%rs10, %rs10};
	mov.b32 	%r124, {%rs11, %rs11};
	mul.rn.f16x2 	%r125, %r122, %r144;
	mul.rn.f16x2 	%r126, %r122, %r143;
	fma.rn.f16x2 	%r127, %r121, %r141, %r126;
	fma.rn.f16x2 	%r128, %r121, %r142, %r125;
	fma.rn.f16x2 	%r129, %r123, %r146, %r128;
	fma.rn.f16x2 	%r130, %r123, %r145, %r127;
	fma.rn.f16x2 	%r131, %r124, %r147, %r130;
	fma.rn.f16x2 	%r132, %r124, %r148, %r129;
	fma.rn.f16x2 	%r140, %r7, %r132, %r140;
	fma.rn.f16x2 	%r139, %r7, %r131, %r139;
	add.s64 	%rd103, %rd103, 1;
	add.s64 	%rd102, %rd102, 2;
	add.s64 	%rd101, %rd101, 4;
	setp.ne.s64 	%p6, %rd103, 3;
	@%p6 bra 	$L__BB0_4;
	bra.uni 	$L__BB0_13;
$L__BB0_4:
	ld.shared.b16 	%rs1, [%rd101+-2];
	cvt.f32.f16 	%f6, %rs1;
	add.rn.f32 	%f2, %f1, %f6;
	add.s64 	%rd75, %rd15, %rd103;
	cvt.u32.u64 	%r75, %rd75;
	cvt.rn.f32.s32 	%f7, %r75;
	ld.shared.b16 	%rs2, [%rd101];
	cvt.f32.f16 	%f8, %rs2;
	add.rn.f32 	%f3, %f7, %f8;
	ld.shared.b16 	%rs3, [%rd102];
	cvt.rmi.f32.f32 	%f4, %f3;
	cvt.rmi.f32.f32 	%f5, %f2;
	cvt.rzi.s32.f32 	%r9, %f4;
	cvt.rzi.s32.f32 	%r76, %f5;
	mul.wide.s32 	%rd23, %r76, 512;
	mul.wide.s32 	%rd76, %r9, 7168;
	add.s64 	%rd24, %rd13, %rd76;
	shl.b64 	%rd77, %rd23, 1;
	shl.b64 	%rd79, %rd11, 1;
	max.u32 	%r77, %r76, %r9;
	setp.gt.u32 	%p2, %r77, 6;
	mov.u32 	%r141, %r62;
	mov.u32 	%r142, %r62;
	@%p2 bra 	$L__BB0_6;
	add.s64 	%rd78, %rd24, %rd77;
	add.s64 	%rd25, %rd78, %rd79;
	ld.global.u16 	%r78, [%rd25+4];
	ld.global.u16 	%r79, [%rd25+6];
	shl.b32 	%r80, %r79, 16;
	or.b32  	%r142, %r80, %r78;
	ld.global.u16 	%r82, [%rd25];
	ld.global.u16 	%r83, [%rd25+2];
	shl.b32 	%r84, %r83, 16;
	or.b32  	%r141, %r84, %r82;
$L__BB0_6:
	cvt.s64.s32 	%rd22, %r76;
	cvt.u32.u64 	%r88, %rd22;
	shl.b64 	%rd80, %rd22, 9;
	add.s32 	%r14, %r88, 1;
	add.s64 	%rd26, %rd80, 512;
	shl.b64 	%rd81, %rd26, 1;
	max.u32 	%r89, %r9, %r14;
	setp.gt.u32 	%p3, %r89, 6;
	mov.u32 	%r143, %r62;
	mov.u32 	%r144, %r62;
	@%p3 bra 	$L__BB0_8;
	add.s64 	%rd82, %rd24, %rd81;
	add.s64 	%rd27, %rd82, %rd79;
	ld.global.u16 	%r90, [%rd27+4];
	ld.global.u16 	%r91, [%rd27+6];
	shl.b32 	%r92, %r91, 16;
	or.b32  	%r144, %r92, %r90;
	ld.global.u16 	%r94, [%rd27];
	ld.global.u16 	%r95, [%rd27+2];
	shl.b32 	%r96, %r95, 16;
	or.b32  	%r143, %r96, %r94;
$L__BB0_8:
	add.s32 	%r19, %r9, 1;
	add.s64 	%rd28, %rd24, 7168;
	max.u32 	%r101, %r88, %r19;
	setp.gt.u32 	%p4, %r101, 6;
	mov.u32 	%r145, %r62;
	mov.u32 	%r146, %r62;
	@%p4 bra 	$L__BB0_10;
	add.s64 	%rd85, %rd28, %rd77;
	add.s64 	%rd29, %rd85, %rd79;
	ld.global.u16 	%r102, [%rd29+4];
	ld.global.u16 	%r103, [%rd29+6];
	shl.b32 	%r104, %r103, 16;
	or.b32  	%r146, %r104, %r102;
	ld.global.u16 	%r106, [%rd29];
	ld.global.u16 	%r107, [%rd29+2];
	shl.b32 	%r108, %r107, 16;
	or.b32  	%r145, %r108, %r106;
$L__BB0_10:
	mov.b32 	%r7, {%rs3, %rs3};
	max.u32 	%r112, %r14, %r19;
	setp.gt.u32 	%p5, %r112, 6;
	mov.u32 	%r147, %r62;
	mov.u32 	%r148, %r62;
	@%p5 bra 	$L__BB0_12;
	add.s64 	%rd88, %rd28, %rd81;
	add.s64 	%rd30, %rd88, %rd79;
	ld.global.u16 	%r113, [%rd30+4];
	ld.global.u16 	%r114, [%rd30+6];
	shl.b32 	%r115, %r114, 16;
	or.b32  	%r148, %r115, %r113;
	ld.global.u16 	%r117, [%rd30];
	ld.global.u16 	%r118, [%rd30+2];
	shl.b32 	%r119, %r118, 16;
	or.b32  	%r147, %r119, %r117;
	bra.uni 	$L__BB0_12;
$L__BB0_14:
	mul.lo.s64 	%rd90, %rd10, 7168;
	add.s64 	%rd91, %rd1, %rd90;
	shl.b64 	%rd92, %rd4, 10;
	add.s64 	%rd93, %rd91, %rd92;
	add.s64 	%rd95, %rd93, %rd79;
	shr.u32 	%r134, %r140, 16;
	st.global.u16 	[%rd95+6], %r134;
	st.global.u16 	[%rd95+4], %r140;
	shr.u32 	%r136, %r139, 16;
	st.global.u16 	[%rd95+2], %r136;
	st.global.u16 	[%rd95], %r139;
	ret;

}
         kernels_cuModule_0_ptx_data 