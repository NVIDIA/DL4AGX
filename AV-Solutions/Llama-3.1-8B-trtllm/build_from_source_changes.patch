# -----------------------------------------------------------------------------
# SPDX-FileCopyrightText: Copyright (c) 2023-2024 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
# SPDX-License-Identifier: Apache-2.0
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
# http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# -----------------------------------------------------------------------------
diff --git a/cpp/CMakeLists.txt b/cpp/CMakeLists.txt
index 32e89ae1..f58c0e90 100644
--- a/cpp/CMakeLists.txt
+++ b/cpp/CMakeLists.txt
@@ -40,7 +40,7 @@ option(FAST_MATH "Compiling in fast math mode" OFF)
 option(INDEX_RANGE_CHECK "Compiling with index range checks" OFF)
 
 # Always use static NVRTC for IP protection reasons.
-set(USE_SHARED_NVRTC OFF)
+set(USE_SHARED_NVRTC ON)
 
 if(NVTX_DISABLE)
   add_compile_definitions("NVTX_DISABLE")
@@ -96,22 +96,22 @@ else()
   message(STATUS "Importing nvrtc wrapper")
 endif()
 
-if(EXISTS
-   "${CMAKE_CURRENT_SOURCE_DIR}/tensorrt_llm/kernels/internal_cutlass_kernels/CMakeLists.txt"
-)
-  set(BUILD_INTERNAL_CUTLASS_KERNELS_DEFAULT ON)
-else()
-  set(BUILD_INTERNAL_CUTLASS_KERNELS_DEFAULT OFF)
-endif()
-option(BUILD_INTERNAL_CUTLASS_KERNELS
-       "Build internal cutlass kernels from source"
-       ${BUILD_INTERNAL_CUTLASS_KERNELS_DEFAULT})
+#if(EXISTS
+#   "${CMAKE_CURRENT_SOURCE_DIR}/tensorrt_llm/kernels/internal_cutlass_kernels/CMakeLists.txt"
+#)
+#  set(BUILD_INTERNAL_CUTLASS_KERNELS_DEFAULT ON)
+#else()
+#  set(BUILD_INTERNAL_CUTLASS_KERNELS_DEFAULT OFF)
+#endif()
+#option(BUILD_INTERNAL_CUTLASS_KERNELS
+#       "Build internal cutlass kernels from source"
+#       ${BUILD_INTERNAL_CUTLASS_KERNELS_DEFAULT})
 
-if(BUILD_INTERNAL_CUTLASS_KERNELS)
-  message(STATUS "Building internal cutlass kernels")
-else()
-  message(STATUS "Importing internal cutlass kernels")
-endif()
+#if(BUILD_INTERNAL_CUTLASS_KERNELS)
+#  message(STATUS "Building internal cutlass kernels")
+#else()
+#  message(STATUS "Importing internal cutlass kernels")
+#endif()
 
 if(BUILD_PYT)
   message(STATUS "Building PyTorch")
@@ -299,12 +299,13 @@ endif()
 set(CUBLAS_LIB CUDA::cublas)
 set(CUBLASLT_LIB CUDA::cublasLt)
 set(CUDA_DRV_LIB CUDA::cuda_driver)
-set(CUDA_NVML_LIB CUDA::nvml)
+#set(CUDA_NVML_LIB CUDA::nvml)
 set(CUDA_RT_LIB CUDA::cudart_static)
 set(CMAKE_CUDA_RUNTIME_LIBRARY Static)
 
 find_library(RT_LIB rt)
 
+set(ENABLE_MULTI_DEVICE 0)
 set_ifndef(ENABLE_MULTI_DEVICE 1)
 if(ENABLE_MULTI_DEVICE)
   # NCCL dependencies
@@ -348,7 +349,7 @@ if(${CUDAToolkit_VERSION} VERSION_GREATER_EQUAL "11")
 endif()
 
 if(${CUDAToolkit_VERSION} VERSION_GREATER_EQUAL "11.8")
-  add_definitions("-DENABLE_FP8")
+  #add_definitions("-DENABLE_FP8")
   message(
     STATUS
       "CUDAToolkit_VERSION ${CUDAToolkit_VERSION_MAJOR}.${CUDAToolkit_VERSION_MINOR} is greater or equal than 11.8, enable -DENABLE_FP8 flag"
diff --git a/cpp/include/tensorrt_llm/batch_manager/inferenceRequest.h b/cpp/include/tensorrt_llm/batch_manager/inferenceRequest.h
index 41637854..66bfd357 100644
--- a/cpp/include/tensorrt_llm/batch_manager/inferenceRequest.h
+++ b/cpp/include/tensorrt_llm/batch_manager/inferenceRequest.h
@@ -41,7 +41,6 @@ auto constexpr kDraftInputIdsTensorName = "draft_input_ids";
 auto constexpr kDraftLogitsTensorName = "draft_logits";
 auto constexpr kMaxNewTokensTensorName = "request_output_len";
 auto constexpr kBeamWidthTensorName = "beam_width";
-auto constexpr kNumReturnSequencesTensorName = "num_return_sequences";
 auto constexpr kEndIdTensorName = "end_id";
 auto constexpr kPadIdTensorName = "pad_id";
 auto constexpr kBadWordsListTensorName = "bad_words_list";
@@ -195,7 +194,6 @@ public:
         inference_request::kDraftLogitsTensorName,
         inference_request::kMaxNewTokensTensorName,
         inference_request::kBeamWidthTensorName,
-        inference_request::kNumReturnSequencesTensorName,
         inference_request::kEndIdTensorName,
         inference_request::kPadIdTensorName,
         inference_request::kBadWordsListTensorName,
@@ -265,7 +263,6 @@ public:
     TENSOR_GETTER_SETTER(DraftLogits, inference_request::kDraftLogitsTensorName)
     TENSOR_GETTER_SETTER(MaxNewTokens, inference_request::kMaxNewTokensTensorName)
     TENSOR_GETTER_SETTER(BeamWidth, inference_request::kBeamWidthTensorName)
-    TENSOR_GETTER_SETTER(NumReturnSequences, inference_request::kNumReturnSequencesTensorName)
     TENSOR_GETTER_SETTER(EndId, inference_request::kEndIdTensorName)
     TENSOR_GETTER_SETTER(PadId, inference_request::kPadIdTensorName)
     TENSOR_GETTER_SETTER(BadWordsList, inference_request::kBadWordsListTensorName)
diff --git a/cpp/include/tensorrt_llm/batch_manager/kvCacheManager.h b/cpp/include/tensorrt_llm/batch_manager/kvCacheManager.h
index 38b49bd2..5f26170e 100644
--- a/cpp/include/tensorrt_llm/batch_manager/kvCacheManager.h
+++ b/cpp/include/tensorrt_llm/batch_manager/kvCacheManager.h
@@ -577,11 +577,11 @@ public:
     /// @return  The number of blocks
     [[nodiscard]] SizeType32 getNeededBlocksOneStep(LlmRequest const& req, bool twoStepsLookAhead) const;
 
-    /// @brief  Function that computes the number of KV cache blocks remaining to advance a request to completion (i.e.
-    /// for maxNewTokens); the allocated blocks are excluded
+    /// @brief  Function that computes the number of KV cache blocks needed to advance a request to completion (i.e. for
+    /// maxNewTokens)
     /// @param req The request for which we need to calculate the number of needed KV cache blocks
     /// @return  The number of blocks
-    [[nodiscard]] SizeType32 getRemainingBlocksToCompletion(LlmRequest const& req) const;
+    [[nodiscard]] SizeType32 getNeededBlocksToCompletion(LlmRequest const& req) const;
 
     void addContextTokens(SizeType32 seqSlotIdx, SizeType32 numTokens);
 
diff --git a/cpp/include/tensorrt_llm/batch_manager/llmRequest.h b/cpp/include/tensorrt_llm/batch_manager/llmRequest.h
index 0124592e..ae199bb3 100644
--- a/cpp/include/tensorrt_llm/batch_manager/llmRequest.h
+++ b/cpp/include/tensorrt_llm/batch_manager/llmRequest.h
@@ -85,7 +85,6 @@ public:
     using TensorPtr = TTensor;
     using LogitsPostProcessor = std::function<void(
         RequestIdType, TensorPtr&, BeamTokens const&, TStream const&, std::optional<RequestIdType>)>;
-    using RequestPtr = std::shared_ptr<GenericLlmRequest>;
 
     GenericLlmRequest(RequestIdType requestId, SizeType32 maxNewTokens, std::shared_ptr<VecTokens> inputTokens,
         runtime::SamplingConfig const& samplingConfig, bool isStreaming, std::optional<SizeType32> endId = std::nullopt,
@@ -108,8 +107,7 @@ public:
         std::optional<TensorPtr> encoderInputFeatures = std::nullopt,
         std::optional<SizeType32> encoderOutputLength = std::nullopt,
         LlmRequestType llmRequestType = LlmRequestType::LLMREQUEST_TYPE_CONTEXT_AND_GENERATION,
-        std::optional<std::shared_ptr<VecTokenExtraIds>> inputTokenExtraIds = std::nullopt,
-        SizeType32 numReturnSequences = 1)
+        std::optional<std::shared_ptr<VecTokenExtraIds>> inputTokenExtraIds = std::nullopt)
         : mRequestId(requestId)
         , mPromptLen(inputTokens->size())
         , mMaxNewTokens(maxNewTokens)
@@ -154,14 +152,11 @@ public:
         , mEncoderOutputLength(encoderOutputLength)
         , mLlmRequestType(llmRequestType)
         , mInputTokenExtraIds(std::move(inputTokenExtraIds))
-        , mNumReturnSequences(numReturnSequences)
-        , mSequenceIndex(0)
     {
         if (mEncoderTokens.has_value() || encoderInputFeatures.has_value())
         {
             mState = REQUEST_STATE_ENCODER_INIT;
         }
-
         initialize(*inputTokens, returnLogProbs);
     }
 
@@ -207,8 +202,6 @@ public:
         , mEncoderOutputLength(req.getEncoderOutputLength())
         , mContextPhaseParams(req.getContextPhaseParams())
         , mInputTokenExtraIds(std::nullopt)
-        , mNumReturnSequences(req.getNumReturnSequences())
-        , mSequenceIndex(0)
     {
         if (req.getRequestType() == executor::RequestType::REQUEST_TYPE_GENERATION_ONLY)
         {
@@ -224,7 +217,6 @@ public:
                 "length).");
             mReturnAllGeneratedTokens = true;
         }
-
         if (mIsStreaming && mSamplingConfig.beamWidth > 1 && mReturnGenerationLogits == true)
         {
             TLLM_LOG_WARNING(
@@ -284,15 +276,13 @@ public:
             mLoraTaskId = loraConfig->getTaskId();
             if (loraConfig.value().getWeights())
             {
-                mLoraWeights = tensorrt_llm::runtime::ITensor::view(
-                    executor::detail::toITensor(loraConfig.value().getWeights().value()));
+                mLoraWeights = executor::detail::toITensor(loraConfig.value().getWeights().value());
                 mLoraWeights.value()->unsqueeze(0);
             }
 
             if (loraConfig.value().getConfig())
             {
-                mLoraConfig = tensorrt_llm::runtime::ITensor::view(
-                    executor::detail::toITensor(loraConfig.value().getConfig().value()));
+                mLoraConfig = executor::detail::toITensor(loraConfig.value().getConfig().value());
                 mLoraConfig.value()->unsqueeze(0);
             }
         }
@@ -439,20 +429,6 @@ public:
         return mTokens.at(beam).size() - mNumPreDecodedTokens[beam];
     }
 
-    /// @brief Get number of return sequences for this req.
-    /// @return  The number of sequences to return.
-    [[nodiscard]] SizeType32 getNumReturnSequences() const
-    {
-        return mNumReturnSequences;
-    }
-
-    /// @brief Get child requests spawned by this req.
-    /// @return A vector of child requests.
-    [[nodiscard]] std::vector<RequestPtr> const& getChildRequests() const
-    {
-        return mChildRequests;
-    }
-
     /// @brief Get max number of tokens across all beams
     /// @return  The number of tokens
     [[nodiscard]] SizeType32 getMaxBeamNumTokens() const
@@ -642,25 +618,6 @@ public:
         }
     }
 
-    /// @brief Sets the number of return sequences.
-    /// @param numReturnSequences The number of return sequences.
-    void setNumReturnSequences(SizeType32 const& numReturnSequences)
-    {
-        TLLM_CHECK_WITH_INFO(!isChild(), "A child request cannot change numReturnSequences.");
-        TLLM_CHECK_WITH_INFO(
-            numReturnSequences > 0, "numReturnSequences should be a positive integer, got %d.", numReturnSequences);
-        TLLM_CHECK_WITH_INFO(mChildRequests.size() <= static_cast<size_t>(numReturnSequences),
-            "Cannot set numReturnSequences %d smaller than the number %ld of child requests that have already created.",
-            numReturnSequences, mChildRequests.size());
-        mNumReturnSequences = numReturnSequences;
-        mSequenceFinalVec->resize(mNumReturnSequences);
-    }
-
-    [[nodiscard]] bool constexpr isChild() const noexcept
-    {
-        return mSequenceIndex > 0;
-    }
-
     /// @brief Return a vector of the last-generated tokens of shape [num_beams]
     [[nodiscard]] VecTokens const& getLastTokens()
     {
@@ -929,11 +886,6 @@ public:
         mEncoderOutputHost = std::move(encoderOutputHost);
     }
 
-    void setEncoderOutput(TensorPtr encoderOutput)
-    {
-        mEncoderOutput = std::move(encoderOutput);
-    }
-
     void allocEncoderOutputHost(SizeType32 encoderHiddenSize, nvinfer1::DataType dataType)
     {
         mEncoderOutputHost = runtime::BufferManager::pinned(
@@ -1252,14 +1204,7 @@ public:
             TLLM_LOG_DEBUG("Creating response for request %lu", mRequestId);
 
             executor::Result result;
-            result.sequenceIndex = mSequenceIndex;
-
-            result.isSequenceFinal = isGenerationCompleteState() || isDisaggContextTransmissionState();
-            mSequenceFinalVec->at(mSequenceIndex) = result.isSequenceFinal;
-
-            result.isFinal = std::all_of(mSequenceFinalVec->begin(), mSequenceFinalVec->end(),
-                [](bool isSequenceFinal) { return isSequenceFinal; });
-
+            result.isFinal = isGenerationCompleteState() || isDisaggContextTransmissionState();
             auto const nbBeams = mSamplingConfig.beamWidth;
             auto const maxNbTokens = getMaxBeamNumTokens();
 
@@ -1350,9 +1295,7 @@ public:
                 // Update position of last sent response
                 setMaxSentTokenLen(maxNbTokens);
 
-                auto requestId = isChild() ? mParentRequestId : mRequestId;
-                auto response = executor::Response(requestId, std::move(result));
-
+                auto response = executor::Response(mRequestId, std::move(result));
                 return response;
             }
         }
@@ -1470,12 +1413,6 @@ protected:
     // TODO: add real extra id for encoder tokens
     std::optional<std::shared_ptr<VecUniqueTokens>> mEncoderUniqueTokens;
 
-    SizeType32 mNumReturnSequences;
-    SizeType32 mSequenceIndex;
-    std::vector<RequestPtr> mChildRequests;
-    RequestIdType mParentRequestId;
-    std::shared_ptr<std::vector<bool>> mSequenceFinalVec; // Indicators whether each sibling completes generation.
-
 private:
     void initialize(VecTokens const& inputTokens, bool outputLogProbs)
     {
@@ -1538,12 +1475,6 @@ private:
         }
 
         setReturnLogProbs(outputLogProbs);
-
-        if (!isChild())
-        {
-            // Initialize result states unless it is a child and a child request should share parent's one.
-            mSequenceFinalVec = std::make_shared<std::vector<bool>>(getNumReturnSequences(), false);
-        }
     }
 
     TensorPtr createListTensor(std::list<VecTokens> const& wordsList)
@@ -1609,8 +1540,7 @@ public:
         std::optional<TensorPtr> encoderInputFeatures = std::nullopt,
         std::optional<SizeType32> encoderOutputLength = std::nullopt,
         LlmRequestType llmRequestType = LlmRequestType::LLMREQUEST_TYPE_CONTEXT_AND_GENERATION,
-        std::optional<std::shared_ptr<VecTokenExtraIds>> inputTokenExtraIds = std::nullopt,
-        SizeType32 numReturnSequences = 1)
+        std::optional<std::shared_ptr<VecTokenExtraIds>> inputTokenExtraIds = std::nullopt)
         : Base(requestId, maxNewTokens, std::move(inputTokens), samplingConfig, isStreaming, endId, padId,
             std::move(embeddingBias), std::move(badWordsList), std::move(stopWordsList), std::move(positionIds),
             std::move(promptEmbeddingTable), promptVocabSize, loraTaskId, std::move(loraWeights), std::move(loraConfig),
@@ -1618,49 +1548,18 @@ public:
             std::move(draftTokens), std::move(draftLogits), excludeInputFromOutput, std::move(logitsPostProcessor),
             applyLogitsPostProcessorBatched, std::move(encoderInputTokens), returnEncoderOutput, clientId, priority,
             std::move(encoderInputFeatures), std::move(encoderOutputLength), llmRequestType,
-            std::move(inputTokenExtraIds), numReturnSequences)
+            std::move(inputTokenExtraIds))
     {
     }
 
-    LlmRequest(RequestIdType requestId, executor::Request const& request,
+    LlmRequest(RequestIdType requestId, executor::Request const& Request,
         std::optional<Base::LogitsPostProcessor> logitsPostProcessor = std::nullopt,
         bool applyLogitsPostProcessorBatched = false)
-        : Base(requestId, request)
+        : Base(requestId, Request)
     {
         mLogitsPostProcessor = std::move(logitsPostProcessor);
         mApplyLogitsPostProcessorBatched = applyLogitsPostProcessorBatched;
-        mLookaheadConfig = request.getLookaheadConfig();
-    }
-
-    std::shared_ptr<LlmRequest> createChildRequest(RequestIdType requestId)
-    {
-        TLLM_CHECK_WITH_INFO(!isChild(), "A child request cannot create its own child.");
-        TLLM_CHECK_WITH_INFO(mChildRequests.size() + 1 < static_cast<size_t>(getNumReturnSequences()),
-            "Cannot create child requests more than the number of return sequences (%d)", getNumReturnSequences());
-        auto childReq = std::make_shared<LlmRequest>(*this);
-        childReq->mRequestId = requestId;
-        childReq->mSequenceIndex = mChildRequests.size() + 1;
-        childReq->mParentRequestId = this->mRequestId;
-        childReq->mSequenceFinalVec = this->mSequenceFinalVec;
-        childReq->mSeqSlot.reset();
-
-        // To ensure different randomness across children, assign a unique random seed to each child
-        // by adding its sequence index to the base seed. If no seed is provided, the parent's seed defaults to 0.
-        using RandomSeedType = tensorrt_llm::executor::RandomSeedType;
-        if (childReq->mSamplingConfig.randomSeed.has_value())
-        {
-            childReq->mSamplingConfig.randomSeed->at(0) += static_cast<RandomSeedType>(childReq->mSequenceIndex);
-        }
-        else
-        {
-            RandomSeedType defaultSeed{0};
-            mSamplingConfig.randomSeed = std::vector<RandomSeedType>(1, defaultSeed);
-            childReq->mSamplingConfig.randomSeed
-                = std::vector<RandomSeedType>(1, defaultSeed + static_cast<RandomSeedType>(childReq->mSequenceIndex));
-        }
-
-        mChildRequests.push_back(childReq);
-        return childReq;
+        mLookaheadConfig = Request.getLookaheadConfig();
     }
 
     void movePromptEmbeddingTableToGpu(runtime::BufferManager const& manager)
diff --git a/cpp/include/tensorrt_llm/batch_manager/peftCacheManagerConfig.h b/cpp/include/tensorrt_llm/batch_manager/peftCacheManagerConfig.h
index c4c54997..acdd85d0 100644
--- a/cpp/include/tensorrt_llm/batch_manager/peftCacheManagerConfig.h
+++ b/cpp/include/tensorrt_llm/batch_manager/peftCacheManagerConfig.h
@@ -32,7 +32,7 @@ using runtime::SizeType32;
 struct PeftCacheManagerConfig
 {
 
-    static float constexpr kDefaultDeviceCachePercent = 0.02;
+    static float constexpr kDefaultDeviceCachePercent = 0.05;
     static size_t constexpr kDefaultHostCacheSize = 1024 * 1024 * 1024;
 
     explicit PeftCacheManagerConfig(SizeType32 numHostModuleLayer = 0, SizeType32 numDeviceModuleLayer = 0,
diff --git a/cpp/include/tensorrt_llm/common/cudaUtils.h b/cpp/include/tensorrt_llm/common/cudaUtils.h
index 71657c0b..92d9b8d9 100644
--- a/cpp/include/tensorrt_llm/common/cudaUtils.h
+++ b/cpp/include/tensorrt_llm/common/cudaUtils.h
@@ -281,7 +281,9 @@ inline int getSMVersion()
     int sm_minor = 0;
     check_cuda_error(cudaDeviceGetAttribute(&sm_major, cudaDevAttrComputeCapabilityMajor, device));
     check_cuda_error(cudaDeviceGetAttribute(&sm_minor, cudaDevAttrComputeCapabilityMinor, device));
-    return sm_major * 10 + sm_minor;
+    int ret = sm_major * 10 + sm_minor;
+    ret = ret == 87 ? 86 : ret;  // pretend to be 86 on orin?
+    return ret;
 }
 
 inline int getDevice()
diff --git a/cpp/include/tensorrt_llm/executor/executor.h b/cpp/include/tensorrt_llm/executor/executor.h
index a96c24d4..0267c1d6 100644
--- a/cpp/include/tensorrt_llm/executor/executor.h
+++ b/cpp/include/tensorrt_llm/executor/executor.h
@@ -24,7 +24,6 @@
 #include <deque>
 #include <filesystem>
 #include <list>
-#include <map>
 #include <memory>
 #include <optional>
 #include <string>
@@ -344,7 +343,6 @@ public:
     /// convolution down-sampling, etc.)
     /// @param type Indicate the request type for disaggregated serving mode.
     /// @param contextPhaseParams Generated token ID  from context only executor.
-    /// @param numReturnSequences The number of returning sequences.
     Request(VecTokens inputTokenIds, SizeType32 maxTokens, bool streaming = false,
         SamplingConfig const& samplingConfig = SamplingConfig(), OutputConfig const& outputConfig = OutputConfig(),
         std::optional<SizeType32> const& endId = std::nullopt, std::optional<SizeType32> const& padId = std::nullopt,
@@ -362,7 +360,7 @@ public:
         RequestType type = RequestType::REQUEST_TYPE_CONTEXT_AND_GENERATION,
         std::optional<ContextPhaseParams> contextPhaseParams = std::nullopt,
         std::optional<Tensor> encoderInputFeatures = std::nullopt,
-        std::optional<SizeType32> encoderOutputLength = std::nullopt, SizeType32 numReturnSequences = 1);
+        std::optional<SizeType32> encoderOutputLength = std::nullopt);
 
     /// @brief This logits postprocessor name will dispatch to the batched logits postprocessor
     static auto constexpr kBatchedPostProcessorName = "batched";
@@ -398,7 +396,6 @@ public:
     [[nodiscard]] std::optional<Tensor> getEncoderInputFeatures() const;
     [[nodiscard]] std::optional<SizeType32> getEncoderOutputLength() const;
     [[nodiscard]] RequestType getRequestType() const;
-    [[nodiscard]] SizeType32 getNumReturnSequences() const;
 
     void setStreaming(bool streaming);
     void setSamplingConfig(SamplingConfig const& config);
@@ -422,7 +419,6 @@ public:
     void setContextPhaseParams(ContextPhaseParams contextPhaseParams);
     void setEncoderInputFeatures(Tensor encoderInputFeatures);
     void setEncoderOutputLength(SizeType32 encoderOutputLength);
-    void setNumReturnSequences(SizeType32 numReturnSequences);
 
 private:
     friend class Serialization;
@@ -465,12 +461,6 @@ struct Result
 
     /// @brief The decoding iterations it takes.
     SizeType32 decodingIter{0};
-
-    /// @brief The index of the output sequence where 0 <= sequenceIndex < numReturnSequences
-    SizeType32 sequenceIndex{0};
-
-    /// @brief Indicates if this is the final result for a given sequence in the request
-    bool isSequenceFinal;
 };
 
 /// @brief Class that holds either an error or a result
@@ -593,7 +583,7 @@ private:
 class ExtendedRuntimePerfKnobConfig
 {
 public:
-    explicit ExtendedRuntimePerfKnobConfig(bool multiBlockMode = true, bool enableContextFMHAFP32Acc = false);
+    explicit ExtendedRuntimePerfKnobConfig(bool multiBlockMode = false, bool enableContextFMHAFP32Acc = false);
 
     bool operator==(ExtendedRuntimePerfKnobConfig const& other) const
     {
@@ -970,8 +960,7 @@ public:
         ModelType modelType, ExecutorConfig const& executorConfig);
 
     Executor(BufferView const& engineBuffer, std::string const& jsonConfigStr, ModelType modelType,
-        ExecutorConfig const& executorConfig,
-        std::optional<std::map<std::string, Tensor>> const& managedWeights = std::nullopt);
+        ExecutorConfig const& executorConfig);
 
     Executor(BufferView const& encoderEngineBuffer, std::string const& encoderJsonConfigStr,
         BufferView const& decoderEngineBuffer, std::string const& decoderJsonConfigStr, ModelType modelType,
diff --git a/cpp/include/tensorrt_llm/executor/serialization.h b/cpp/include/tensorrt_llm/executor/serialization.h
index 11d22c3f..ec227b85 100644
--- a/cpp/include/tensorrt_llm/executor/serialization.h
+++ b/cpp/include/tensorrt_llm/executor/serialization.h
@@ -25,13 +25,6 @@
 namespace tensorrt_llm::executor
 {
 
-namespace kv_cache
-{
-class CommState;
-class CacheState;
-class SocketState;
-} // namespace kv_cache
-
 class Serialization
 {
 public:
@@ -60,21 +53,6 @@ public:
     static void serialize(LoraConfig const& config, std::ostream& os);
     [[nodiscard]] static size_t serializedSize(LoraConfig const& config);
 
-    // CommState
-    [[nodiscard]] static kv_cache::CommState deserializeCommState(std::istream& is);
-    static void serialize(kv_cache::CommState const& state, std::ostream& os);
-    [[nodiscard]] static size_t serializedSize(kv_cache::CommState const& state);
-
-    // SocketState
-    [[nodiscard]] static kv_cache::SocketState deserializeSocketState(std::istream& is);
-    static void serialize(kv_cache::SocketState const& state, std::ostream& os);
-    [[nodiscard]] static size_t serializedSize(kv_cache::SocketState const& state);
-
-    // CacheState
-    [[nodiscard]] static kv_cache::CacheState deserializeCacheState(std::istream& is);
-    static void serialize(kv_cache::CacheState const& state, std::ostream& os);
-    [[nodiscard]] static size_t serializedSize(kv_cache::CacheState const& state);
-
     // ContextPhaseState
     [[nodiscard]] static ContextPhaseState deserializeContextPhaseState(std::istream& is);
     static void serialize(ContextPhaseState const& contextPhaseState, std::ostream& os);
diff --git a/cpp/tensorrt_llm/CMakeLists.txt b/cpp/tensorrt_llm/CMakeLists.txt
index 10debf56..df5691b6 100644
--- a/cpp/tensorrt_llm/CMakeLists.txt
+++ b/cpp/tensorrt_llm/CMakeLists.txt
@@ -37,7 +37,7 @@ add_subdirectory(common)
 add_subdirectory(kernels)
 add_subdirectory(layers)
 add_subdirectory(runtime)
-add_subdirectory(executor_worker)
+#add_subdirectory(executor_worker)
 
 set(TARGET_ARCH "unknown")
 
@@ -61,10 +61,10 @@ if(NOT WIN32) # Linux
     set(TARGET_ARCH "x86_64-linux-gnu")
   elseif(CMAKE_SYSTEM_PROCESSOR MATCHES "aarch64")
     set(TARGET_ARCH "aarch64-linux-gnu")
-    if(NOT ${OS_ID} MATCHES "ubuntu" OR ${OS_VERSION_ID} VERSION_LESS 22.04)
+    if(NOT ${OS_ID} MATCHES "ubuntu" OR ${OS_VERSION_ID} VERSION_LESS 20.04)
       message(
         FATAL_ERROR
-          "The minimum system requirement for aarch64 is Ubuntu 22.04.")
+          "The minimum system requirement for aarch64 is Ubuntu 20.04.")
     endif()
   else()
     message(
@@ -151,36 +151,36 @@ endif()
 set(INTERNAL_CUTLASS_KERNELS_TARGET
     tensorrt_llm_internal_cutlass_kernels_static)
 set(INTERNAL_CUTLASS_KERNELS_TARGET_ARCH ${TARGET_ARCH})
-if(BUILD_INTERNAL_CUTLASS_KERNELS)
-  add_subdirectory(kernels/internal_cutlass_kernels)
-else()
-  add_library(${INTERNAL_CUTLASS_KERNELS_TARGET} STATIC IMPORTED)
-  if(NOT WIN32) # Linux
-    if(USE_CXX11_ABI)
-      set(INTERNAL_CUTLASS_KERNELS_LIB_LOC
-          "${CMAKE_CURRENT_SOURCE_DIR}/kernels/internal_cutlass_kernels/${INTERNAL_CUTLASS_KERNELS_TARGET_ARCH}/libtensorrt_llm_internal_cutlass_kernels_static.a"
-      )
-    else()
-      set(INTERNAL_CUTLASS_KERNELS_LIB_LOC
-          "${CMAKE_CURRENT_SOURCE_DIR}/kernels/internal_cutlass_kernels/${INTERNAL_CUTLASS_KERNELS_TARGET_ARCH}/libtensorrt_llm_internal_cutlass_kernels_static.pre_cxx11.a"
-      )
-    endif()
-  else() # Windows
-    set(INTERNAL_CUTLASS_KERNELS_LIB_LOC
-        "${CMAKE_CURRENT_SOURCE_DIR}/kernels/internal_cutlass_kernels/${INTERNAL_CUTLASS_KERNELS_TARGET_ARCH}/tensorrt_llm_internal_cutlass_kernels_static.lib"
-    )
-  endif()
-  set_property(TARGET ${INTERNAL_CUTLASS_KERNELS_TARGET}
-               PROPERTY IMPORTED_LOCATION ${INTERNAL_CUTLASS_KERNELS_LIB_LOC})
-  file(SIZE ${INTERNAL_CUTLASS_KERNELS_LIB_LOC}
-       INTERNAL_CUTLASS_KERNELS_LIB_SIZE)
-  if(INTERNAL_CUTLASS_KERNELS_LIB_SIZE LESS 1024)
-    message(
-      FATAL_ERROR
-        "The internal_cutlass_kernels library is truncated or incomplete. This is usually caused by using Git LFS (Large File Storage) incorrectly. Please try running command `git lfs install && git lfs pull`."
-    )
-  endif()
-endif()
+#if(BUILD_INTERNAL_CUTLASS_KERNELS)
+#  add_subdirectory(kernels/internal_cutlass_kernels)
+#else()
+#  add_library(${INTERNAL_CUTLASS_KERNELS_TARGET} STATIC IMPORTED)
+#  if(NOT WIN32) # Linux
+#    if(USE_CXX11_ABI)
+#      set(INTERNAL_CUTLASS_KERNELS_LIB_LOC
+#          "${CMAKE_CURRENT_SOURCE_DIR}/kernels/internal_cutlass_kernels/${INTERNAL_CUTLASS_KERNELS_TARGET_ARCH}/libtensorrt_llm_internal_cutlass_kernels_static.a"
+#      )
+#    else()
+#      set(INTERNAL_CUTLASS_KERNELS_LIB_LOC
+#          "${CMAKE_CURRENT_SOURCE_DIR}/kernels/internal_cutlass_kernels/${INTERNAL_CUTLASS_KERNELS_TARGET_ARCH}/libtensorrt_llm_internal_cutlass_kernels_static.pre_cxx11.a"
+#      )
+#    endif()
+#  else() # Windows
+#    set(INTERNAL_CUTLASS_KERNELS_LIB_LOC
+#        "${CMAKE_CURRENT_SOURCE_DIR}/kernels/internal_cutlass_kernels/${INTERNAL_CUTLASS_KERNELS_TARGET_ARCH}/tensorrt_llm_internal_cutlass_kernels_static.lib"
+#    )
+#  endif()
+#  set_property(TARGET ${INTERNAL_CUTLASS_KERNELS_TARGET}
+#               PROPERTY IMPORTED_LOCATION ${INTERNAL_CUTLASS_KERNELS_LIB_LOC})
+#  file(SIZE ${INTERNAL_CUTLASS_KERNELS_LIB_LOC}
+#       INTERNAL_CUTLASS_KERNELS_LIB_SIZE)
+#  if(INTERNAL_CUTLASS_KERNELS_LIB_SIZE LESS 1024)
+#    message(
+#      FATAL_ERROR
+#        "The internal_cutlass_kernels library is truncated or incomplete. This is usually caused by using Git LFS (Large File Storage) incorrectly. Please try running command `git lfs install && git lfs pull`."
+#    )
+#  endif()
+#endif()
 
 find_package(Threads REQUIRED)
 target_link_libraries(${BATCH_MANAGER_TARGET} INTERFACE Threads::Threads)
@@ -216,29 +216,29 @@ else()
   add_custom_target(check_symbol)
 endif()
 
-if(NOT WIN32)
-  if(USE_CXX11_ABI)
-    add_custom_command(
-      OUTPUT
-        "${CMAKE_CURRENT_BINARY_DIR}/.check_symbol_internal_cutlass_kernels"
-      COMMAND nm -C $<TARGET_FILE:${INTERNAL_CUTLASS_KERNELS_TARGET}> | grep -q
-              'std::__cxx11::'
-      DEPENDS ${INTERNAL_CUTLASS_KERNELS_TARGET})
-  else()
-    add_custom_command(
-      OUTPUT
-        "${CMAKE_CURRENT_BINARY_DIR}/.check_symbol_internal_cutlass_kernels"
-      COMMAND nm -C $<TARGET_FILE:${INTERNAL_CUTLASS_KERNELS_TARGET}> | grep -qv
-              'std::__cxx11::'
-      DEPENDS ${INTERNAL_CUTLASS_KERNELS_TARGET})
-  endif()
-  add_custom_target(
-    check_symbol_internal_cutlass_kernels
-    DEPENDS "${CMAKE_CURRENT_BINARY_DIR}/.check_symbol_internal_cutlass_kernels"
-  )
-else()
-  add_custom_target(check_symbol_internal_cutlass_kernels)
-endif()
+#if(NOT WIN32)
+#  if(USE_CXX11_ABI)
+#    add_custom_command(
+#      OUTPUT
+#        "${CMAKE_CURRENT_BINARY_DIR}/.check_symbol_internal_cutlass_kernels"
+#      COMMAND nm -C $<TARGET_FILE:${INTERNAL_CUTLASS_KERNELS_TARGET}> | grep -q
+#              'std::__cxx11::'
+#      DEPENDS ${INTERNAL_CUTLASS_KERNELS_TARGET})
+#  else()
+#    add_custom_command(
+#      OUTPUT
+#        "${CMAKE_CURRENT_BINARY_DIR}/.check_symbol_internal_cutlass_kernels"
+#      COMMAND nm -C $<TARGET_FILE:${INTERNAL_CUTLASS_KERNELS_TARGET}> | grep -qv
+#              'std::__cxx11::'
+#      DEPENDS ${INTERNAL_CUTLASS_KERNELS_TARGET})
+#  endif()
+#  add_custom_target(
+#    check_symbol_internal_cutlass_kernels
+#    DEPENDS "${CMAKE_CURRENT_BINARY_DIR}/.check_symbol_internal_cutlass_kernels"
+#  )
+#else()
+#  add_custom_target(check_symbol_internal_cutlass_kernels)
+#endif()
 
 if(NOT WIN32)
   if(USE_CXX11_ABI)
@@ -327,10 +327,10 @@ set(TRTLLM_LINK_LIBS
     kernels_src
     context_attention_src
     decoder_attention_src
-    selective_scan_src
+    # selective_scan_src
     fpA_intB_gemm_src
     moe_gemm_src
-    fb_gemm_src
+    #fb_gemm_src
     gemm_swiglu_sm90_src
     cutlass_src
     layers_src
@@ -379,7 +379,7 @@ target_link_libraries(${SHARED_TARGET} PUBLIC ${TRTLLM_LINK_LIBS})
 
 link_whole_archive(${SHARED_TARGET} ${BATCH_MANAGER_TARGET})
 link_whole_archive(${SHARED_TARGET} ${EXECUTOR_TARGET})
-link_whole_archive(${SHARED_TARGET} ${INTERNAL_CUTLASS_KERNELS_TARGET})
+#link_whole_archive(${SHARED_TARGET} ${INTERNAL_CUTLASS_KERNELS_TARGET})
 if(ENABLE_UCX)
   link_whole_archive(${SHARED_TARGET} ucxx::ucxx)
 endif()
@@ -389,8 +389,8 @@ target_link_libraries(${BATCH_MANAGER_TARGET} INTERFACE ${SHARED_TARGET})
 # Cyclic dependency of executor on TRT-LLM
 target_link_libraries(${EXECUTOR_TARGET} INTERFACE ${SHARED_TARGET})
 # Cyclic dependency of internal_cutlass_kernels on TRT-LLM
-target_link_libraries(${INTERNAL_CUTLASS_KERNELS_TARGET}
-                      INTERFACE ${SHARED_TARGET})
+#target_link_libraries(${INTERNAL_CUTLASS_KERNELS_TARGET}
+#                      INTERFACE ${SHARED_TARGET})
 
 if(NOT WIN32)
   set_target_properties(${SHARED_TARGET} PROPERTIES LINK_FLAGS
@@ -401,7 +401,7 @@ target_link_libraries(${SHARED_TARGET} PUBLIC ${NVRTC_WRAPPER_TARGET})
 
 add_dependencies(${SHARED_TARGET} check_symbol)
 add_dependencies(${SHARED_TARGET} check_symbol_executor)
-add_dependencies(${SHARED_TARGET} check_symbol_internal_cutlass_kernels)
+#add_dependencies(${SHARED_TARGET} check_symbol_internal_cutlass_kernels)
 
 if(BUILD_PYT)
   add_subdirectory(thop)
diff --git a/cpp/tensorrt_llm/common/cublasMMWrapper.cpp b/cpp/tensorrt_llm/common/cublasMMWrapper.cpp
index 27f17923..83001a9b 100644
--- a/cpp/tensorrt_llm/common/cublasMMWrapper.cpp
+++ b/cpp/tensorrt_llm/common/cublasMMWrapper.cpp
@@ -118,7 +118,10 @@ void CublasMMWrapper::Gemm(cublasOperation_t transa, cublasOperation_t transb, i
 void CublasMMWrapper::Gemm(cublasOperation_t transa, cublasOperation_t transb, int const m, int const n, int const k,
     void const* A, int const lda, void const* B, int const ldb, void* C, int const ldc, float f_alpha, float f_beta)
 {
-    bool usingCublasLt = mAType == CUDA_R_16F || mAType == CUDA_R_8F_E4M3;
+    bool usingCublasLt = mAType == CUDA_R_16F;
+#ifdef ENABLE_FP8
++    usingCublasLt = usingCublasLt || mAType == CUDA_R_8F_E4M3;
+#endif
 
     Gemm(transa, transb, m, n, k, A, lda, B, ldb, C, ldc, f_alpha, f_beta, {}, /* hasAlgo */ false,
         /* usingCublasLt */ usingCublasLt);
@@ -132,7 +135,11 @@ void CublasMMWrapper::Gemm(cublasOperation_t transa, cublasOperation_t transb, i
     half h_beta = (half) (f_beta);
 
     // TODO: default cublas libs
-    usingCublasLt = usingCublasLt && (mAType == CUDA_R_16F || mAType == CUDA_R_8F_E4M3);
+#ifdef ENABLE_FP8
+     usingCublasLt = usingCublasLt && (mAType == CUDA_R_16F || mAType == CUDA_R_8F_E4M3);
+#else
+    usingCublasLt = usingCublasLt && mAType == CUDA_R_16F;    
+#endif
     bool isFp16ComputeType = mComputeType == CUBLAS_COMPUTE_16F;
     int batch_count = 1;
     // fp32 use cublas as default
diff --git a/cpp/tensorrt_llm/common/cudaDriverWrapper.cpp b/cpp/tensorrt_llm/common/cudaDriverWrapper.cpp
index abe8d0a4..5adb7d34 100644
--- a/cpp/tensorrt_llm/common/cudaDriverWrapper.cpp
+++ b/cpp/tensorrt_llm/common/cudaDriverWrapper.cpp
@@ -84,7 +84,9 @@ CUDADriverWrapper::CUDADriverWrapper()
     *(void**) (&_cuLinkAddData) = load_sym(handle, "cuLinkAddData_v2");
     *(void**) (&_cuLaunchCooperativeKernel) = load_sym(handle, "cuLaunchCooperativeKernel");
     *(void**) (&_cuLaunchKernel) = load_sym(handle, "cuLaunchKernel");
+#if (__CUDACC_VER_MAJOR__ >= 12)
     *(void**) (&_cuTensorMapEncodeTiled) = load_sym(handle, "cuTensorMapEncodeTiled");
+#endif
     *(void**) (&_cuMemcpyDtoH) = load_sym(handle, "cuMemcpyDtoH_v2");
 }
 
@@ -166,7 +168,7 @@ CUresult CUDADriverWrapper::cuLaunchKernel(CUfunction f, unsigned int gridDimX,
     return (*_cuLaunchKernel)(
         f, gridDimX, gridDimY, gridDimZ, blockDimX, blockDimY, blockDimZ, sharedMemBytes, hStream, kernelParams, extra);
 }
-
+#if (__CUDACC_VER_MAJOR__ >= 12)
 CUresult CUDADriverWrapper::cuTensorMapEncodeTiled(CUtensorMap* tensorMap, CUtensorMapDataType tensorDataType,
     cuuint32_t tensorRank, void* globalAddress, cuuint64_t const* globalDim, cuuint64_t const* globalStrides,
     cuuint32_t const* boxDim, cuuint32_t const* elementStrides, CUtensorMapInterleave interleave,
@@ -175,6 +177,7 @@ CUresult CUDADriverWrapper::cuTensorMapEncodeTiled(CUtensorMap* tensorMap, CUten
     return (*_cuTensorMapEncodeTiled)(tensorMap, tensorDataType, tensorRank, globalAddress, globalDim, globalStrides,
         boxDim, elementStrides, interleave, swizzle, l2Promotion, oobFill);
 }
+#endif
 
 CUresult CUDADriverWrapper::cuMemcpyDtoH(void* dstHost, CUdeviceptr srcDevice, size_t ByteCount) const
 {
diff --git a/cpp/tensorrt_llm/common/cudaDriverWrapper.h b/cpp/tensorrt_llm/common/cudaDriverWrapper.h
index a29c3452..c93de4a5 100644
--- a/cpp/tensorrt_llm/common/cudaDriverWrapper.h
+++ b/cpp/tensorrt_llm/common/cudaDriverWrapper.h
@@ -74,12 +74,12 @@ public:
     CUresult cuLaunchKernel(CUfunction f, unsigned int gridDimX, unsigned int gridDimY, unsigned int gridDimZ,
         unsigned int blockDimX, unsigned int blockDimY, unsigned int blockDimZ, unsigned int sharedMemBytes,
         CUstream hStream, void** kernelParams, void** extra) const;
-
+#if (__CUDACC_VER_MAJOR__ >= 12)
     CUresult cuTensorMapEncodeTiled(CUtensorMap* tensorMap, CUtensorMapDataType tensorDataType, cuuint32_t tensorRank,
         void* globalAddress, cuuint64_t const* globalDim, cuuint64_t const* globalStrides, cuuint32_t const* boxDim,
         cuuint32_t const* elementStrides, CUtensorMapInterleave interleave, CUtensorMapSwizzle swizzle,
         CUtensorMapL2promotion l2Promotion, CUtensorMapFloatOOBfill oobFill) const;
-
+#endif
     CUresult cuMemcpyDtoH(void* dstHost, CUdeviceptr srcDevice, size_t ByteCount) const;
 
 private:
@@ -101,10 +101,12 @@ private:
     CUresult (*_cuLaunchKernel)(CUfunction f, unsigned int gridDimX, unsigned int gridDimY, unsigned int gridDimZ,
         unsigned int blockDimX, unsigned int blockDimY, unsigned int blockDimZ, unsigned int sharedMemBytes,
         CUstream hStream, void** kernelParams, void** extra);
+#if (__CUDACC_VER_MAJOR__ >= 12)
     CUresult (*_cuTensorMapEncodeTiled)(CUtensorMap* tensorMap, CUtensorMapDataType tensorDataType,
         cuuint32_t tensorRank, void* globalAddress, cuuint64_t const* globalDim, cuuint64_t const* globalStrides,
         cuuint32_t const* boxDim, cuuint32_t const* elementStrides, CUtensorMapInterleave interleave,
         CUtensorMapSwizzle swizzle, CUtensorMapL2promotion l2Promotion, CUtensorMapFloatOOBfill oobFill);
+#endif
     CUresult (*_cuMemcpyDtoH)(void* dstHost, CUdeviceptr srcDevice, size_t ByteCount);
 };
 
diff --git a/cpp/tensorrt_llm/cutlass_extensions/include/cutlass_extensions/gemm/kernel/fused_moe_kernel.cuh b/cpp/tensorrt_llm/cutlass_extensions/include/cutlass_extensions/gemm/kernel/fused_moe_kernel.cuh
index 1bd0a3f1..e5a21066 100644
--- a/cpp/tensorrt_llm/cutlass_extensions/include/cutlass_extensions/gemm/kernel/fused_moe_kernel.cuh
+++ b/cpp/tensorrt_llm/cutlass_extensions/include/cutlass_extensions/gemm/kernel/fused_moe_kernel.cuh
@@ -167,12 +167,21 @@ struct Fused_Moe_Kernel_sm80
     }
 };
 
+#if CUDA_VERSION >= 11070
 template <typename GemmType>
 __global__ void run_global(__grid_constant__ typename GemmType::Params const params)
 {
     GemmType gemm;
     gemm.run_device(params);
 }
+#else
+template <typename GemmType>
+__global__ void run_global(typename GemmType::Params const params)
+{
+    GemmType gemm;
+    gemm.run_device(params);
+}
+#endif
 
 /// Computes the maximum number of active blocks per multiprocessor
 template <typename GemmType>
diff --git a/cpp/tensorrt_llm/kernels/CMakeLists.txt b/cpp/tensorrt_llm/kernels/CMakeLists.txt
index 6d908153..4e2985f8 100644
--- a/cpp/tensorrt_llm/kernels/CMakeLists.txt
+++ b/cpp/tensorrt_llm/kernels/CMakeLists.txt
@@ -67,4 +67,4 @@ set_property(TARGET kernels_src PROPERTY CUDA_RESOLVE_DEVICE_SYMBOLS ON)
 add_subdirectory(cutlass_kernels)
 add_subdirectory(contextFusedMultiHeadAttention)
 add_subdirectory(decoderMaskedMultiheadAttention)
-add_subdirectory(selectiveScan)
+# add_subdirectory(selectiveScan)
diff --git a/cpp/tensorrt_llm/kernels/beamSearchKernels/beamSearchKernelsTemplate.h b/cpp/tensorrt_llm/kernels/beamSearchKernels/beamSearchKernelsTemplate.h
index 8b44a419..08d0c65a 100644
--- a/cpp/tensorrt_llm/kernels/beamSearchKernels/beamSearchKernelsTemplate.h
+++ b/cpp/tensorrt_llm/kernels/beamSearchKernels/beamSearchKernelsTemplate.h
@@ -16,7 +16,7 @@
 
 #ifndef CUDART_VERSION
 #error CUDART_VERSION Undefined!
-#elif (CUDART_VERSION >= 11050)
+#elif (CUDART_VERSION >= 11040)
 #include <cub/cub.cuh>
 #else
 #include "3rdparty/cub/cub.cuh"
diff --git a/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/fmhaRunner.cpp b/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/fmhaRunner.cpp
index 5857e927..bafd8663 100644
--- a/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/fmhaRunner.cpp
+++ b/cpp/tensorrt_llm/kernels/contextFusedMultiHeadAttention/fmhaRunner.cpp
@@ -80,8 +80,8 @@ static inline void set_alpha(uint32_t& alpha, float norm, Data_type dtype)
 FusedMHARunnerV2::FusedMHARunnerV2(MHARunnerFixedParams fixedParams)
     : mFixedParams(fixedParams)
 {
-    TLLM_CHECK_WITH_INFO((mSM == kSM_70 || mSM == kSM_80 || mSM == kSM_86 || mSM == kSM_89 || mSM == kSM_90),
-        "Unsupported architecture");
+    TLLM_CHECK_WITH_INFO((mSM == kSM_70 || mSM == kSM_80 || mSM == kSM_86 || mSM == kSM_87 || mSM == kSM_89 || mSM == kSM_90),
+         "Unsupported architecture");
     TLLM_CHECK_WITH_INFO((mFixedParams.dataType == DATA_TYPE_FP16 || mFixedParams.dataType == DATA_TYPE_BF16
                              || mFixedParams.dataType == DATA_TYPE_E4M3),
         "Unsupported data type");
diff --git a/cpp/tensorrt_llm/kernels/customAllReduceKernels.cu b/cpp/tensorrt_llm/kernels/customAllReduceKernels.cu
index 0f2a514b..f7a89122 100644
--- a/cpp/tensorrt_llm/kernels/customAllReduceKernels.cu
+++ b/cpp/tensorrt_llm/kernels/customAllReduceKernels.cu
@@ -339,6 +339,7 @@ void rms_norm_kernel_launcher(AllReduceParams params, cudaStream_t stream)
         smem_size = params.fusion_params.hidden_size * sizeof(T);
         if (tensorrt_llm::common::getEnvEnablePDL())
         {
+            #if __CUDACC_VER_MAJOR__ >= 12
             TLLM_LOG_DEBUG("Enable PDL in rms_norm_kernel");
             cudaLaunchConfig_t kernelConfig = {0};
             kernelConfig.gridDim = cta_num;
@@ -354,6 +355,9 @@ void rms_norm_kernel_launcher(AllReduceParams params, cudaStream_t stream)
 
             TLLM_CUDA_CHECK(
                 cudaLaunchKernelEx(&kernelConfig, rms_norm_kernel<T, Bias, Residual, Affine, true>, params));
+            #else
+                TLLM_THROW("unsupported");
+            #endif
         }
         else
         {
@@ -364,6 +368,7 @@ void rms_norm_kernel_launcher(AllReduceParams params, cudaStream_t stream)
     {
         if (tensorrt_llm::common::getEnvEnablePDL())
         {
+            #if __CUDACC_VER_MAJOR__ >= 12
             TLLM_LOG_DEBUG("Enable PDL in rms_norm_kernel");
             cudaLaunchConfig_t kernelConfig = {0};
             kernelConfig.gridDim = cta_num;
@@ -379,6 +384,9 @@ void rms_norm_kernel_launcher(AllReduceParams params, cudaStream_t stream)
 
             TLLM_CUDA_CHECK(
                 cudaLaunchKernelEx(&kernelConfig, rms_norm_kernel<T, Bias, Residual, Affine, false>, params));
+            #else
+                TLLM_THROW("unsupported");
+            #endif
         }
         else
         {
@@ -519,6 +527,7 @@ void one_shot_all_reduce_norm_kernel_launcher(AllReduceParams params, cudaStream
         smem_size = params.fusion_params.hidden_size * sizeof(T);
         if (tensorrt_llm::common::getEnvEnablePDL())
         {
+            #if __CUDACC_VER_MAJOR__ >= 12
             TLLM_LOG_DEBUG("Enable PDL in one_shot_all_reduce_norm_kernel");
 
             cudaLaunchConfig_t kernelConfig = {0};
@@ -535,6 +544,9 @@ void one_shot_all_reduce_norm_kernel_launcher(AllReduceParams params, cudaStream
 
             TLLM_CUDA_CHECK(cudaLaunchKernelEx(
                 &kernelConfig, one_shot_all_reduce_norm_kernel<T, RanksPerNode, Bias, Affine, true>, params));
+            #else
+                TLLM_THROW("unsupported");
+            #endif
         }
         else
         {
@@ -546,6 +558,7 @@ void one_shot_all_reduce_norm_kernel_launcher(AllReduceParams params, cudaStream
     {
         if (tensorrt_llm::common::getEnvEnablePDL())
         {
+            #if __CUDACC_VER_MAJOR__ >= 12
             cudaLaunchConfig_t kernelConfig = {0};
             kernelConfig.gridDim = cta_num;
             kernelConfig.blockDim = cta_size;
@@ -561,6 +574,9 @@ void one_shot_all_reduce_norm_kernel_launcher(AllReduceParams params, cudaStream
             TLLM_LOG_DEBUG("Enable PDL in one_shot_all_reduce_norm_kernel");
             TLLM_CUDA_CHECK(cudaLaunchKernelEx(
                 &kernelConfig, one_shot_all_reduce_norm_kernel<T, RanksPerNode, Bias, Affine, false>, params));
+            #else
+                TLLM_THROW("unsupported");
+            #endif
         }
         else
         {
@@ -982,6 +998,7 @@ void AllReduceNormKernelLaunch(AllReduceStrategyType algo, AllReduceStrategyConf
 
         if (tensorrt_llm::common::getEnvEnablePDL())
         {
+            #if __CUDACC_VER_MAJOR__ >= 12
             TLLM_LOG_DEBUG("Enable PDL in twoShotAllReduceKernel");
             cudaLaunchConfig_t kernelConfig = {0};
             kernelConfig.gridDim = blocks_per_grid;
@@ -997,6 +1014,9 @@ void AllReduceNormKernelLaunch(AllReduceStrategyType algo, AllReduceStrategyConf
 
             TLLM_CUDA_CHECK(cudaLaunchKernelEx(
                 &kernelConfig, twoShotAllReduceKernel<T, RANKS_PER_NODE, !USE_MEMCPY, PUSH_MODE, Bias, true>, params));
+            #else
+                TLLM_THROW("unsupported");
+            #endif
         }
         else
         {
diff --git a/cpp/tensorrt_llm/kernels/cutlass_kernels/CMakeLists.txt b/cpp/tensorrt_llm/kernels/cutlass_kernels/CMakeLists.txt
index a457b77a..12ff2d01 100644
--- a/cpp/tensorrt_llm/kernels/cutlass_kernels/CMakeLists.txt
+++ b/cpp/tensorrt_llm/kernels/cutlass_kernels/CMakeLists.txt
@@ -26,7 +26,7 @@ endif()
 
 execute_process(
   WORKING_DIRECTORY ${3RDPARTY_DIR}/cutlass/python/
-  COMMAND ${Python3_EXECUTABLE} setup_library.py develop --user
+  COMMAND ${Python3_EXECUTABLE} setup_library.py develop
   RESULT_VARIABLE _CUTLASS_LIBRARY_SUCCESS)
 
 if(NOT _CUTLASS_LIBRARY_SUCCESS MATCHES 0)
@@ -68,15 +68,15 @@ file(GLOB_RECURSE GROUPED_SRC_CPP moe_gemm/*.cpp)
 file(GLOB_RECURSE GROUPED_SRC_CU moe_gemm/*.cu)
 
 # Get the sources for FP8 Rowwise GEMM launchers
-file(GLOB_RECURSE FBGEMM_CU_INSTANTIATIONS
-     ${INSTANTIATION_GENERATION_DIR}/fp8_rowwise_gemm/*.cu)
-file(GLOB_RECURSE FBGEMM_SRC_CU fp8_rowwise_gemm/*.cu)
+#file(GLOB_RECURSE FBGEMM_CU_INSTANTIATIONS
+#     ${INSTANTIATION_GENERATION_DIR}/fp8_rowwise_gemm/*.cu)
+#file(GLOB_RECURSE FBGEMM_SRC_CU fp8_rowwise_gemm/*.cu)
 
 # Get the sources for all the remaining sources
 file(GLOB_RECURSE SRC_CPP *.cpp)
 file(GLOB_RECURSE SRC_CU *.cu)
 set(ALL_SRCS ${SRC_CPP};${SRC_CU})
-list(FILTER ALL_SRCS EXCLUDE REGEX "fpA_intB_gemm/.*")
+#list(FILTER ALL_SRCS EXCLUDE REGEX "fpA_intB_gemm/.*")
 list(FILTER ALL_SRCS EXCLUDE REGEX "moe_gemm/.*")
 list(FILTER ALL_SRCS EXCLUDE REGEX "fp8_rowwise_gemm/.*")
 list(REMOVE_ITEM ALL_SRCS
@@ -89,7 +89,7 @@ message(
   VERBOSE
   "Group srcs ${GROUPED_SRC_CU} ${GROUPED_SRC_CPP} ${GROUPED_CU_INSTANTIATIONS}"
 )
-message(VERBOSE "Fbgemm srcs ${FBGEMM_SRC_CU} ${FBGEMM_CU_INSTANTIATIONS}")
+#message(VERBOSE "Fbgemm srcs ${FBGEMM_SRC_CU} ${FBGEMM_CU_INSTANTIATIONS}")
 message(VERBOSE "All srcs ${ALL_SRCS}")
 
 add_library(cutlass_src STATIC ${ALL_SRCS})
@@ -101,13 +101,15 @@ add_library(fpA_intB_gemm_src STATIC ${MIXED_SRC_CPP} ${MIXED_SRC_CU}
 # WARNING: Building with `-G` flag may generate invalid results for this target
 add_library(moe_gemm_src STATIC ${GROUPED_SRC_CU} ${GROUPED_SRC_CPP}
                                 ${GROUPED_CU_INSTANTIATIONS})
-add_library(fb_gemm_src STATIC ${FBGEMM_SRC_CU} ${FBGEMM_CU_INSTANTIATIONS})
+#add_library(fb_gemm_src STATIC ${FBGEMM_SRC_CU} ${FBGEMM_CU_INSTANTIATIONS})
 
 set(GEMM_SWIGLU_SM90_SRC_CU
     ${CMAKE_CURRENT_SOURCE_DIR}/fused_gated_gemm/gemm_swiglu_e4m3.cu)
 add_library(gemm_swiglu_sm90_src STATIC ${GEMM_SWIGLU_SM90_SRC_CU})
+#foreach(target_name
+#        fpA_intB_gemm_src;moe_gemm_src;fb_gemm_src;gemm_swiglu_sm90_src)
 foreach(target_name
-        fpA_intB_gemm_src;moe_gemm_src;fb_gemm_src;gemm_swiglu_sm90_src)
+        fpA_intB_gemm_src;moe_gemm_src;gemm_swiglu_sm90_src)
   set_property(TARGET ${target_name} PROPERTY POSITION_INDEPENDENT_CODE ON)
   set_property(TARGET ${target_name} PROPERTY CUDA_RESOLVE_DEVICE_SYMBOLS ON)
 
diff --git a/cpp/tensorrt_llm/kernels/cutlass_kernels/cutlass_type_conversion.h b/cpp/tensorrt_llm/kernels/cutlass_kernels/cutlass_type_conversion.h
index 0ec8ab2e..501ff652 100644
--- a/cpp/tensorrt_llm/kernels/cutlass_kernels/cutlass_type_conversion.h
+++ b/cpp/tensorrt_llm/kernels/cutlass_kernels/cutlass_type_conversion.h
@@ -18,7 +18,9 @@
 
 #include <cuda_bf16.h>
 #include <cuda_fp16.h>
+#if defined(ENABLE_FP8)
 #include <cuda_fp8.h>
+#endif
 
 #include "cutlass/bfloat16.h"
 #include "cutlass/float8.h"
diff --git a/cpp/tensorrt_llm/kernels/cutlass_kernels/fp8_rowwise_gemm/fp8_rowwise_gemm.h b/cpp/tensorrt_llm/kernels/cutlass_kernels/fp8_rowwise_gemm/fp8_rowwise_gemm.h
deleted file mode 100644
index 7d0816e2..00000000
--- a/cpp/tensorrt_llm/kernels/cutlass_kernels/fp8_rowwise_gemm/fp8_rowwise_gemm.h
+++ /dev/null
@@ -1,88 +0,0 @@
-/*
- * Copyright (c) 2023, NVIDIA CORPORATION.  All rights reserved.
- *
- * Licensed under the Apache License, Version 2.0 (the "License");
- * you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-#pragma once
-
-#include "cutlass_extensions/gemm_configs.h"
-#include "tensorrt_llm/common/quantization.h"
-
-#include <cuda_runtime_api.h>
-#include <vector>
-
-namespace tk = tensorrt_llm::common;
-namespace tkc = tensorrt_llm::cutlass_extensions;
-
-namespace tensorrt_llm
-{
-namespace kernels
-{
-namespace cutlass_kernels
-{
-
-/*
-  This runner supports:
-
-  Activations and outputs are all assumed to be row-major.
-  Weights are assumed to be column-major.
-*/
-
-class CutlassFp8RowwiseGemmRunnerInterface
-{
-public:
-    CutlassFp8RowwiseGemmRunnerInterface() {}
-
-    virtual ~CutlassFp8RowwiseGemmRunnerInterface() {}
-
-    virtual void gemm(void* D, void const* A, void const* B, void const* C_bias, tk::QuantMode quantOption, int m,
-        int n, int k, float const* scale_d0, float const* scale_d1, tkc::CutlassGemmConfig gemmConfig, char* workspace,
-        size_t workspaceBytes, cudaStream_t stream, int* occupancy = nullptr)
-        = 0;
-
-    // Returns desired workspace size in bytes.
-    virtual size_t getWorkspaceSize(int const m, int const n, int const k) = 0;
-
-    virtual std::vector<tkc::CutlassGemmConfig> getConfigs() const = 0;
-};
-
-template <typename T>
-class CutlassFp8RowwiseGemmRunner : public virtual CutlassFp8RowwiseGemmRunnerInterface
-{
-public:
-    CutlassFp8RowwiseGemmRunner();
-    ~CutlassFp8RowwiseGemmRunner();
-
-    void gemm(void* D, void const* A, void const* B, void const* C_bias, tk::QuantMode quantOption, int m, int n, int k,
-        float const* scale_d0, float const* scale_d1, tkc::CutlassGemmConfig gemmConfig, char* workspace,
-        size_t workspaceBytes, cudaStream_t stream, int* occupancy = nullptr) override;
-
-    // Returns desired workspace size in bytes.
-    size_t getWorkspaceSize(int const m, int const n, int const k) override;
-
-    std::vector<tkc::CutlassGemmConfig> getConfigs() const override;
-
-private:
-    size_t dispatchToArch(void* D, void const* A, void const* B, void const* C_bias, tk::QuantMode quantOption, int m,
-        int n, int k, float const* scale_d0, float const* scale_d1, tkc::CutlassGemmConfig gemmConfig, char* workspace,
-        size_t workspaceBytes, cudaStream_t stream, int* occupancy = nullptr);
-
-    size_t getWorkspaceSizeImpl(int const m, int const n, int const k);
-
-    int mSm;
-};
-
-} // namespace cutlass_kernels
-} // namespace kernels
-} // namespace tensorrt_llm
diff --git a/cpp/tensorrt_llm/kernels/cutlass_kernels/fp8_rowwise_gemm/fp8_rowwise_gemm_bf16.cu b/cpp/tensorrt_llm/kernels/cutlass_kernels/fp8_rowwise_gemm/fp8_rowwise_gemm_bf16.cu
deleted file mode 100644
index a1fcb7a5..00000000
--- a/cpp/tensorrt_llm/kernels/cutlass_kernels/fp8_rowwise_gemm/fp8_rowwise_gemm_bf16.cu
+++ /dev/null
@@ -1,30 +0,0 @@
-/*
- * Copyright (c) 2020-2023, NVIDIA CORPORATION.  All rights reserved.
- *
- * Licensed under the Apache License, Version 2.0 (the "License");
- * you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-#include "fp8_rowwise_gemm_template.h"
-
-namespace tensorrt_llm
-{
-namespace kernels
-{
-namespace cutlass_kernels
-{
-#ifdef ENABLE_BF16
-template class CutlassFp8RowwiseGemmRunner<__nv_bfloat16>;
-#endif
-} // namespace cutlass_kernels
-} // namespace kernels
-} // namespace tensorrt_llm
diff --git a/cpp/tensorrt_llm/kernels/cutlass_kernels/fp8_rowwise_gemm/fp8_rowwise_gemm_fp16.cu b/cpp/tensorrt_llm/kernels/cutlass_kernels/fp8_rowwise_gemm/fp8_rowwise_gemm_fp16.cu
deleted file mode 100644
index 83582db6..00000000
--- a/cpp/tensorrt_llm/kernels/cutlass_kernels/fp8_rowwise_gemm/fp8_rowwise_gemm_fp16.cu
+++ /dev/null
@@ -1,30 +0,0 @@
-/*
- * Copyright (c) 2020-2023, NVIDIA CORPORATION.  All rights reserved.
- *
- * Licensed under the Apache License, Version 2.0 (the "License");
- * you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-#include "fp8_rowwise_gemm_template.h"
-
-namespace tensorrt_llm
-{
-namespace kernels
-{
-namespace cutlass_kernels
-{
-
-template class CutlassFp8RowwiseGemmRunner<half>;
-
-} // namespace cutlass_kernels
-} // namespace kernels
-} // namespace tensorrt_llm
diff --git a/cpp/tensorrt_llm/kernels/cutlass_kernels/fp8_rowwise_gemm/fp8_rowwise_gemm_kernel_template_sm90.h b/cpp/tensorrt_llm/kernels/cutlass_kernels/fp8_rowwise_gemm/fp8_rowwise_gemm_kernel_template_sm90.h
deleted file mode 100644
index 8be00b59..00000000
--- a/cpp/tensorrt_llm/kernels/cutlass_kernels/fp8_rowwise_gemm/fp8_rowwise_gemm_kernel_template_sm90.h
+++ /dev/null
@@ -1,174 +0,0 @@
-/*
- * Copyright (c) 2022-2024, Meta Platforms, Inc. and affiliates.
- * All rights reserved.
- *
- * This source code is licensed under the BSD-style license found in the
- * LICENSE file in the root directory of this source tree.
- *
- * Copyright (c) 2023-2024, NVIDIA CORPORATION.  All rights reserved.
- *
- * Licensed under the Apache License, Version 2.0 (the "License");
- * you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-#pragma once
-
-#ifndef _WIN32
-#pragma GCC diagnostic push
-#pragma GCC diagnostic ignored "-Wstrict-aliasing"
-#endif // #ifndef _WIN32
-
-#include "cute/tensor.hpp"
-#include "cutlass/conv/convolution.h"
-// Order matters here, packed_stride.hpp is missing cute and convolution includes
-#include "cutlass/util/packed_stride.hpp"
-
-#include "cutlass/epilogue/collective/default_epilogue.hpp"
-#include "cutlass/epilogue/thread/linear_combination.h"
-#include "cutlass/gemm/collective/collective_builder.hpp"
-#include "cutlass/gemm/dispatch_policy.hpp"
-
-#include "cutlass/epilogue/thread/activation.h"
-#include "cutlass/gemm/kernel/gemm_universal.hpp"
-
-#include "cutlass/epilogue/collective/collective_builder.hpp"
-#include "cutlass/gemm/device/gemm_universal_adapter.h"
-
-#ifndef _WIN32
-#pragma GCC diagnostic pop
-#endif // #ifndef _WIN32
-
-using namespace cute;
-
-namespace tensorrt_llm
-{
-namespace kernels
-{
-namespace cutlass_kernels
-{
-
-template <typename ElementType, typename OutElementType, typename AccumElementType, typename CTAShape,
-    typename ClusterShape, typename MainloopScheduleType, typename EpilogueScheduleType,
-    typename TileSchedulerType = void>
-struct DeviceGemmFp8RowwiseSm90
-{
-    static_assert(std::is_same_v<ElementType, cutlass::float_e4m3_t>, "ElementType must be FP8(e4m3)");
-
-    // A matrix configuration
-    using ElementA = ElementType;                      // Element type for A matrix operand
-    using LayoutA = cutlass::layout::RowMajor;         // Layout type for A matrix operand
-    static constexpr int AlignmentA
-        = 128 / cutlass::sizeof_bits<ElementA>::value; // Memory access granularity/alignment of A
-                                                       // matrix in units of elements (up to 16 bytes)
-
-    // B matrix configuration
-    using ElementB = ElementType;                      // Element type for B matrix operand
-    using LayoutB = cutlass::layout::ColumnMajor;      // Layout type for B matrix operand
-    static constexpr int AlignmentB
-        = 128 / cutlass::sizeof_bits<ElementB>::value; // Memory access granularity/alignment of B
-                                                       // matrix in units of elements (up to 16 bytes)
-
-    // C/D matrix configuration
-    using ElementC = void;                                   // Element type for C matrix operands
-    using LayoutC = cutlass::layout::RowMajor;               // Layout type for C matrix operands
-    static constexpr int AlignmentC
-        = 128 / cutlass::sizeof_bits<OutElementType>::value; // Memory access granularity/alignment of C matrices in
-                                                             // units of elements (up to 16 bytes)
-
-    // Output matrix configuration
-    using ElementOutput = OutElementType;           // Element type for output matrix operands
-    using LayoutOutput = cutlass::layout::RowMajor; // Layout type for output matrix operands
-    static constexpr int AlignmentOutput = 128 / cutlass::sizeof_bits<ElementOutput>::value;
-
-    // Auxiliary matrix configuration and other fusion types
-    using ElementBias = float;
-
-    // Multiply-accumulate blocking/pipelining details
-    using ElementAccumulator = AccumElementType; // Element type for internal accumulation
-    using ElementCompute = float;                // Element type for compute
-    using ElementComputeEpilogue = float;
-    using ArchTag = cutlass::arch::Sm90;         // Tag indicating the minimum SM that supports the intended feature
-    using OperatorClass = cutlass::arch::OpClassTensorOp; // Operator class tag
-    using TileShape = CTAShape;                           // Threadblock-level tile size
-    using TileScheduler = TileSchedulerType;
-
-    static constexpr bool PONG = false;
-    static constexpr bool FAST_ACCUM = true;
-    static constexpr bool USE_BIAS = false;
-
-    using StageCountType = cutlass::gemm::collective::StageCountAuto;     // Stage count maximized
-                                                                          // based on the tile size
-    using KernelSchedule = cutlass::gemm::collective::KernelScheduleAuto; // Kernel to launch based on the default
-                                                                          // setting in the Collective Builder
-    // Implement rowwise scaling epilogue.
-    using XScale = cutlass::epilogue::fusion::Sm90ColBroadcast<0, TileShape, ElementComputeEpilogue,
-        cute::Stride<cute::Int<1>, cute::Int<0>, cute::Int<0>>>;
-
-    using WScale = cutlass::epilogue::fusion::Sm90RowBroadcast<0, TileShape, ElementComputeEpilogue,
-        cute::Stride<cute::Int<0>, cute::Int<1>, cute::Int<0>>>;
-
-    using Bias = cutlass::epilogue::fusion::Sm90RowBroadcast<0, TileShape, ElementBias,
-        cute::Stride<cute::Int<0>, cute::Int<1>, cute::Int<0>>>;
-
-    using Accum = cutlass::epilogue::fusion::Sm90AccFetch;
-
-    using Compute0 = cutlass::epilogue::fusion::Sm90Compute<cutlass::multiplies,
-        ElementComputeEpilogue, // First stage output type.
-        ElementComputeEpilogue, // First stage input types.
-        cutlass::FloatRoundStyle::round_to_nearest>;
-
-    using EVTCompute0 = cutlass::epilogue::fusion::Sm90EVT<Compute0, WScale, Accum>;
-
-    using Compute1 = cutlass::epilogue::fusion::Sm90Compute<cutlass::multiplies, ElementOutput,
-        ElementComputeEpilogue, // Second stage input types.
-        cutlass::FloatRoundStyle::round_to_nearest>;
-
-    using EVTCompute1 = cutlass::epilogue::fusion::Sm90EVT<Compute1, XScale, EVTCompute0>;
-
-    using ComputeBias = cutlass::epilogue::fusion::Sm90Compute<cutlass::plus,
-        ElementOutput, // Final (optional) stage output type.
-        ElementBias,   // Final stage input types.
-        cutlass::FloatRoundStyle::round_to_nearest>;
-
-    using EVTComputeBias = cutlass::epilogue::fusion::Sm90EVT<ComputeBias, Bias, EVTCompute1>;
-
-    using EpilogueEVT = EVTCompute1;
-
-    using CollectiveEpilogue = typename cutlass::epilogue::collective::CollectiveBuilder<cutlass::arch::Sm90,
-        cutlass::arch::OpClassTensorOp, TileShape, ClusterShape, cutlass::epilogue::collective::EpilogueTileAuto,
-        ElementAccumulator, ElementComputeEpilogue, ElementC, LayoutC, AlignmentC, ElementOutput, LayoutOutput,
-        AlignmentOutput, cutlass::epilogue::TmaWarpSpecialized, EpilogueEVT>::CollectiveOp;
-
-    using DefaultSchedule = cutlass::gemm::KernelTmaWarpSpecialized;
-    using PongSchedule = cutlass::gemm::KernelTmaWarpSpecializedPingpong;
-    using FastDefaultSchedule = cutlass::gemm::KernelTmaWarpSpecializedFP8FastAccum;
-    using FastPongSchedule = cutlass::gemm::KernelTmaWarpSpecializedPingpongFP8FastAccum;
-
-    using SlowAccum = DefaultSchedule;
-    using FastAccum = FastDefaultSchedule;
-    using MainLoopSchedule = cute::conditional_t<FAST_ACCUM, FastAccum, SlowAccum>;
-
-    using CollectiveMainloop = typename cutlass::gemm::collective::CollectiveBuilder<ArchTag, OperatorClass, ElementA,
-        LayoutA, AlignmentA, ElementB, LayoutB, AlignmentB, ElementAccumulator, TileShape, ClusterShape,
-        cutlass::gemm::collective::StageCountAutoCarveout<static_cast<int>(
-            sizeof(typename CollectiveEpilogue::SharedStorage))>,
-        MainLoopSchedule>::CollectiveOp;
-
-    using GemmKernel = cutlass::gemm::kernel::GemmUniversal<Shape<int, int, int, int>, // Indicates ProblemShape
-        CollectiveMainloop, CollectiveEpilogue, TileScheduler>;
-
-    using Gemm = cutlass::gemm::device::GemmUniversalAdapter<GemmKernel>;
-};
-
-} // namespace cutlass_kernels
-} // namespace kernels
-} // namespace tensorrt_llm
diff --git a/cpp/tensorrt_llm/kernels/cutlass_kernels/fp8_rowwise_gemm/fp8_rowwise_gemm_template.h b/cpp/tensorrt_llm/kernels/cutlass_kernels/fp8_rowwise_gemm/fp8_rowwise_gemm_template.h
deleted file mode 100644
index 8b731aae..00000000
--- a/cpp/tensorrt_llm/kernels/cutlass_kernels/fp8_rowwise_gemm/fp8_rowwise_gemm_template.h
+++ /dev/null
@@ -1,443 +0,0 @@
-/*
- * Copyright (c) 2022-2024, Meta Platforms, Inc. and affiliates.
- * All rights reserved.
- *
- * This source code is licensed under the BSD-style license found in the
- * LICENSE file in the root directory of this source tree.
- *
- * Copyright (c) 2023-2024, NVIDIA CORPORATION.  All rights reserved.
- *
- * Licensed under the Apache License, Version 2.0 (the "License");
- * you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-#pragma once
-
-#ifndef _WIN32
-#pragma GCC diagnostic push
-#pragma GCC diagnostic ignored "-Wstrict-aliasing"
-#endif // #ifndef _WIN32
-
-#include "cute/tensor.hpp"
-#include "cutlass/conv/convolution.h"
-// Order matters here, packed_stride.hpp is missing cute and convolution includes
-#include "cutlass/util/packed_stride.hpp"
-#include "cutlass_extensions/gemm_configs.h"
-
-#ifndef _WIN32
-#pragma GCC diagnostic pop
-#endif // #ifndef _WIN32
-
-#include "fp8_rowwise_gemm.h"
-#include "fp8_rowwise_gemm_kernel_template_sm90.h"
-#include "tensorrt_llm/common/cudaUtils.h"
-#include "tensorrt_llm/common/quantization.h"
-#include "tensorrt_llm/kernels/cutlass_kernels/cutlass_heuristic.h"
-#include "tensorrt_llm/kernels/cutlass_kernels/cutlass_type_conversion.h"
-
-#include <algorithm>
-#include <vector>
-
-namespace tk = tensorrt_llm::common;
-namespace tkc = tensorrt_llm::cutlass_extensions;
-
-using namespace cute;
-
-namespace tensorrt_llm
-{
-namespace kernels
-{
-namespace cutlass_kernels
-{
-
-template <typename Gemm>
-size_t typedFp8RowwiseGemmKernelLauncher(Gemm gemm, typename Gemm::Arguments args, void* D, void const* A,
-    void const* B, void const* C_bias, char* workspace, size_t workspaceBytes, cudaStream_t stream,
-    int* occupancy = nullptr)
-{
-    TLLM_LOG_DEBUG(__PRETTY_FUNCTION__);
-
-    using ElementT = typename Gemm::ElementA;
-
-    // Check shared memory size; throw when SMEM exceeds
-    int smem_size = int(sizeof(typename Gemm::GemmKernel::SharedStorage));
-    static int mMaxSmemSize = tk::getMaxSharedMemoryPerBlockOptin();
-    if (smem_size > mMaxSmemSize)
-    {
-        std::string errMsg = "SMEM size exceeds maximum allowed. Required " + std::to_string(smem_size) + ", got "
-            + std::to_string(mMaxSmemSize);
-        throw std::runtime_error("[TensorRT-LLM Error][fp8RowwiseGemm Runner] " + errMsg);
-    }
-
-    // Return workspace size
-    if (!A && !B && !D)
-    {
-        return gemm.get_workspace_size(args);
-    }
-
-    if (gemm.get_workspace_size(args) > workspaceBytes)
-    {
-        std::string errMsg("Requested workspace size insufficient. Required "
-            + std::to_string(gemm.get_workspace_size(args)) + ", got " + std::to_string(workspaceBytes));
-        throw std::runtime_error("[TensorRT-LLM Error][fp8RowwiseGemm Runner] " + errMsg);
-    }
-
-    auto can_implement = gemm.can_implement(args);
-    if (can_implement != cutlass::Status::kSuccess)
-    {
-        std::string errMsg = "fp8RowwiseGemm cutlass kernel not implemented given the params. Error: "
-            + std::string(cutlassGetStatusString(can_implement));
-        throw std::runtime_error("[TensorRT-LLM Error][fp8RowwiseGemm Runner] " + errMsg);
-    }
-
-    auto initStatus = gemm.initialize(args, workspace, stream);
-    if (initStatus != cutlass::Status::kSuccess)
-    {
-        std::string errMsg = "Failed to initialize. Error: " + std::string(cutlassGetStatusString(initStatus));
-        throw std::runtime_error("[TensorRT-LLM Error][fp8RowwiseGemm Runner] " + errMsg);
-    }
-
-    auto runStatus = gemm.run(stream);
-    if (runStatus != cutlass::Status::kSuccess)
-    {
-        std::string errMsg = "Failed to run gemm. Error: " + std::string(cutlassGetStatusString(runStatus));
-        throw std::runtime_error("[TensorRT-LLM Error][fp8RowwiseGemm Runner] " + errMsg);
-    }
-    return gemm.get_workspace_size(args);
-}
-
-template <typename Gemm>
-typename Gemm::Arguments prepareGemmArgsSm90(void* D, void const* A, void const* B, void const* C_bias,
-    tk::QuantMode quantOption, int m, int n, int k, float const* scale_d0, float const* scale_d1,
-    tkc::CutlassGemmConfig gemmConfig)
-{
-    using ElementT = typename Gemm::ElementA;
-    using ElementOutput = typename Gemm::ElementD;
-    using ElementComputeEpilogue = float;
-    using StrideA = typename Gemm::GemmKernel::StrideA;
-    using StrideB = typename Gemm::GemmKernel::StrideB;
-    using StrideC = typename Gemm::GemmKernel::StrideC;
-    using StrideD = typename Gemm::GemmKernel::StrideD;
-    int arg_m = m;
-    int arg_n = n;
-    ElementT const* ptr_A = reinterpret_cast<ElementT const*>(A);
-    ElementT const* ptr_B = reinterpret_cast<ElementT const*>(B);
-
-    StrideA stride_A = cutlass::make_cute_packed_stride(StrideA{}, cute::make_shape(arg_m, k, 1));
-    StrideB stride_B = cutlass::make_cute_packed_stride(StrideB{}, cute::make_shape(arg_n, k, 1));
-    StrideC stride_C;
-    StrideD stride_D = cutlass::make_cute_packed_stride(StrideD{}, cute::make_shape(arg_m, arg_n, 1));
-    typename Gemm::Arguments args
-        = {cutlass::gemm::GemmUniversalMode::kGemm, {arg_m, arg_n, k, 1}, {ptr_A, stride_A, ptr_B, stride_B},
-            {{}, // epilogue.thread
-                nullptr, stride_C, reinterpret_cast<ElementOutput*>(D), stride_D}};
-    args.epilogue.thread = {
-        {reinterpret_cast<ElementComputeEpilogue*>(const_cast<float*>(scale_d0))},
-        {
-            {reinterpret_cast<ElementComputeEpilogue*>(const_cast<float*>(scale_d1))}, {}, // Accumulator
-            {}                                                                             // Multiplies
-        },
-        {},                                                                                // Multiplies
-    };
-    return args;
-}
-
-template <typename T, typename CTAShape, typename ClusterShape>
-size_t genericFp8RowwiseGemmKernelLauncherSm90(void* D, void const* A, void const* B, void const* C_bias,
-    tk::QuantMode quantOption, int m, int n, int k, float const* scale_d0, float const* scale_d1,
-    tkc::CutlassGemmConfig gemmConfig, char* workspace, size_t workspaceBytes, cudaStream_t stream,
-    int* occupancy = nullptr)
-{
-    TLLM_LOG_DEBUG(__PRETTY_FUNCTION__);
-
-#ifdef COMPILE_HOPPER_TMA_GEMMS
-    using ElementInput = cutlass::float_e4m3_t;
-    using ElementOutput_ =
-        typename cutlass::platform::conditional<cutlass::platform::is_same<T, half>::value, cutlass::half_t, T>::type;
-#ifdef ENABLE_BF16
-    using ElementOutput =
-        typename cutlass::platform::conditional<cutlass::platform::is_same<ElementOutput_, __nv_bfloat16>::value,
-            cutlass::bfloat16_t, ElementOutput_>::type;
-#else
-    using ElementOutput = ElementOutput_;
-#endif
-
-    using AccumElementType = float;
-    using MainloopScheduleType = cutlass::gemm::KernelTmaWarpSpecializedFP8FastAccum;
-    using EpilogueScheduleType = cutlass::epilogue::TmaWarpSpecialized;
-    using TileSchedulerType = void;
-    using Gemm = typename DeviceGemmFp8RowwiseSm90<ElementInput, ElementOutput, AccumElementType, CTAShape,
-        ClusterShape, MainloopScheduleType, EpilogueScheduleType, TileSchedulerType>::Gemm;
-    auto args = prepareGemmArgsSm90<Gemm>(D, A, B, C_bias, quantOption, m, n, k, scale_d0, scale_d1, gemmConfig);
-    return typedFp8RowwiseGemmKernelLauncher(
-        Gemm{}, args, D, A, B, C_bias, workspace, workspaceBytes, stream, occupancy);
-#else  // COMPILE_HOPPER_TMA_GEMMS
-    throw std::runtime_error(
-        "[TensorRT-LLm Error][Fp8RowwiseGemmKernelLauncherSm90] Please recompile with support for hopper by passing "
-        "90-real "
-        "as an arch to build_wheel.py.");
-#endif // COMPILE_HOPPER_TMA_GEMMS
-}
-
-template <typename T, typename CTAShape>
-size_t dispatchGemmConfigSm90(void* D, void const* A, void const* B, void const* C_bias, tk::QuantMode quantOption,
-    int m, int n, int k, float const* scale_d0, float const* scale_d1, tkc::CutlassGemmConfig gemmConfig,
-    char* workspace, size_t workspaceBytes, cudaStream_t stream, int* occupancy = nullptr)
-{
-    TLLM_LOG_DEBUG(__PRETTY_FUNCTION__);
-    switch (gemmConfig.cluster_shape)
-    {
-    case tkc::ClusterShape::ClusterShape_1x1x1:
-        return genericFp8RowwiseGemmKernelLauncherSm90<T, CTAShape, Shape<_1, _1, _1>>(D, A, B, C_bias, quantOption, m,
-            n, k, scale_d0, scale_d1, gemmConfig, workspace, workspaceBytes, stream, occupancy);
-        break;
-    case tkc::ClusterShape::ClusterShape_2x1x1:
-        return genericFp8RowwiseGemmKernelLauncherSm90<T, CTAShape, Shape<_2, _1, _1>>(D, A, B, C_bias, quantOption, m,
-            n, k, scale_d0, scale_d1, gemmConfig, workspace, workspaceBytes, stream, occupancy);
-        break;
-    case tkc::ClusterShape::ClusterShape_1x2x1:
-        return genericFp8RowwiseGemmKernelLauncherSm90<T, CTAShape, Shape<_1, _2, _1>>(D, A, B, C_bias, quantOption, m,
-            n, k, scale_d0, scale_d1, gemmConfig, workspace, workspaceBytes, stream, occupancy);
-        break;
-    case tkc::ClusterShape::ClusterShape_2x2x1:
-        return genericFp8RowwiseGemmKernelLauncherSm90<T, CTAShape, Shape<_2, _2, _1>>(D, A, B, C_bias, quantOption, m,
-            n, k, scale_d0, scale_d1, gemmConfig, workspace, workspaceBytes, stream, occupancy);
-        break;
-    case tkc::ClusterShape::ClusterShape_1x8x1:
-        return genericFp8RowwiseGemmKernelLauncherSm90<T, CTAShape, Shape<_1, _8, _1>>(D, A, B, C_bias, quantOption, m,
-            n, k, scale_d0, scale_d1, gemmConfig, workspace, workspaceBytes, stream, occupancy);
-        break;
-    case tkc::ClusterShape::ClusterShape_8x1x1:
-        return genericFp8RowwiseGemmKernelLauncherSm90<T, CTAShape, Shape<_8, _1, _1>>(D, A, B, C_bias, quantOption, m,
-            n, k, scale_d0, scale_d1, gemmConfig, workspace, workspaceBytes, stream, occupancy);
-        break;
-    default:
-        throw std::runtime_error(
-            "[TensorRT-LLM Error][CutlassFp8RowwiseGemmRunner][dispatchGemmConfigSm90] Config is invalid for "
-            "Fp8 Rowwise GEMM.");
-        break;
-    }
-}
-
-template <typename T>
-size_t dispatchGemmToCutlassSm90(void* D, void const* A, void const* B, void const* C_bias, tk::QuantMode quantOption,
-    int m, int n, int k, float const* scale_d0, float const* scale_d1, tkc::CutlassGemmConfig gemmConfig,
-    char* workspace, size_t workspaceBytes, cudaStream_t stream, int* occupancy = nullptr)
-{
-    TLLM_LOG_DEBUG(__PRETTY_FUNCTION__);
-    constexpr int Ktile = 128 / sizeof(T);
-    using _Ktile = Int<Ktile>;
-    switch (gemmConfig.tile_config_sm90)
-    {
-    case tkc::CutlassTileConfigSM90::CtaShape64x16x128B:
-        return dispatchGemmConfigSm90<T, Shape<_64, _16, _Ktile>>(D, A, B, C_bias, quantOption, m, n, k, scale_d0,
-            scale_d1, gemmConfig, workspace, workspaceBytes, stream, occupancy);
-        break;
-    case tkc::CutlassTileConfigSM90::CtaShape64x32x128B:
-        return dispatchGemmConfigSm90<T, Shape<_64, _32, _Ktile>>(D, A, B, C_bias, quantOption, m, n, k, scale_d0,
-            scale_d1, gemmConfig, workspace, workspaceBytes, stream, occupancy);
-        break;
-    case tkc::CutlassTileConfigSM90::CtaShape64x64x128B:
-        return dispatchGemmConfigSm90<T, Shape<_64, _64, _Ktile>>(D, A, B, C_bias, quantOption, m, n, k, scale_d0,
-            scale_d1, gemmConfig, workspace, workspaceBytes, stream, occupancy);
-        break;
-    case tkc::CutlassTileConfigSM90::CtaShape64x128x128B:
-        return dispatchGemmConfigSm90<T, Shape<_64, _128, _Ktile>>(D, A, B, C_bias, quantOption, m, n, k, scale_d0,
-            scale_d1, gemmConfig, workspace, workspaceBytes, stream, occupancy);
-        break;
-    case tkc::CutlassTileConfigSM90::CtaShape128x16x128B:
-        return dispatchGemmConfigSm90<T, Shape<_128, _16, _Ktile>>(D, A, B, C_bias, quantOption, m, n, k, scale_d0,
-            scale_d1, gemmConfig, workspace, workspaceBytes, stream, occupancy);
-        break;
-    case tkc::CutlassTileConfigSM90::CtaShape128x32x128B:
-        return dispatchGemmConfigSm90<T, Shape<_128, _32, _Ktile>>(D, A, B, C_bias, quantOption, m, n, k, scale_d0,
-            scale_d1, gemmConfig, workspace, workspaceBytes, stream, occupancy);
-        break;
-    case tkc::CutlassTileConfigSM90::CtaShape128x64x128B:
-        return dispatchGemmConfigSm90<T, Shape<_128, _64, _Ktile>>(D, A, B, C_bias, quantOption, m, n, k, scale_d0,
-            scale_d1, gemmConfig, workspace, workspaceBytes, stream, occupancy);
-        break;
-    case tkc::CutlassTileConfigSM90::CtaShape128x128x128B:
-        return dispatchGemmConfigSm90<T, Shape<_128, _128, _Ktile>>(D, A, B, C_bias, quantOption, m, n, k, scale_d0,
-            scale_d1, gemmConfig, workspace, workspaceBytes, stream, occupancy);
-        break;
-    case tkc::CutlassTileConfigSM90::Undefined:
-        throw std::runtime_error(
-            "[TensorRT-LLm Error][CutlassFp8RowwiseGemmRunner][dispatchGemmToCutlassSm90] gemm config undefined.");
-        break;
-    case tkc::CutlassTileConfigSM90::ChooseWithHeuristic:
-        throw std::runtime_error(
-            "[TensorRT-LLm Error][CutlassFp8RowwiseGemmRunner][dispatchGemmToCutlassSm90] gemm config should have "
-            "already been set by "
-            "heuristic.");
-        break;
-    default:
-        throw std::runtime_error(
-            "[TensorRT-LLm Error][CutlassFp8RowwiseGemmRunner][dispatchGemmToCutlassSm90] Config is invalid for "
-            "Fp8 Rowwise GEMM.");
-        break;
-    }
-}
-
-template <typename T>
-CutlassFp8RowwiseGemmRunner<T>::CutlassFp8RowwiseGemmRunner()
-{
-    TLLM_LOG_DEBUG(__PRETTY_FUNCTION__);
-    mSm = tk::getSMVersion();
-}
-
-template <typename T>
-CutlassFp8RowwiseGemmRunner<T>::~CutlassFp8RowwiseGemmRunner()
-{
-    TLLM_LOG_DEBUG(__PRETTY_FUNCTION__);
-}
-
-template <typename T>
-size_t CutlassFp8RowwiseGemmRunner<T>::dispatchToArch(void* D, void const* A, void const* B, void const* C_bias,
-    tk::QuantMode quantOption, int m, int n, int k, float const* scale_d0, float const* scale_d1,
-    tkc::CutlassGemmConfig gemmConfig, char* workspace, size_t workspaceBytes, cudaStream_t stream, int* occupancy)
-{
-    TLLM_LOG_DEBUG(__PRETTY_FUNCTION__);
-    if (mSm == 90)
-    {
-        return dispatchGemmToCutlassSm90<T>(D, A, B, C_bias, quantOption, m, n, k, scale_d0, scale_d1, gemmConfig,
-            workspace, workspaceBytes, stream, occupancy);
-    }
-    else
-    {
-        throw std::runtime_error(
-            "[TensorRT-LLM Error][CutlassFp8RowwiseGemmRunner][GEMM Dispatch] Arch unsupported for CUTLASS "
-            "Fp8 Rowwise GEMM");
-    }
-    return 0;
-}
-
-template <typename T>
-void CutlassFp8RowwiseGemmRunner<T>::gemm(void* D, void const* A, void const* B, void const* C_bias,
-    tk::QuantMode quantOption, int m, int n, int k, float const* scale_d0, float const* scale_d1,
-    tkc::CutlassGemmConfig gemmConfig, char* workspace, size_t workspaceBytes, cudaStream_t stream, int* occupancy)
-{
-    TLLM_LOG_DEBUG(__PRETTY_FUNCTION__);
-    dispatchToArch(D, A, B, C_bias, quantOption, m, n, k, scale_d0, scale_d1, gemmConfig, workspace, workspaceBytes,
-        stream, occupancy);
-}
-
-template <typename T>
-std::vector<tkc::CutlassGemmConfig> CutlassFp8RowwiseGemmRunner<T>::getConfigs() const
-{
-    using tkc::CutlassTileConfig;
-    using tkc::CutlassGemmConfig;
-    using tkc::SplitKStyle;
-
-    std::vector<CutlassGemmConfig> candidateConfigs;
-
-    if (mSm != 90)
-    {
-        throw std::runtime_error(
-            "[TensorRT-LLM Error][CutlassFp8RowwiseGemmRunner][GEMM Dispatch] Arch unsupported for CUTLASS "
-            "gated GEMM");
-    }
-    tkc::CutlassGemmConfig::CandidateConfigTypeParam config_type_param
-        = tkc::CutlassGemmConfig::CandidateConfigTypeParam::HOPPER;
-    std::vector<CutlassGemmConfig> commonConfigs = get_candidate_configs(mSm, 2, config_type_param);
-    candidateConfigs.insert(candidateConfigs.end(), commonConfigs.begin(), commonConfigs.end());
-    // registers are not enough when N_tile is 256, remove some configs
-    candidateConfigs.erase(std::remove_if(candidateConfigs.begin(), candidateConfigs.end(),
-                               [](auto const& config)
-                               {
-                                   return config.tile_config_sm90 == tkc::CutlassTileConfigSM90::CtaShape64x256x128B
-                                       || config.tile_config_sm90 == tkc::CutlassTileConfigSM90::CtaShape128x256x128B;
-                               }),
-        candidateConfigs.end());
-    std::vector<tkc::CutlassTileConfigSM90> tilesSm90
-        = {tkc::CutlassTileConfigSM90::CtaShape64x16x128B, tkc::CutlassTileConfigSM90::CtaShape64x32x128B,
-            tkc::CutlassTileConfigSM90::CtaShape64x64x128B, tkc::CutlassTileConfigSM90::CtaShape64x128x128B,
-            tkc::CutlassTileConfigSM90::CtaShape128x16x128B, tkc::CutlassTileConfigSM90::CtaShape128x32x128B,
-            tkc::CutlassTileConfigSM90::CtaShape128x64x128B, tkc::CutlassTileConfigSM90::CtaShape128x128x128B};
-    for (auto const& tile_config : tilesSm90)
-    {
-        {
-            CutlassGemmConfig config(tile_config, tkc::MainloopScheduleType::AUTO, tkc::EpilogueScheduleType::AUTO,
-                tkc::ClusterShape::ClusterShape_1x8x1);
-            candidateConfigs.push_back(config);
-        }
-        {
-            CutlassGemmConfig config(tile_config, tkc::MainloopScheduleType::AUTO, tkc::EpilogueScheduleType::AUTO,
-                tkc::ClusterShape::ClusterShape_8x1x1);
-            candidateConfigs.push_back(config);
-        }
-    }
-    return candidateConfigs;
-}
-
-// Note: can be quite heavyweight; when possible, call once
-template <typename T>
-size_t CutlassFp8RowwiseGemmRunner<T>::getWorkspaceSizeImpl(int const m, int const n, int const k)
-{
-    TLLM_LOG_DEBUG(__PRETTY_FUNCTION__);
-    size_t workspace_size = 0;
-    auto gemmConfigs = CutlassFp8RowwiseGemmRunner<T>{}.getConfigs();
-    for (auto const& gemmConfig : gemmConfigs)
-    {
-        try
-        {
-            size_t curr_workspace_size = CutlassFp8RowwiseGemmRunner<T>::dispatchToArch(nullptr, nullptr, nullptr,
-                nullptr, tk::QuantMode{}, m, n, k, nullptr, nullptr, gemmConfig, nullptr, 0, 0);
-            workspace_size = std::max(workspace_size, curr_workspace_size);
-        }
-        catch (std::runtime_error& e)
-        {
-            // Swallow errors when SMEM exceeds maximum allowed
-            continue;
-        }
-    }
-
-    return workspace_size;
-}
-
-template <typename T>
-size_t CutlassFp8RowwiseGemmRunner<T>::getWorkspaceSize(int const m, int const n, int const k)
-{
-    TLLM_LOG_DEBUG(__PRETTY_FUNCTION__);
-
-    // Custom hash function for the MNK type
-    using MNK = std::tuple<int, int, int>;
-
-    struct MNKHash
-    {
-        size_t operator()(const MNK& mnk) const
-        {
-            auto h1 = std::hash<int>{}(std::get<0>(mnk));
-            auto h2 = std::hash<int>{}(std::get<1>(mnk));
-            auto h3 = std::hash<int>{}(std::get<2>(mnk));
-            return h1 ^ h2 ^ h3;
-        }
-    };
-
-    static std::unordered_map<MNK, size_t, MNKHash> workspace_hashmap;
-
-    size_t workspace_size = 0;
-    if (workspace_hashmap.find(std::make_tuple(m, n, k)) == workspace_hashmap.end())
-    {
-        workspace_size = CutlassFp8RowwiseGemmRunner<T>::getWorkspaceSizeImpl(m, n, k);
-        workspace_hashmap[std::make_tuple(m, n, k)] = workspace_size;
-    }
-    else
-    {
-        workspace_size = workspace_hashmap[std::make_tuple(m, n, k)];
-    }
-    return workspace_size;
-}
-
-} // namespace cutlass_kernels
-} // namespace kernels
-} // namespace tensorrt_llm
diff --git a/cpp/tensorrt_llm/kernels/cutlass_kernels/fpA_intB_gemm/fpA_intB_gemm_template.h b/cpp/tensorrt_llm/kernels/cutlass_kernels/fpA_intB_gemm/fpA_intB_gemm_template.h
index f1b7bb0f..c0c1c2d5 100644
--- a/cpp/tensorrt_llm/kernels/cutlass_kernels/fpA_intB_gemm/fpA_intB_gemm_template.h
+++ b/cpp/tensorrt_llm/kernels/cutlass_kernels/fpA_intB_gemm/fpA_intB_gemm_template.h
@@ -256,6 +256,7 @@ void filter_and_run_mixed_gemm(ActivationType const* A, WeightType const* B, Sca
             + std::to_string(arch::kMinComputeCapability) + " with stages set to " + std::to_string(Stages);
         throw std::runtime_error("[TensorRT-LLm Error][filter_and_run_mixed_gemm] " + err_msg);
     }
+#if defined(ENABLE_FP8)
     else if constexpr (cutlass::platform::is_same<ActivationType, __nv_fp8_e4m3>::value
         && arch::kMinComputeCapability < 89)
     {
@@ -264,6 +265,7 @@ void filter_and_run_mixed_gemm(ActivationType const* A, WeightType const* B, Sca
             + std::to_string(arch::kMinComputeCapability) + " with activation type set to FP8";
         throw std::runtime_error("[TensorRT-LLm Error][filter_and_run_mixed_gemm] " + err_msg);
     }
+#endif
     else
     {
         generic_mixed_gemm_kernelLauncher<ActivationType, WeightType, ScaleZeroType, BiasType, OutputType, arch,
@@ -309,7 +311,11 @@ void dispatch_gemm_config(ActivationType const* A, WeightType const* B, ScaleZer
 template <typename T>
 constexpr bool is_fp8()
 {
+#if defined(ENABLE_FP8)
     return std::is_same_v<T, __nv_fp8_e4m3> || std::is_same_v<T, __nv_fp8_e5m2>;
+#else
+    return false;
+#endif
 }
 
 template <typename ActivationType, typename WeightType, typename ScaleZeroType, typename BiasType, typename OutputType,
diff --git a/cpp/tensorrt_llm/kernels/cutlass_kernels/fpA_intB_gemm/fpA_intB_gemm_template_sm90.h b/cpp/tensorrt_llm/kernels/cutlass_kernels/fpA_intB_gemm/fpA_intB_gemm_template_sm90.h
index dd7bd96c..3540e398 100644
--- a/cpp/tensorrt_llm/kernels/cutlass_kernels/fpA_intB_gemm/fpA_intB_gemm_template_sm90.h
+++ b/cpp/tensorrt_llm/kernels/cutlass_kernels/fpA_intB_gemm/fpA_intB_gemm_template_sm90.h
@@ -56,10 +56,10 @@ void sm90_dispatch_epilogue_schedules(ActivationType const* A, WeightType const*
     case tkc::EpilogueScheduleType::AUTO:
         using EpilogueScheduleType = cute::conditional_t<size<0>(CTAShape{}) == Int<64>{},
             cutlass::epilogue::TmaWarpSpecialized, cutlass::epilogue::TmaWarpSpecializedCooperative>;
-        sm90_generic_mixed_gemm_kernelLauncher<ActivationType, WeightType, ScaleZeroType, BiasType, OutputType, QuantOp,
-            EpilogueTag, CTAShape, ClusterShape, MainloopScheduleType, EpilogueScheduleType>(A, B, weight_scales,
-            weight_zero_points, biases, alpha, C, m, n, k, group_size, gemm_config, workspace, workspace_bytes, stream,
-            occupancy);
+        // sm90_generic_mixed_gemm_kernelLauncher<ActivationType, WeightType, ScaleZeroType, BiasType, OutputType, QuantOp,
+        //   EpilogueTag, CTAShape, ClusterShape, MainloopScheduleType, EpilogueScheduleType>(A, B, weight_scales,
+        //    weight_zero_points, biases, alpha, C, m, n, k, group_size, gemm_config, workspace, workspace_bytes, stream,
+        //    occupancy);
         break;
     default:
         throw std::runtime_error(
diff --git a/cpp/tensorrt_llm/kernels/cutlass_kernels/fused_gated_gemm/fused_gated_gemm_template.h b/cpp/tensorrt_llm/kernels/cutlass_kernels/fused_gated_gemm/fused_gated_gemm_template.h
index 8ef0f9bb..b4c8aa0d 100644
--- a/cpp/tensorrt_llm/kernels/cutlass_kernels/fused_gated_gemm/fused_gated_gemm_template.h
+++ b/cpp/tensorrt_llm/kernels/cutlass_kernels/fused_gated_gemm/fused_gated_gemm_template.h
@@ -217,6 +217,7 @@ size_t dispatchGemmToCutlassSm90(void* D, void const* A, void const* B, void con
     char* workspace, size_t workspaceBytes, cudaStream_t stream, int* occupancy = nullptr)
 {
     TLLM_LOG_DEBUG(__PRETTY_FUNCTION__);
+#if defined(ENABLE_FP8)
     static_assert(std::is_same_v<T, __nv_fp8_e4m3>, "fusedGatedGemmSm90 only support FP8(e4m3)");
     constexpr int Ktile = 128 / sizeof(T);
     using _Ktile = Int<Ktile>;
@@ -270,6 +271,7 @@ size_t dispatchGemmToCutlassSm90(void* D, void const* A, void const* B, void con
             "gated GEMM.");
         break;
     }
+#endif
 }
 
 template <typename T>
@@ -290,6 +292,7 @@ size_t CutlassFusedGatedGemmRunner<T>::dispatchToArch(void* D, void const* A, vo
     tk::QuantMode quantOption, int m, int n, int k, float scale_d0, float scale_d1, float scale_output,
     tkc::CutlassGemmConfig gemmConfig, char* workspace, size_t workspaceBytes, cudaStream_t stream, int* occupancy)
 {
+#if defined(ENABLE_FP8)
     TLLM_LOG_DEBUG(__PRETTY_FUNCTION__);
     if constexpr (std::is_same_v<T, __nv_fp8_e4m3>)
     {
@@ -312,6 +315,7 @@ size_t CutlassFusedGatedGemmRunner<T>::dispatchToArch(void* D, void const* A, vo
             "gated "
             "GEMM");
     }
+#endif
     return 0;
 }
 
@@ -333,7 +337,7 @@ std::vector<tkc::CutlassGemmConfig> CutlassFusedGatedGemmRunner<T>::getConfigs()
     using tkc::SplitKStyle;
 
     std::vector<CutlassGemmConfig> candidateConfigs;
-
+#if defined(ENABLE_FP8)
     if constexpr (std::is_same_v<T, __nv_fp8_e4m3>)
     {
         if (mSm != 90)
@@ -381,6 +385,7 @@ std::vector<tkc::CutlassGemmConfig> CutlassFusedGatedGemmRunner<T>::getConfigs()
             "gated "
             "GEMM");
     }
+#endif
     return candidateConfigs;
 }
 
diff --git a/cpp/tensorrt_llm/kernels/cutlass_kernels/fused_gated_gemm/gemm_swiglu_e4m3.cu b/cpp/tensorrt_llm/kernels/cutlass_kernels/fused_gated_gemm/gemm_swiglu_e4m3.cu
index 2e603cfb..813a1851 100644
--- a/cpp/tensorrt_llm/kernels/cutlass_kernels/fused_gated_gemm/gemm_swiglu_e4m3.cu
+++ b/cpp/tensorrt_llm/kernels/cutlass_kernels/fused_gated_gemm/gemm_swiglu_e4m3.cu
@@ -22,7 +22,9 @@ namespace kernels
 {
 namespace cutlass_kernels
 {
+#if defined(ENABLE_FP8)
 template class CutlassFusedGatedGemmRunner<__nv_fp8_e4m3>;
+#endif
 } // namespace cutlass_kernels
 } // namespace kernels
 } // namespace tensorrt_llm
diff --git a/cpp/tensorrt_llm/kernels/cutlass_kernels/moe_gemm/moe_gemm_kernels.h b/cpp/tensorrt_llm/kernels/cutlass_kernels/moe_gemm/moe_gemm_kernels.h
index 0616c063..32503077 100644
--- a/cpp/tensorrt_llm/kernels/cutlass_kernels/moe_gemm/moe_gemm_kernels.h
+++ b/cpp/tensorrt_llm/kernels/cutlass_kernels/moe_gemm/moe_gemm_kernels.h
@@ -61,7 +61,7 @@ struct HopperGroupedGemmInput
     using StrideC = std::remove_pointer_t<cutlass::detail::TagToStrideC_t<LayoutC*>>;
 
     template <class T>
-    constexpr static bool IsFP8_v = std::is_same_v<T, __nv_fp8_e4m3> || std::is_same_v<T, __nv_fp8_e5m2>;
+    constexpr static bool IsFP8_v = false; // std::is_same_v<T, __nv_fp8_e4m3> || std::is_same_v<T, __nv_fp8_e5m2>;
 
     // Currently this should always just be T
     template <class T>
diff --git a/cpp/tensorrt_llm/kernels/cutlass_kernels/python/generate_kernels.py b/cpp/tensorrt_llm/kernels/cutlass_kernels/python/generate_kernels.py
index 096a8fe2..e9f208c7 100644
--- a/cpp/tensorrt_llm/kernels/cutlass_kernels/python/generate_kernels.py
+++ b/cpp/tensorrt_llm/kernels/cutlass_kernels/python/generate_kernels.py
@@ -83,7 +83,7 @@ QuantOpTag = {
 # The activations, biases, scales and zeros are instantiated using CUDA types,
 # not CUTLASS types. This map materializes the name of the CUDA type.
 CudaTypeName = {
-    DataType.e4m3: "__nv_fp8_e4m3",
+    # DataType.e4m3: "__nv_fp8_e4m3",
     DataType.bf16: "__nv_bfloat16",
     DataType.f16: "half",
     DataType.f32: "float"
@@ -229,8 +229,8 @@ def instantiate_operation_sm80(operation):
 def instantiate_operation(operation):
     if operation.arch == 80:
         return instantiate_operation_sm80(operation)
-    elif operation.arch == 90:
-        return instantiate_operation_sm90(operation)
+    # elif operation.arch == 90:
+    #    return instantiate_operation_sm90(operation)
 
 
 def get_file_content(launcher_inl_files, operations):
@@ -320,7 +320,7 @@ def generate_sm90_mixed_gemm_operations():
     # will remap those back to the signed type.
     # Takes the form (activation_type, weight_type, scalezero_type, bias_type, output_type)
     supported_dtypes = [
-        (DataType.e4m3, DataType.u4, DataType.f16, DataType.f16, DataType.f16),
+        # (DataType.e4m3, DataType.u4, DataType.f16, DataType.f16, DataType.f16),
         (DataType.f16, DataType.u4, DataType.f16, DataType.f16, DataType.f16),
         (DataType.bf16, DataType.u4, DataType.bf16, DataType.bf16,
          DataType.bf16),
@@ -487,8 +487,8 @@ if __name__ == "__main__":
 
     # The goal here is to group kernels with common instantiations together in order to reduce template instantiation overheads.
     # Template instantiation dominates the time in a compilation unit, so it is the most important factor to improve.
-    operations = generate_sm90_operations()
-    operations += generate_sm80_operations()
+    # operations = generate_sm90_operations()
+    operations = generate_sm80_operations()
     op_groups = dict()
     for op in operations:
         dict_key = (op.gemm_kind, op.arch, op.cta_shape[0])
diff --git a/cpp/tensorrt_llm/kernels/decoderMaskedMultiheadAttention/decoderMaskedMultiheadAttentionLaunch.h b/cpp/tensorrt_llm/kernels/decoderMaskedMultiheadAttention/decoderMaskedMultiheadAttentionLaunch.h
index b116bb39..a5be54e9 100644
--- a/cpp/tensorrt_llm/kernels/decoderMaskedMultiheadAttention/decoderMaskedMultiheadAttentionLaunch.h
+++ b/cpp/tensorrt_llm/kernels/decoderMaskedMultiheadAttention/decoderMaskedMultiheadAttentionLaunch.h
@@ -146,7 +146,7 @@ inline void multi_block_grid_setup(dim3& grid, Multihead_attention_params<T, DO_
     grid.z = params.seq_len_tile;
 }
 
-#define MMHA_LAUNCH_CHECK(DYNAMIC_THDS_PER_BLOCK, DO_MULTI_BLOCK)                                                      \
+#define MMHA_LAUNCH_CHECK(DYNAMIC_THDS_PER_BLOCK)                                                                      \
     std::size_t const dynamic_smem_sz{                                                                                 \
         mmha::smem_size_in_bytes<T, Dh, DO_MULTI_BLOCK>(params, DYNAMIC_THDS_PER_BLOCK)};                              \
     /* Set 46KB threshold here because we have to take static/driver shared memory into consideration. */              \
@@ -183,33 +183,14 @@ inline void multi_block_grid_setup(dim3& grid, Multihead_attention_params<T, DO_
     const auto mmhaFunc = mmha::masked_multihead_attention_kernel<T, T_cache, TKcache, KVCacheBuffer, KCacheBuffer,    \
         Dh, DYNAMIC_THDS_PER_BLOCK, KernelParamsType::DO_CROSS_ATTENTION, HAS_BEAMS, ENABLE_MULTI_BLOCK, POS_SHIFT,    \
         BLOCK_SPARSE_ATTN, IMPLICIT_REL_ATTN_BIAS, QK_TANH_SCALE>;                                                     \
-    if (tensorrt_llm::common::getEnvEnablePDL())                                                                       \
-    {                                                                                                                  \
-        TLLM_LOG_DEBUG("Enable PDL in MMHA");                                                                          \
-        cudaLaunchConfig_t kernelConfig = {0};                                                                         \
-        kernelConfig.gridDim = grid;                                                                                   \
-        kernelConfig.blockDim = DYNAMIC_THDS_PER_BLOCK;                                                                \
-        kernelConfig.dynamicSmemBytes = dynamic_smem_sz;                                                               \
-        kernelConfig.stream = stream;                                                                                  \
-                                                                                                                       \
-        cudaLaunchAttribute attribute[1];                                                                              \
-        attribute[0].id = cudaLaunchAttributeProgrammaticStreamSerialization;                                          \
-        attribute[0].val.programmaticStreamSerializationAllowed = 1;                                                   \
-        kernelConfig.attrs = attribute;                                                                                \
-        kernelConfig.numAttrs = 1;                                                                                     \
-        TLLM_CUDA_CHECK(cudaLaunchKernelEx(&kernelConfig, mmhaFunc, params, kv_cache_buffer, k_cache_buffer));         \
-    }                                                                                                                  \
-    else                                                                                                               \
-    {                                                                                                                  \
-        mmhaFunc<<<grid, DYNAMIC_THDS_PER_BLOCK, dynamic_smem_sz, stream>>>(params, kv_cache_buffer, k_cache_buffer);  \
-    }
+    mmhaFunc<<<grid, DYNAMIC_THDS_PER_BLOCK, dynamic_smem_sz, stream>>>(params, kv_cache_buffer, k_cache_buffer);
 
 // if resources are not enough to launch 512 threads per block, we will fallback to 256.
-#define MMHA_512_BLOCKSIZE_CHECK(DO_MULTI_BLOCK)                                                                       \
-    MMHA_LAUNCH_CHECK(512, DO_MULTI_BLOCK);                                                                            \
+#define MMHA_512_BLOCKSIZE_CHECK()                                                                                     \
+    MMHA_LAUNCH_CHECK(512);                                                                                            \
     if (available_blocks <= 0)                                                                                         \
     {                                                                                                                  \
-        MMHA_LAUNCH_CHECK(256, DO_MULTI_BLOCK);                                                                        \
+        MMHA_LAUNCH_CHECK(256);                                                                                        \
         dynamic_block_size = 256;                                                                                      \
     }                                                                                                                  \
     else                                                                                                               \
@@ -218,32 +199,15 @@ inline void multi_block_grid_setup(dim3& grid, Multihead_attention_params<T, DO_
     }
 
 // if resources are not enough to launch 1024 threads per block, we will fallback to 512.
-#define MMHA_1024_BLOCKSIZE_CHECK(DO_MULTI_BLOCK)                                                                      \
-    MMHA_LAUNCH_CHECK(1024, DO_MULTI_BLOCK);                                                                           \
+#define MMHA_1024_BLOCKSIZE_CHECK()                                                                                    \
+    MMHA_LAUNCH_CHECK(1024);                                                                                           \
     if (available_blocks > 0)                                                                                          \
     {                                                                                                                  \
         dynamic_block_size = 1024;                                                                                     \
     }                                                                                                                  \
     else                                                                                                               \
     {                                                                                                                  \
-        MMHA_512_BLOCKSIZE_CHECK(DO_MULTI_BLOCK);                                                                      \
-    }
-
-// The previous dynamic_block_size might be calculated based on muli-block-mode enabled,
-// while the final launch set it disabled, so we need to fallback to smaller kernel block size
-// if there are not enough resources.
-#define MMHA_DYNAMIC_LAUNCH(DO_MULTI_BLOCK)                                                                            \
-    if (dynamic_block_size == 256)                                                                                     \
-    {                                                                                                                  \
-        MMHA_KERNEL(256, DO_MULTI_BLOCK);                                                                              \
-    }                                                                                                                  \
-    else if (dynamic_block_size == 512)                                                                                \
-    {                                                                                                                  \
-        MMHA_KERNEL(512, DO_MULTI_BLOCK);                                                                              \
-    }                                                                                                                  \
-    else if (dynamic_block_size == 1024)                                                                               \
-    {                                                                                                                  \
-        MMHA_KERNEL(1024, DO_MULTI_BLOCK);                                                                             \
+        MMHA_512_BLOCKSIZE_CHECK();                                                                                    \
     }
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
@@ -285,16 +249,16 @@ void mmha_launch_kernel_ex(KernelParamsType const& params, KVCacheBuffer const&
     int available_blocks = -1;
     if (dynamic_block_size < 512)
     {
-        MMHA_LAUNCH_CHECK(256, DO_MULTI_BLOCK);
+        MMHA_LAUNCH_CHECK(256);
         dynamic_block_size = 256;
     }
     else if (dynamic_block_size < 1024)
     {
-        MMHA_512_BLOCKSIZE_CHECK(DO_MULTI_BLOCK);
+        MMHA_512_BLOCKSIZE_CHECK();
     }
     else if (dynamic_block_size == 1024)
     {
-        MMHA_1024_BLOCKSIZE_CHECK(DO_MULTI_BLOCK);
+        MMHA_1024_BLOCKSIZE_CHECK();
     }
 
     // Block size can be finetuned by TRTLLM_MMHA_KERNEL_BLOCK_SIZE.
@@ -323,8 +287,7 @@ void mmha_launch_kernel_ex(KernelParamsType const& params, KVCacheBuffer const&
         }
         else
         {
-            MMHA_512_BLOCKSIZE_CHECK(false);
-            MMHA_DYNAMIC_LAUNCH(false);
+            MMHA_KERNEL(512, false);
         }
         break;
     case 1024:
@@ -334,8 +297,7 @@ void mmha_launch_kernel_ex(KernelParamsType const& params, KVCacheBuffer const&
         }
         else
         {
-            MMHA_1024_BLOCKSIZE_CHECK(false);
-            MMHA_DYNAMIC_LAUNCH(false);
+            MMHA_KERNEL(1024, false);
         }
         break;
     default: TLLM_CHECK_WITH_INFO(false, "Wrong kernel block size for launching the MMHA kernel.");
@@ -489,3 +451,4 @@ void mmha_launch_kernel(KernelParamsType const& params, KVCacheBuffer const& kv_
 
 } // namespace kernels
 } // namespace tensorrt_llm
+
diff --git a/cpp/tensorrt_llm/kernels/decoderMaskedMultiheadAttention/decoderMaskedMultiheadAttentionTemplate.h b/cpp/tensorrt_llm/kernels/decoderMaskedMultiheadAttention/decoderMaskedMultiheadAttentionTemplate.h
index ba8a9fa9..f99f53ad 100644
--- a/cpp/tensorrt_llm/kernels/decoderMaskedMultiheadAttention/decoderMaskedMultiheadAttentionTemplate.h
+++ b/cpp/tensorrt_llm/kernels/decoderMaskedMultiheadAttention/decoderMaskedMultiheadAttentionTemplate.h
@@ -1209,7 +1209,7 @@ struct Launch_bounds_config
     static constexpr int MAX_THREADS_PER_BLOCK = 0;
     static constexpr int MIN_BLOCKS_PER_SM = 0;
 };
-
+#ifdef ENABLE_FP8
 template <>
 struct Launch_bounds_config<uint16_t, __nv_fp8_e4m3, 256u, 64u, false, false, false>
 {
@@ -1232,7 +1232,7 @@ struct Launch_bounds_config<uint16_t, __nv_fp8_e4m3, 256u, 256u, false, true, fa
     static constexpr int MAX_THREADS_PER_BLOCK = 256u;
     static constexpr int MIN_BLOCKS_PER_SM = 3u;
 };
-
+#endif
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 inline __device__ constexpr uint32_t shfl_mask(int threads)
@@ -1312,8 +1312,8 @@ __global__ void __launch_bounds__(MAX_THEADS_PER_BLOCK, MIN_BLOCKS_PER_SM) maske
     static constexpr bool ENABLE_8BITS_K_CACHE = sizeof(TKcache) == 1;
     static constexpr bool ENABLE_8BITS_KV_CACHE = sizeof(Tcache) == 1;
     // FP8 KV Cache.
-    static constexpr bool FP8_K_CACHE = std::is_same<TKcache, __nv_fp8_e4m3>::value;
-    static constexpr bool FP8_KV_CACHE = std::is_same<Tcache, __nv_fp8_e4m3>::value;
+    static constexpr bool FP8_K_CACHE = false; // std::is_same<TKcache, __nv_fp8_e4m3>::value;
+    static constexpr bool FP8_KV_CACHE = false; // std::is_same<Tcache, __nv_fp8_e4m3>::value;
     // INT8 KV Cache.
     static constexpr bool INT8_KV_CACHE = std::is_same<Tcache, int8_t>::value;
 
@@ -2448,8 +2448,8 @@ __global__ void __launch_bounds__(MAX_THEADS_PER_BLOCK, MIN_BLOCKS_PER_SM) maske
     }
 
     // Quantized output only supports fp8 currently, which should be used together with FP8 Context FMHA.
-    using Quantized_t = __nv_fp8_e4m3;
-    using Quantized_vec = typename packed_type<__nv_fp8_e4m3, num_elems<V_vec_accum>::value>::type;
+    // using Quantized_t = __nv_fp8_e4m3;
+    // using Quantized_vec = typename packed_type<__nv_fp8_e4m3, num_elems<V_vec_accum>::value>::type;
     auto const bhi = tensorrt_llm::common::flat_index2(batch_beam_idx, hi, num_heads);
     auto const bhi_seq_len_tile = bhi * params.seq_len_tile;
     // Output the final values.
@@ -2459,20 +2459,20 @@ __global__ void __launch_bounds__(MAX_THEADS_PER_BLOCK, MIN_BLOCKS_PER_SM) maske
 #ifdef MMHA_USE_FP32_ACCUM_FOR_OUT
         if (!MULTI_BLOCK_FLAG)
         {
-            if (write_attention_quant)
-            {
-                out = mul<V_vec_accum, float>(*params.attention_out_scale_orig_quant, out);
-                Quantized_vec final_out;
-                convert_to_fp8(&final_out, out);
-                *reinterpret_cast<Quantized_vec*>(reinterpret_cast<Quantized_t*>(params.out) + bhvi) = final_out;
-            }
-            else
-            {
+            // if (write_attention_quant)
+            // {
+            //    out = mul<V_vec_accum, float>(*params.attention_out_scale_orig_quant, out);
+            //    Quantized_vec final_out;
+            //    convert_to_fp8(&final_out, out);
+            //    *reinterpret_cast<Quantized_vec*>(reinterpret_cast<Quantized_t*>(params.out) + bhvi) = final_out;
+            // }
+            // else
+            // {
                 // This makes sure we have coalesced memory access.
                 V_vec_k final_out;
                 convert_from_float(&final_out, out);
                 *reinterpret_cast<V_vec_k*>(static_cast<T*>(params.out) + bhvi) = final_out;
-            }
+            // }
         }
         else
         {
@@ -2638,17 +2638,17 @@ __global__ void __launch_bounds__(MAX_THEADS_PER_BLOCK, MIN_BLOCKS_PER_SM) maske
 
                 thread_accumulated_out = mul<V_vec_k, Tk, V_vec_k>(inv_sum_compute, thread_accumulated_out);
 
-                if (write_attention_quant)
-                {
-                    Quantized_vec final_out;
-                    convert_to_fp8(&final_out, thread_accumulated_out);
-                    *reinterpret_cast<Quantized_vec*>(reinterpret_cast<Quantized_t*>(params.out) + bhi * Dh + oi)
-                        = final_out;
-                }
-                else
+                // if (write_attention_quant)
+                // {
+                //    Quantized_vec final_out;
+                //    convert_to_fp8(&final_out, thread_accumulated_out);
+                //    *reinterpret_cast<Quantized_vec*>(reinterpret_cast<Quantized_t*>(params.out) + bhi * Dh + oi)
+                //        = final_out;
+                // }
+                // else
                 {
                     *reinterpret_cast<V_vec_k*>(static_cast<T*>(params.out) + (bhi * Dh + oi)) = thread_accumulated_out;
-                }
+                //}
             }
 
             // Reset qk_current_smem and block_counter for the next timestep
diff --git a/cpp/tensorrt_llm/kernels/decoderMaskedMultiheadAttention/decoderXQAImplJIT/decoderXQAImplJIT.cpp b/cpp/tensorrt_llm/kernels/decoderMaskedMultiheadAttention/decoderXQAImplJIT/decoderXQAImplJIT.cpp
index 8bfd097e..e02756ba 100644
--- a/cpp/tensorrt_llm/kernels/decoderMaskedMultiheadAttention/decoderXQAImplJIT/decoderXQAImplJIT.cpp
+++ b/cpp/tensorrt_llm/kernels/decoderMaskedMultiheadAttention/decoderXQAImplJIT/decoderXQAImplJIT.cpp
@@ -80,6 +80,7 @@ bool DecoderXQAImplJIT::mayHavePerfGain(XQAParams const& xqaParams) const
 
 bool DecoderXQAImplJIT::shouldUse(XQAParams const& umbrellaXQAParams, bool forConfigurePlugin)
 {
+    return false;
     if (forConfigurePlugin)
     {
         for (int beam_width = 1; beam_width <= umbrellaXQAParams.beam_width; ++beam_width)
@@ -275,12 +276,14 @@ void DecoderXQAImplJIT::runImpl(XQAParams const& xqaParams, KVCacheBuffer const&
     }
     appendParam(&launchParams.batch_size);
     appendParam(&launchParams.kv_scale_quant_orig);
+#if (__CUDACC_VER_MAJOR__ >= 12)
     CUtensorMap tensorMap{};
     if (isGMMAKernel)
     {
         tensorMap = makeTensorMapForKVCache(mDriver, xqaParams, kv_cache_buffer);
         appendParam(&tensorMap);
     }
+#endif
     appendParam(&launchParams.semaphores);
     appendParam(&launchParams.scratch);
     kernelParams[idxNextParam] = nullptr; // one extra nullptr at end as guard.
@@ -294,7 +297,7 @@ void DecoderXQAImplJIT::runImpl(XQAParams const& xqaParams, KVCacheBuffer const&
     dim3 blockDim(128, 1, isGMMAKernel ? 3 : 2);
     cubinObj->launch(gridDim, blockDim, stream, kernelParams);
     sync_check_cuda_error();
-
+#ifdef ENABLE_FP8
     if (needOutputCvt)
     {
         tensorrt_llm::kernels::invokeConversion<__nv_fp8_e4m3, T>(static_cast<__nv_fp8_e4m3*>(xqaParams.output),
@@ -303,6 +306,7 @@ void DecoderXQAImplJIT::runImpl(XQAParams const& xqaParams, KVCacheBuffer const&
             stream);
         sync_check_cuda_error();
     }
+#endif
 }
 
 } // namespace kernels
diff --git a/cpp/tensorrt_llm/kernels/decoderMaskedMultiheadAttention/decoderXQAImplPrecompiled.cpp b/cpp/tensorrt_llm/kernels/decoderMaskedMultiheadAttention/decoderXQAImplPrecompiled.cpp
index f1736651..2ebedf7a 100644
--- a/cpp/tensorrt_llm/kernels/decoderMaskedMultiheadAttention/decoderXQAImplPrecompiled.cpp
+++ b/cpp/tensorrt_llm/kernels/decoderMaskedMultiheadAttention/decoderXQAImplPrecompiled.cpp
@@ -284,12 +284,12 @@ public:
             }
             appendParam(&launchParams.batch_size);
             appendParam(&launchParams.kv_scale_quant_orig);
-            CUtensorMap tensorMap{};
-            if (isGmmaKernel)
-            {
-                tensorMap = makeTensorMapForKVCache(mDriver, xqaParams, kv_cache_buffer);
-                appendParam(&tensorMap);
-            }
+            // CUtensorMap tensorMap{};
+            // if (isGmmaKernel)
+            // {
+            //     tensorMap = makeTensorMapForKVCache(mDriver, xqaParams, kv_cache_buffer);
+            //     appendParam(&tensorMap);
+            // }
             appendParam(&launchParams.semaphores);
             appendParam(&launchParams.scratch);
             kernelParams[idxNextParam] = nullptr; // one extra nullptr at end as guard.
@@ -305,14 +305,14 @@ public:
 
         sync_check_cuda_error();
 
-        if (needOutputCvt)
-        {
-            tensorrt_llm::kernels::invokeConversion<__nv_fp8_e4m3, T>(static_cast<__nv_fp8_e4m3*>(xqaParams.output),
-                static_cast<T const*>(launchParams.output),
-                xqaParams.head_size * xqaParams.num_q_heads * xqaParams.total_num_input_tokens, xqaParams.fp8_out_scale,
-                stream);
-            sync_check_cuda_error();
-        }
+        // if (needOutputCvt)
+        // {
+        //    tensorrt_llm::kernels::invokeConversion<__nv_fp8_e4m3, T>(static_cast<__nv_fp8_e4m3*>(xqaParams.output),
+        //        static_cast<T const*>(launchParams.output),
+        //        xqaParams.head_size * xqaParams.num_q_heads * xqaParams.total_num_input_tokens, xqaParams.fp8_out_scale,
+        //        stream);
+        //    sync_check_cuda_error();
+        // }
     }
 
 protected:
diff --git a/cpp/tensorrt_llm/kernels/decoderMaskedMultiheadAttention/tensorMapUtils.cpp b/cpp/tensorrt_llm/kernels/decoderMaskedMultiheadAttention/tensorMapUtils.cpp
index ca2a2c57..82c1d994 100644
--- a/cpp/tensorrt_llm/kernels/decoderMaskedMultiheadAttention/tensorMapUtils.cpp
+++ b/cpp/tensorrt_llm/kernels/decoderMaskedMultiheadAttention/tensorMapUtils.cpp
@@ -24,6 +24,7 @@ namespace tensorrt_llm
 namespace kernels
 {
 
+#if __CUDACC_VER_MAJOR__ >= 12
 namespace
 {
 
@@ -138,6 +139,7 @@ template CUtensorMap makeTensorMapForKVCache(
     std::shared_ptr<CUDADriverWrapper> const&, XQAParams const&, KVBlockArray const&);
 template CUtensorMap makeTensorMapForKVCache(
     std::shared_ptr<CUDADriverWrapper> const&, XQAParams const&, KVLinearBuffer const&);
+#endif
 
 } // namespace kernels
 } // namespace tensorrt_llm
diff --git a/cpp/tensorrt_llm/kernels/decoderMaskedMultiheadAttention/tensorMapUtils.h b/cpp/tensorrt_llm/kernels/decoderMaskedMultiheadAttention/tensorMapUtils.h
index 8570c418..711233de 100644
--- a/cpp/tensorrt_llm/kernels/decoderMaskedMultiheadAttention/tensorMapUtils.h
+++ b/cpp/tensorrt_llm/kernels/decoderMaskedMultiheadAttention/tensorMapUtils.h
@@ -21,10 +21,11 @@ namespace tensorrt_llm
 {
 namespace kernels
 {
-
+#if __CUDACC_VER_MAJOR__ >= 12
 template <typename KVCacheBuffer>
 CUtensorMap makeTensorMapForKVCache(std::shared_ptr<tensorrt_llm::common::CUDADriverWrapper> const& driver,
     XQAParams const& xqaParams, KVCacheBuffer const& kv_cache_buffer);
-
+#else
+#endif
 } // namespace kernels
 } // namespace tensorrt_llm
diff --git a/cpp/tensorrt_llm/kernels/decoderMaskedMultiheadAttentionUtils.h b/cpp/tensorrt_llm/kernels/decoderMaskedMultiheadAttentionUtils.h
index 43476a8b..34f91cd9 100644
--- a/cpp/tensorrt_llm/kernels/decoderMaskedMultiheadAttentionUtils.h
+++ b/cpp/tensorrt_llm/kernels/decoderMaskedMultiheadAttentionUtils.h
@@ -399,7 +399,7 @@ inline __device__ float4 add(float4 a, float4 b)
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
-#ifdef ENABLE_FP8
+// #ifdef ENABLE_FP8
 inline __device__ Float8_ add(Float8_ a, Float8_ b)
 {
     Float8_ c;
@@ -409,7 +409,7 @@ inline __device__ Float8_ add(Float8_ a, Float8_ b)
     c.w = add(a.w, b.w);
     return c;
 }
-#endif
+// #endif
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
diff --git a/cpp/tensorrt_llm/kernels/decodingKernels.cu b/cpp/tensorrt_llm/kernels/decodingKernels.cu
index d3e2cbce..7233b226 100644
--- a/cpp/tensorrt_llm/kernels/decodingKernels.cu
+++ b/cpp/tensorrt_llm/kernels/decodingKernels.cu
@@ -22,7 +22,7 @@
 #include "tensorrt_llm/kernels/decodingKernels.h"
 #ifndef CUDART_VERSION
 #error CUDART_VERSION Undefined!
-#elif (CUDART_VERSION >= 11050)
+#elif (CUDART_VERSION >= 11040)
 #include <cub/cub.cuh>
 #else
 #include "3rdparty/cub/cub.cuh"
diff --git a/cpp/tensorrt_llm/kernels/internal_cutlass_kernels/include/low_latency_gemm.h b/cpp/tensorrt_llm/kernels/internal_cutlass_kernels/include/low_latency_gemm.h
deleted file mode 100644
index ee4da4b8..00000000
--- a/cpp/tensorrt_llm/kernels/internal_cutlass_kernels/include/low_latency_gemm.h
+++ /dev/null
@@ -1,128 +0,0 @@
-/*
- * Copyright (c) 2020-2024, NVIDIA CORPORATION.  All rights reserved.
- *
- * Licensed under the Apache License, Version 2.0 (the "License");
- * you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-#pragma once
-
-#include "cutlass_extensions/gemm_configs.h"
-#include "tensorrt_llm/common/cudaUtils.h"
-
-#include <cuda_runtime_api.h>
-#include <vector>
-
-// namespace tk = tensorrt_llm::common;
-
-namespace tkc = tensorrt_llm::cutlass_extensions;
-
-namespace tensorrt_llm
-{
-namespace kernels
-{
-namespace internal_cutlass_kernels
-{
-
-enum class KernelScheduleType
-{
-    AUTO,
-    WS_PREFETECH,       // KernelTmaWarpSpecializedFP8FastAccumWithPrefetch
-    WS_SPLIT_PREFETECH, // KernelTmaWarpSpecializedFP8FastAccumWithPrefetchAndSplitDMA
-    //
-};
-
-struct LowLatencyCutlassGemmConfig
-{
-
-    cutlass_extensions::CutlassGemmConfig cutlass_gemm_config;
-    KernelScheduleType kernel_schedule = KernelScheduleType::AUTO;
-
-    std::string toString() const
-    {
-
-        std::stringstream tactic;
-        tactic << cutlass_gemm_config.toString();
-
-        tactic << "\tkernel sched: " << static_cast<int>(kernel_schedule);
-        tactic << "\n";
-        return tactic.str();
-    }
-};
-
-inline std::ostream& operator<<(std::ostream& out, LowLatencyCutlassGemmConfig const& config)
-{
-    // clang-format off
-    if (config.cutlass_gemm_config.is_sm90)
-    {
-        out << "tile_config_sm90_enum: " << int(config.cutlass_gemm_config.tile_config_sm90)
-            << ", mainloop_schedule_enum: " << int(config.cutlass_gemm_config.mainloop_schedule)
-            << ", epilogue_schedule_enum: " << int(config.cutlass_gemm_config.epilogue_schedule)
-            << ", cluster_shape_enum: " << int(config.cutlass_gemm_config.cluster_shape);
-    }
-    else
-    {
-        out << "tile_config_enum: " << int(config.cutlass_gemm_config.tile_config)
-            << ", split_k_style_enum: " << int(config.cutlass_gemm_config.split_k_style)
-            << ", split_k_factor: " << config.cutlass_gemm_config.split_k_factor
-            << ", stages: " << config.cutlass_gemm_config.stages;
-    }
-    out<<config.cutlass_gemm_config<<" kernel_schedule_enum: "<<static_cast<int>(config.kernel_schedule);
-    return out;
-}
-
-
-
-class CutlassLowLatencyFp8GemmRunnerInterface
-{
-public:
-
-    using ConfigType = LowLatencyCutlassGemmConfig;
-
-    CutlassLowLatencyFp8GemmRunnerInterface() {}
-
-    virtual ~CutlassLowLatencyFp8GemmRunnerInterface() {}
-
-    virtual void gemm(__nv_fp8_e4m3* A, __nv_fp8_e4m3* B, float alpha, float beta, void const* C, void* D, int m, int n,
-        int k,float pdl_overlap_ratio,float prefetch_ratio, ConfigType gemmConfig, char* workspacePtr, size_t const workspaceBytes, cudaStream_t stream)
-        = 0;
-
-    virtual size_t getWorkspaceSize(int const m, int const n, int const k) = 0;
-
-    virtual std::vector<ConfigType> getConfigs() const = 0;
-};
-
-template <typename T>
-
-class CutlassLowLatencyFp8GemmRunner : public virtual CutlassLowLatencyFp8GemmRunnerInterface
-{
-public:
-
-    CutlassLowLatencyFp8GemmRunner();
-    ~CutlassLowLatencyFp8GemmRunner() = default;
-    // gemm A rowMajor,  B colMajor, C and D rowMajor
-    void gemm(__nv_fp8_e4m3* A, __nv_fp8_e4m3* B, float alpha, float beta, void const* C, void* D, int m, int n, int k, float pdl_overlap_ratio,float prefetech_ratio,
-        ConfigType gemmConfig, char* workspacePtr, size_t const workspaceBytes,
-        cudaStream_t stream) override;
-    size_t getWorkspaceSize(int const m, int const n, int const k) override;
-    std::vector<ConfigType> getConfigs() const override;
-
-private:
-    size_t dispatchToArch(__nv_fp8_e4m3 const* A, __nv_fp8_e4m3 const* B, float alpha, float beta, void const* C,
-        void* D, int m, int n, int k,float pdl_overlap_ratio,float prefetech_ratio, ConfigType gemmConfig, char* workspacePtr,
-        size_t const workspaceBytes, cudaStream_t stream);
-    int mSm;
-};
-
-}; // namespace internal_cutlass_kernels
-}; // namespace kernels
-}; // namespace tensorrt_llm
diff --git a/cpp/tensorrt_llm/kernels/lruKernel.cu b/cpp/tensorrt_llm/kernels/lruKernel.cu
deleted file mode 100644
index a0fc4fdb..00000000
--- a/cpp/tensorrt_llm/kernels/lruKernel.cu
+++ /dev/null
@@ -1,440 +0,0 @@
-/*
- * Copyright (c) 2020-2024, NVIDIA CORPORATION.  All rights reserved.
- *
- * Licensed under the Apache License, Version 2.0 (the "License");
- * you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-#include <cuda_runtime_api.h>
-
-#include <cooperative_groups/memcpy_async.h>
-#include <cuda/pipeline>
-
-#include <cuda_bf16.h>
-#include <cuda_fp16.h>
-
-#ifdef ENABLE_FP8
-#include <cuda_fp8.h>
-#endif
-
-#include "lruKernel.h"
-#include "tensorrt_llm/common/cudaTypeUtils.cuh"
-
-using namespace tensorrt_llm::common;
-
-namespace tensorrt_llm
-{
-namespace kernels
-{
-
-__forceinline__ __device__ float copysignf_pos(float a, float b)
-{
-    float r;
-    r = __int_as_float(__float_as_int(a) | (__float_as_int(b) & 0x80000000));
-    return r;
-}
-
-#pragma nv_diag_suppress static_var_with_dynamic_init
-
-template <typename T, int CHANNELS_PER_BLOCK = 128, int STAGES = 20, int SEQ_UNROLL = 10>
-__launch_bounds__(256, 1) __global__ void rg_lru_kernel(lruParams params)
-{
-    T* output = reinterpret_cast<T*>(params.out_ptr);
-    float* state = reinterpret_cast<float*>(params.state_ptr);
-    T* x = reinterpret_cast<T*>(params.x_ptr);
-    T* y = reinterpret_cast<T*>(params.y_ptr);
-    T* y_bias = reinterpret_cast<T*>(params.y_bias_ptr);
-    T* A = reinterpret_cast<T*>(params.A_ptr);
-    int num_channels = params.width;
-    int block_size = params.block_size;
-
-    bool enable_fuse_gate = (params.gate_ptr != nullptr);
-    bool enable_gate_bias;
-    T *gate_x, *gate_a, *gate_x_bias, *gate_a_bias;
-    if (enable_fuse_gate)
-    {
-        enable_gate_bias = (params.gate_bias_ptr != nullptr);
-        gate_x = reinterpret_cast<T*>(params.gate_ptr);
-        gate_a = reinterpret_cast<T*>(params.gate_ptr);
-        if (enable_gate_bias)
-        {
-            gate_x_bias = reinterpret_cast<T*>(params.gate_bias_ptr);
-            gate_a_bias = reinterpret_cast<T*>(params.gate_bias_ptr);
-        }
-    }
-    else
-    {
-        enable_gate_bias = (params.gate_x_bias_ptr != nullptr);
-        gate_x = reinterpret_cast<T*>(params.gate_x_ptr);
-        gate_a = reinterpret_cast<T*>(params.gate_a_ptr);
-        if (enable_gate_bias)
-        {
-            gate_x_bias = reinterpret_cast<T*>(params.gate_x_bias_ptr);
-            gate_a_bias = reinterpret_cast<T*>(params.gate_a_bias_ptr);
-        }
-    }
-
-    __shared__ cuda::pipeline_shared_state<cuda::thread_scope::thread_scope_block, STAGES / SEQ_UNROLL> pipeline_state;
-    auto block = cooperative_groups::this_thread_block();
-
-    __shared__ __align__(128) T sh_gx[STAGES][CHANNELS_PER_BLOCK];
-    __shared__ __align__(128) T sh_gate_x_bias[CHANNELS_PER_BLOCK];
-    __shared__ __align__(128) T sh_ga[STAGES][CHANNELS_PER_BLOCK];
-    __shared__ __align__(128) T sh_gate_a_bias[CHANNELS_PER_BLOCK];
-    __shared__ __align__(128) T sh_x[STAGES][CHANNELS_PER_BLOCK];
-    __shared__ __align__(128) T sh_y[STAGES][CHANNELS_PER_BLOCK];
-    __shared__ __align__(128) T sh_y_bias[CHANNELS_PER_BLOCK];
-    __shared__ __align__(128) float sh_a[CHANNELS_PER_BLOCK];
-
-    int const channel = blockIdx.x * blockDim.x + threadIdx.x;
-    int const sample = blockIdx.y; // batch id
-    int const tid = threadIdx.x;
-
-    int const slot_idx = params.slot_mapping_ptr == nullptr ? sample : params.slot_mapping_ptr[sample];
-    int num_tokens;
-    int start_token_idx;
-    if (params.remove_padding)
-    {
-        start_token_idx = sample == 0 ? 0 : params.last_token_ids_ptr[sample - 1];
-        int end_token_idx = params.last_token_ids_ptr[sample];
-        num_tokens = end_token_idx - start_token_idx;
-    }
-    else
-    {
-        start_token_idx = sample * params.max_seqlen;
-        num_tokens = params.last_token_ids_ptr[sample];
-    }
-
-    int const seq_loops = (num_tokens + SEQ_UNROLL - 1) / SEQ_UNROLL;
-    int const block_channel_base = start_token_idx * num_channels + blockIdx.x * blockDim.x;
-    int const gate_num_channels = enable_fuse_gate ? num_channels * 2 : num_channels;
-    int const gate_block_channel_base = start_token_idx * gate_num_channels;
-    int const tid_offset = tid < 64 ? 32 : 64;
-    int const gchannel = sizeof(T) == 4 ? channel : blockIdx.x * blockDim.x + (threadIdx.x - tid_offset) * 4;
-    int const gx_dim_idx = enable_fuse_gate ? gchannel / block_size * block_size * 2 + gchannel % block_size : gchannel;
-    int const ga_dim_idx = enable_fuse_gate ? gx_dim_idx + block_size : gchannel;
-    int const gx_bias_idx = enable_fuse_gate ? channel / block_size * block_size * 2 + channel % block_size : channel;
-    int const ga_bias_idx = enable_fuse_gate ? gx_bias_idx + block_size : channel;
-
-    if (threadIdx.y == 1)
-    {
-        // Data loading warps
-
-        // Bias and param A are independent of token
-        if (y_bias)
-            sh_y_bias[tid] = y_bias[channel];
-        if (enable_gate_bias)
-        {
-            sh_gate_x_bias[tid] = gate_x_bias[gx_bias_idx];
-            sh_gate_a_bias[tid] = gate_a_bias[ga_bias_idx];
-        }
-        float param_a = cuda_cast<float>(A[channel]);
-        sh_a[tid] = param_a <= 20.f ? -8.0f * __logf(1.0f + __expf(param_a)) : -8.0f * param_a;
-
-        cuda::pipeline pipeline = cuda::make_pipeline(block, &pipeline_state, cuda::pipeline_role::producer);
-
-        int stage = 0;
-        for (int si = 0; si < seq_loops; si++)
-        {
-            pipeline.producer_acquire();
-#pragma unroll
-            for (int token_id = si * SEQ_UNROLL; token_id < num_tokens && token_id < (si + 1) * SEQ_UNROLL; token_id++)
-            {
-                int block_channel = block_channel_base + token_id * num_channels;
-                int gate_block_channel = gate_block_channel_base + token_id * gate_num_channels;
-                if (sizeof(T) == 4)
-                {
-                    cuda::memcpy_async(
-                        &sh_gx[stage][tid], &gate_x[gate_block_channel + gx_dim_idx], sizeof(T), pipeline);
-                    cuda::memcpy_async(
-                        &sh_ga[stage][tid], &gate_a[gate_block_channel + ga_dim_idx], sizeof(T), pipeline);
-                    cuda::memcpy_async(&sh_x[stage][tid], &x[block_channel + tid], sizeof(T), pipeline);
-                    if (y)
-                        cuda::memcpy_async(&sh_y[stage][tid], &y[block_channel + tid], sizeof(T), pipeline);
-                }
-                else
-                {
-                    if (tid < 32)
-                    {
-                        float2* block_x = (float2*) &x[block_channel];
-                        cuda::memcpy_async((float2*) &sh_x[stage][tid * 4], &block_x[tid], sizeof(float2), pipeline);
-                    }
-                    else if (tid < 64)
-                    {
-                        int tid_tmp = tid - 32;
-                        float2* block_gx = (float2*) &gate_x[gate_block_channel];
-                        cuda::memcpy_async(
-                            (float2*) &sh_gx[stage][tid_tmp * 4], &block_gx[gx_dim_idx >> 2], sizeof(float2), pipeline);
-                    }
-                    else if (tid < 96)
-                    {
-                        int tid_tmp = tid - 64;
-                        float2* block_ga = (float2*) &gate_a[gate_block_channel];
-                        cuda::memcpy_async(
-                            (float2*) &sh_ga[stage][tid_tmp * 4], &block_ga[ga_dim_idx >> 2], sizeof(float2), pipeline);
-                    }
-                    else if (tid < 128)
-                    {
-                        if (y)
-                        {
-                            int tid_tmp = tid - 96;
-                            float2* block_y = (float2*) &y[block_channel];
-                            cuda::memcpy_async(
-                                (float2*) &sh_y[stage][tid_tmp * 4], &block_y[tid_tmp], sizeof(float2), pipeline);
-                        }
-                    }
-                }
-                stage++;
-                if (stage >= STAGES)
-                    stage = 0;
-            }
-            pipeline.producer_commit();
-        }
-    }
-    else
-    {
-        // Compute warps
-
-        cuda::pipeline pipeline = cuda::make_pipeline(block, &pipeline_state, cuda::pipeline_role::consumer);
-
-        float state_reg = 0.f;
-        int stage = 0;
-
-        for (int si = 0; si < seq_loops; si++)
-        {
-            pipeline.consumer_wait();
-#pragma unroll
-            for (int token_id = si * SEQ_UNROLL; token_id < num_tokens && token_id < (si + 1) * SEQ_UNROLL; token_id++)
-            {
-                // Read y
-                float y_reg;
-                if (y_bias)
-                {
-                    y_reg = cuda_cast<float>(sh_y[stage][tid] + sh_y_bias[tid]);
-                    // GELU
-                    float k0 = float(0.7978845608028654);
-                    float k1 = float(0.044715);
-                    float y_tanh = k0 * y_reg * (1.0 + k1 * y_reg * y_reg);
-                    float exp_val = -1.f * cuda_abs(y_tanh * 2);
-                    y_reg = 0.5f * y_reg
-                        * (1.f + copysignf_pos(__fdividef((1.f - __expf(exp_val)), (1.f + __expf(exp_val))), y_tanh));
-                }
-                else if (y)
-                {
-                    y_reg = cuda_cast<float>(sh_y[stage][tid]);
-                }
-                else
-                {
-                    y_reg = 1.f;
-                }
-                // Read gate_x
-                float gate_x_reg, gate_a_reg;
-                if (enable_gate_bias)
-                {
-                    gate_x_reg = cuda_cast<float>(-sh_gx[stage][tid] - sh_gate_x_bias[tid]);
-                    gate_a_reg = cuda_cast<float>(-sh_ga[stage][tid] - sh_gate_a_bias[tid]);
-                }
-                else
-                {
-                    gate_x_reg = cuda_cast<float>(-sh_gx[stage][tid]);
-                    gate_a_reg = cuda_cast<float>(-sh_ga[stage][tid]);
-                }
-                // Get gated inputs
-                float x_reg = cuda_cast<float>(sh_x[stage][tid]);
-                float sigmoid_x = __fdividef(1.0f, (1.0f + __expf(gate_x_reg)));
-                float sigmoid_a = __fdividef(1.0f, (1.0f + __expf(gate_a_reg)));
-                float log_a = sigmoid_a * sh_a[tid];
-                float a = __expf(log_a);
-                float a_square = __expf(2.0 * log_a);
-                float outf = y_reg;
-                float normalized_x = x_reg * sigmoid_x;
-                if (si != 0 || token_id != 0)
-                    normalized_x *= sqrtf(1 - a_square);
-
-                // RNN scan
-                state_reg = a * state_reg + normalized_x;
-                outf *= state_reg;
-
-                // Write output
-                T* out = &output[start_token_idx * num_channels + token_id * num_channels];
-                out[channel] = cuda_cast<T>(outf);
-
-                stage++;
-                if (stage >= STAGES)
-                    stage = 0;
-            }
-            pipeline.consumer_release();
-        }
-        // Write the new state back out to the cache
-        state[slot_idx * num_channels + channel] = state_reg;
-    }
-}
-
-template <typename T>
-void invokeRGLRU(lruParams& params, cudaStream_t stream)
-{
-    int samples = params.batch;
-    int channels = params.width;
-
-    int const threads = 128;
-    int const blocks = (channels + threads - 1) / threads;
-    dim3 block(threads, 2);
-    dim3 grid(blocks, samples);
-    TLLM_CHECK((channels % block.x) == 0);
-    TLLM_CHECK(!(params.block_size % 4 != 0 && sizeof(T) == 2));
-
-    rg_lru_kernel<T><<<grid, block, 0, stream>>>(params);
-}
-
-#define INSTANTIATE_RGLRU_DATA_TYPE(T) template void invokeRGLRU<T>(lruParams & params, cudaStream_t stream);
-
-INSTANTIATE_RGLRU_DATA_TYPE(float);
-INSTANTIATE_RGLRU_DATA_TYPE(half);
-#ifdef ENABLE_BF16
-INSTANTIATE_RGLRU_DATA_TYPE(__nv_bfloat16);
-#endif
-#undef INSTANTIATE_RGLRU_DATA_TYPE
-
-////////////////////////////////////////////////////////////////////////////////////////////////////
-
-template <typename T>
-__launch_bounds__(128, 2) __global__ void rg_lru_update_kernel(lruParams params)
-{
-    T* output = reinterpret_cast<T*>(params.out_ptr);
-    float* state = reinterpret_cast<float*>(params.state_ptr);
-    T* x = reinterpret_cast<T*>(params.x_ptr);
-    T* y = reinterpret_cast<T*>(params.y_ptr);
-    T* y_bias = reinterpret_cast<T*>(params.y_bias_ptr);
-    T* A = reinterpret_cast<T*>(params.A_ptr);
-    int num_channels = params.width;
-    int block_size = params.block_size;
-
-    bool enable_fuse_gate = (params.gate_ptr != nullptr);
-    bool enable_gate_bias;
-    T *gate_x, *gate_a, *gate_x_bias, *gate_a_bias;
-    if (enable_fuse_gate)
-    {
-        enable_gate_bias = (params.gate_bias_ptr != nullptr);
-        gate_x = reinterpret_cast<T*>(params.gate_ptr);
-        gate_a = reinterpret_cast<T*>(params.gate_ptr);
-        if (enable_gate_bias)
-        {
-            gate_x_bias = reinterpret_cast<T*>(params.gate_bias_ptr);
-            gate_a_bias = reinterpret_cast<T*>(params.gate_bias_ptr);
-        }
-    }
-    else
-    {
-        enable_gate_bias = (params.gate_x_bias_ptr != nullptr);
-        gate_x = reinterpret_cast<T*>(params.gate_x_ptr);
-        gate_a = reinterpret_cast<T*>(params.gate_a_ptr);
-        if (enable_gate_bias)
-        {
-            gate_x_bias = reinterpret_cast<T*>(params.gate_x_bias_ptr);
-            gate_a_bias = reinterpret_cast<T*>(params.gate_a_bias_ptr);
-        }
-    }
-
-    int const channel = blockIdx.x * blockDim.x + threadIdx.x;
-    if (channel >= num_channels)
-        return;
-    int const sample = blockIdx.y; // batch id
-    int const slot_idx = params.slot_mapping_ptr == nullptr ? sample : params.slot_mapping_ptr[sample];
-    int const idx = sample * num_channels + channel;
-    int const gate_num_channels = enable_fuse_gate ? num_channels * 2 : num_channels;
-    int const gate_base_idx = sample * gate_num_channels;
-    int const gx_dim_idx = enable_fuse_gate ? channel / block_size * block_size * 2 + channel % block_size : channel;
-    int const ga_dim_idx = enable_fuse_gate ? gx_dim_idx + block_size : channel;
-
-    float state_reg = state[slot_idx * num_channels + channel];
-
-    // Read a
-    float param_a = cuda_cast<float>(A[channel]);
-    float c = param_a <= 20.f ? -8.0f * __logf(1.0f + __expf(param_a)) : -8.0f * param_a;
-
-    // Read y
-    float y_reg;
-    if (y_bias)
-    {
-        y_reg = cuda_cast<float>(y[idx] + y_bias[channel]);
-        // GELU
-        float k0 = float(0.7978845608028654);
-        float k1 = float(0.044715);
-        float y_tanh = k0 * y_reg * (1.0 + k1 * y_reg * y_reg);
-        float exp_val = -1.f * cuda_abs(y_tanh * 2);
-        y_reg = 0.5f * y_reg
-            * (1.f + copysignf_pos(__fdividef((1.f - __expf(exp_val)), (1.f + __expf(exp_val))), y_tanh));
-    }
-    else if (y)
-    {
-        y_reg = cuda_cast<float>(y[idx]);
-    }
-    else
-    {
-        y_reg = 1.f;
-    }
-    // Read gate_x
-    float gate_x_reg, gate_a_reg;
-    if (enable_gate_bias)
-    {
-        gate_x_reg = cuda_cast<float>(-gate_x[gate_base_idx + gx_dim_idx] - gate_x_bias[gx_dim_idx]);
-        gate_a_reg = cuda_cast<float>(-gate_a[gate_base_idx + ga_dim_idx] - gate_a_bias[ga_dim_idx]);
-    }
-    else
-    {
-        gate_x_reg = cuda_cast<float>(-gate_x[gate_base_idx + gx_dim_idx]);
-        gate_a_reg = cuda_cast<float>(-gate_a[gate_base_idx + ga_dim_idx]);
-    }
-    // Get gated inputs
-    float sigmoid_x = __fdividef(1.0f, (1.0f + __expf(gate_x_reg)));
-    float sigmoid_a = __fdividef(1.0f, (1.0f + __expf(gate_a_reg)));
-    float log_a = sigmoid_a * c;
-    float a = __expf(log_a);
-    float a_square = __expf(2.0 * log_a);
-    float outf = y_reg;
-    float normalized_x = cuda_cast<float>(x[idx]) * sigmoid_x * sqrtf(1 - a_square);
-
-    // RNN update
-    state_reg = a * state_reg + normalized_x;
-    outf *= state_reg;
-
-    // Write output and state
-    output[sample * num_channels + channel] = cuda_cast<T>(outf);
-    state[slot_idx * num_channels + channel] = state_reg;
-}
-
-template <typename T>
-void invokeRGLRUUpdate(lruParams& params, cudaStream_t stream)
-{
-    int samples = params.batch;
-    int channels = params.width;
-
-    int const threads = 128;
-    int const blocks = (channels + threads - 1) / threads;
-    dim3 block(threads, 1);
-    dim3 grid(blocks, samples);
-
-    rg_lru_update_kernel<T><<<grid, block, 0, stream>>>(params);
-}
-
-#define INSTANTIATE_RGLRU_UPDATE_DATA_TYPE(T)                                                                          \
-    template void invokeRGLRUUpdate<T>(lruParams & params, cudaStream_t stream)
-
-INSTANTIATE_RGLRU_UPDATE_DATA_TYPE(float);
-INSTANTIATE_RGLRU_UPDATE_DATA_TYPE(half);
-#ifdef ENABLE_BF16
-INSTANTIATE_RGLRU_UPDATE_DATA_TYPE(__nv_bfloat16);
-#endif
-#undef INSTANTIATE_RGLRU_UPDATE_DATA_TYPE
-
-} // namespace kernels
-} // namespace tensorrt_llm
diff --git a/cpp/tensorrt_llm/kernels/lruKernel.h b/cpp/tensorrt_llm/kernels/lruKernel.h
index c49f039d..87834ba7 100644
--- a/cpp/tensorrt_llm/kernels/lruKernel.h
+++ b/cpp/tensorrt_llm/kernels/lruKernel.h
@@ -49,11 +49,11 @@ struct lruParams
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
-template <typename T>
-void invokeRGLRU(lruParams& params, cudaStream_t stream);
+// template <typename T>
+// void invokeRGLRU(lruParams& params, cudaStream_t stream);
 
-template <typename T>
-void invokeRGLRUUpdate(lruParams& params, cudaStream_t stream);
+// template <typename T>
+// void invokeRGLRUUpdate(lruParams& params, cudaStream_t stream);
 
 } // namespace kernels
 } // namespace tensorrt_llm
diff --git a/cpp/tensorrt_llm/kernels/mixtureOfExperts/moe_kernels.cu b/cpp/tensorrt_llm/kernels/mixtureOfExperts/moe_kernels.cu
index bfde5bae..14c32412 100644
--- a/cpp/tensorrt_llm/kernels/mixtureOfExperts/moe_kernels.cu
+++ b/cpp/tensorrt_llm/kernels/mixtureOfExperts/moe_kernels.cu
@@ -49,7 +49,7 @@
 
 #ifndef CUDART_VERSION
 #error CUDART_VERSION Undefined!
-#elif (CUDART_VERSION >= 11050)
+#elif (CUDART_VERSION >= 11040)
 #include <cub/cub.cuh>
 #include <cub/device/device_radix_sort.cuh>
 #include <cub/util_type.cuh>
@@ -2026,9 +2026,12 @@ void CutlassMoeFCRunner<T, WeightType, OutputType, ScaleBiasType, Enable>::runMo
 {
     static constexpr bool int_scales_required
         = std::is_same<WeightType, uint8_t>::value || std::is_same<WeightType, cutlass::uint4b_t>::value;
+#ifdef ENABLE_FP8
     static constexpr bool fp8_scales_required
         = std::is_same<WeightType, __nv_fp8_e4m3>::value || std::is_same<WeightType, __nv_fp8_e5m2>::value;
-
+#else
+    static constexpr bool fp8_scales_required = false;
+#endif
     auto const* input_activations = static_cast<T const*>(input_activations_void);
     auto const* fc1_expert_weights = static_cast<WeightType const*>(fc1_expert_weights_void);
     auto const* fc1_expert_biases = reinterpret_cast<ScaleBiasType const*>(fc1_expert_biases_void);
@@ -2374,10 +2377,13 @@ std::vector<size_t> GemmProfilerBackend::getProfilerWorkspaces(int maxM, bool is
     float weight_bytes
         = mWType == nvinfer1::DataType::kINT4 ? 0.5f : static_cast<float>(tensorrt_llm::common::getDTypeSize(mWType));
     size_t output_bytes = tensorrt_llm::common::getDTypeSize(mOType);
+#ifdef ENABLE_FP8
     size_t gemm_output_bytes = (mOType == nvinfer1::DataType::kFP8)
         ? sizeof(HopperGroupedGemmInput::OutputTypeAdaptor_t<__nv_fp8_e4m3>)
         : output_bytes;
-
+#else
+    size_t gemm_output_bytes = output_bytes;
+#endif
     size_t hidden_size = mExpertHiddenSize;
     size_t inter_size = mExpertInterSize; // Already divided by TP
     size_t num_experts_per_node = mNumExpertsPerNode;
diff --git a/cpp/tensorrt_llm/kernels/mixtureOfExperts/moe_kernels.h b/cpp/tensorrt_llm/kernels/mixtureOfExperts/moe_kernels.h
index 964e0452..2db2f53b 100644
--- a/cpp/tensorrt_llm/kernels/mixtureOfExperts/moe_kernels.h
+++ b/cpp/tensorrt_llm/kernels/mixtureOfExperts/moe_kernels.h
@@ -264,7 +264,7 @@ class CutlassMoeFCRunner : public CutlassMoeFCRunnerInterface
     // This should leave the variable unchanged in any currently supported configuration
     using UnfusedGemmOutputType = typename HopperGroupedGemmInput::OutputTypeAdaptor_t<OutputType>;
 
-    static_assert(!std::is_same_v<OutputType, __nv_fp8_e4m3>, "Current logic requires output type to be non-FP8");
+    // static_assert(!std::is_same_v<OutputType, __nv_fp8_e4m3>, "Current logic requires output type to be non-FP8");
     // We introduce this as a separate parameter, so that if we ever remove the above condition we can decouple
     // ScaleBiasType and OutputType easily. For now these are required to be equivalent
     static_assert(std::is_same_v<OutputType, ScaleBiasType>, "Scale and bias types must match OutputType");
diff --git a/cpp/tensorrt_llm/kernels/multiHeadAttentionCommon.h b/cpp/tensorrt_llm/kernels/multiHeadAttentionCommon.h
index 1893407c..be5828d9 100644
--- a/cpp/tensorrt_llm/kernels/multiHeadAttentionCommon.h
+++ b/cpp/tensorrt_llm/kernels/multiHeadAttentionCommon.h
@@ -68,6 +68,7 @@ constexpr int32_t kSM_72 = 72;
 constexpr int32_t kSM_75 = 75;
 constexpr int32_t kSM_80 = 80;
 constexpr int32_t kSM_86 = 86;
+constexpr int32_t kSM_87 = 87;
 constexpr int32_t kSM_89 = 89;
 constexpr int32_t kSM_90 = 90;
 
diff --git a/cpp/tensorrt_llm/kernels/preQuantScaleKernel.h b/cpp/tensorrt_llm/kernels/preQuantScaleKernel.h
index 5ac02cff..bc22b0af 100644
--- a/cpp/tensorrt_llm/kernels/preQuantScaleKernel.h
+++ b/cpp/tensorrt_llm/kernels/preQuantScaleKernel.h
@@ -17,7 +17,9 @@
 #pragma once
 
 #include <cuda_fp16.h>
+#if defined(ENABLE_FP8)
 #include <cuda_fp8.h>
+#endif
 #include <cuda_runtime.h>
 #include <cuda_runtime_api.h>
 
diff --git a/cpp/tensorrt_llm/kernels/samplingAirTopPKernels.cu b/cpp/tensorrt_llm/kernels/samplingAirTopPKernels.cu
index 21891c35..1a5c8929 100644
--- a/cpp/tensorrt_llm/kernels/samplingAirTopPKernels.cu
+++ b/cpp/tensorrt_llm/kernels/samplingAirTopPKernels.cu
@@ -16,7 +16,7 @@
 
 #ifndef CUDART_VERSION
 #error CUDART_VERSION Undefined!
-#elif (CUDART_VERSION >= 11050)
+#elif (CUDART_VERSION >= 11040)
 #include <cub/cub.cuh>
 #else
 #include "3rdparty/cub/cub.cuh"
@@ -788,7 +788,7 @@ __device__ void lastFilter(T const* inBuf, IdxT const* inIdxBuf, IdxT currentLen
             __threadfence_block();
             __syncthreads();
 
-            cuda::atomic_ref<IdxT, cuda::thread_scope_block> refLast(lastIdxBuf[neededNumOfKth - 1]);
+            cuda::atomic<IdxT, cuda::thread_scope_block> refLast(lastIdxBuf[neededNumOfKth - 1]);
 
             for (IdxT i = threadIdx.x; i < currentLen; i += blockDim.x)
             {
@@ -1125,20 +1125,21 @@ __global__ void airTopPSampling(Counter<T, IdxT, AccT>* counters, HisT* histogra
             __syncthreads();
         }
 
+        // TODO: check when will this kernel be used
         // Acquire the summation of each 32 buckets
         for (int i = threadIdx.x; i < numBuckets; i += BlockSize)
         {
-            reduce_store_async(warp, warpSum + i / WARP_SIZE, histPtr[i], cg::plus<float>{});
+            // reduce_store_async(warp, warpSum + i / WARP_SIZE, histPtr[i], cg::plus<float>{});
         }
         __syncthreads();
 
         // Acquire the summation of all the 2048 buckets
         if (threadIdx.x < WARP_SIZE)
         {
-            reduce_store_async(warp, blockSum, warpSum[threadIdx.x], cg::plus<float>{});
+            // reduce_store_async(warp, blockSum, warpSum[threadIdx.x], cg::plus<float>{});
             if constexpr (BitsPerPass == 11)
             {
-                reduce_update_async(warp, blockSum, warpSum[threadIdx.x + WARP_SIZE], cg::plus<float>{});
+                // reduce_update_async(warp, blockSum, warpSum[threadIdx.x + WARP_SIZE], cg::plus<float>{});
             }
         }
         __syncthreads();
diff --git a/cpp/tensorrt_llm/kernels/samplingTopKKernels.cu b/cpp/tensorrt_llm/kernels/samplingTopKKernels.cu
index 04edc841..6b37e2d5 100644
--- a/cpp/tensorrt_llm/kernels/samplingTopKKernels.cu
+++ b/cpp/tensorrt_llm/kernels/samplingTopKKernels.cu
@@ -17,7 +17,7 @@
 
 #ifndef CUDART_VERSION
 #error CUDART_VERSION Undefined!
-#elif (CUDART_VERSION >= 11050)
+#elif (CUDART_VERSION >= 11040)
 #include <cub/cub.cuh>
 #else
 #include "3rdparty/cub/cub.cuh"
diff --git a/cpp/tensorrt_llm/kernels/samplingTopPKernels.cu b/cpp/tensorrt_llm/kernels/samplingTopPKernels.cu
index 13da77bd..81fca539 100644
--- a/cpp/tensorrt_llm/kernels/samplingTopPKernels.cu
+++ b/cpp/tensorrt_llm/kernels/samplingTopPKernels.cu
@@ -15,7 +15,7 @@
  */
 #ifndef CUDART_VERSION
 #error CUDART_VERSION Undefined!
-#elif (CUDART_VERSION >= 11050)
+#elif (CUDART_VERSION >= 11040)
 #include <cub/cub.cuh>
 #else
 #include "3rdparty/cub/cub.cuh"
diff --git a/cpp/tensorrt_llm/kernels/selectiveScan.cu b/cpp/tensorrt_llm/kernels/selectiveScan.cu
index 064b10cc..fb90d240 100644
--- a/cpp/tensorrt_llm/kernels/selectiveScan.cu
+++ b/cpp/tensorrt_llm/kernels/selectiveScan.cu
@@ -28,13 +28,14 @@
 #endif
 
 #include "selectiveScan.h"
-
+#if 0
 #include "selectiveScan/CudaType.h"
 #include "selectiveScan/bmmchunk.h"
 #include "selectiveScan/chunkcumsum.h"
 #include "selectiveScan/chunkscan.h"
 #include "selectiveScan/chunkstate.h"
 #include "selectiveScan/statepassing.h"
+#endif
 
 namespace tensorrt_llm
 {
@@ -340,6 +341,7 @@ void invokeSelectiveScan(SSMParamsBase& params, cudaStream_t stream)
 template <typename input_t, typename weight_t>
 void invokeChunkScan(SSMParamsBase& params, cudaStream_t stream, tensorrt_llm::common::CUDADriverWrapper* driver)
 {
+#if 0
     int B = params.batch;
     int L = params.max_seqlen;
     int H = params.nheads;
@@ -436,6 +438,7 @@ void invokeChunkScan(SSMParamsBase& params, cudaStream_t stream, tensorrt_llm::c
     cudaFuncSetAttribute(chunk_scan, cudaFuncAttributeMaxDynamicSharedMemorySize, shms[4]);
     chunk_scan<<<bds[4], tds[4], shms[4], stream>>>(
         B, L, H, P, G, N, mxY, mxOs, mxdc, mxdA, mxCB, mxD, (useTmas[4] ? &descs[4] : mxXBC), mxZ, rp, ltip);
+#endif
 }
 
 #define INSTANTIATE_SELECTIVE_SCAN_DATA_TYPE(input_t, weight_t)                                                        \
diff --git a/cpp/tensorrt_llm/kernels/selectiveScan/CudaType.h b/cpp/tensorrt_llm/kernels/selectiveScan/CudaType.h
index 66f8182e..d6264bb4 100644
--- a/cpp/tensorrt_llm/kernels/selectiveScan/CudaType.h
+++ b/cpp/tensorrt_llm/kernels/selectiveScan/CudaType.h
@@ -15,16 +15,17 @@
  */
 
 #pragma once
-
+#if defined(ENABLE_FP8)
 #include <cuda_fp8.h>
+typedef __nv_fp8_e4m3 e4m3_t;
+typedef __nv_fp8_e5m2 e5m2_t;
+#endif
 #include <mma.h>
 
 typedef __nv_half fp16_t;
 typedef __nv_bfloat16 bf16_t;
 typedef float fp32_t;
 typedef double fp64_t;
-typedef __nv_fp8_e4m3 e4m3_t;
-typedef __nv_fp8_e5m2 e5m2_t;
 
 enum CudaType
 {
@@ -32,8 +33,10 @@ enum CudaType
     CT_BF16,
     CT_FP32,
     CT_FP64,
+#ifdef ENABLE_FP8    
     CT_E4M3,
     CT_E5M2,
+#endif
 };
 
 template <CudaType>
@@ -62,7 +65,7 @@ struct EnToTp<CT_FP64>
 {
     typedef fp64_t type;
 };
-
+#ifdef ENABLE_FP8
 template <>
 struct EnToTp<CT_E4M3>
 {
@@ -74,6 +77,7 @@ struct EnToTp<CT_E5M2>
 {
     typedef e5m2_t type;
 };
+#endif
 
 template <CudaType en_>
 using EnToTp_t = typename EnToTp<en_>::type;
@@ -105,6 +109,7 @@ struct TpToEn<fp64_t>
     static constexpr CudaType value = CT_FP64;
 };
 
+#ifdef ENABLE_FP8
 template <>
 struct TpToEn<e4m3_t>
 {
@@ -116,6 +121,7 @@ struct TpToEn<e5m2_t>
 {
     static constexpr CudaType value = CT_E5M2;
 };
+#endif
 
 template <class Tp_>
 constexpr CudaType TpToEn_v = TpToEn<Tp_>::value;
diff --git a/cpp/tensorrt_llm/kernels/selectiveScan/bmmchunk.h b/cpp/tensorrt_llm/kernels/selectiveScan/bmmchunk.h
index e1a62cdd..2301a76e 100644
--- a/cpp/tensorrt_llm/kernels/selectiveScan/bmmchunk.h
+++ b/cpp/tensorrt_llm/kernels/selectiveScan/bmmchunk.h
@@ -17,7 +17,9 @@
 #pragma once
 
 #include <cuda.h>
+#if defined(ENABLE_FP8)
 #include <cuda_fp8.h>
+#endif
 #include <mma.h>
 
 #include "tensorrt_llm/common/cudaBf16Fallbacks.cuh"
diff --git a/cpp/tensorrt_llm/kernels/selectiveScan/chunkcumsum.h b/cpp/tensorrt_llm/kernels/selectiveScan/chunkcumsum.h
index ac4db9d9..16aafc90 100644
--- a/cpp/tensorrt_llm/kernels/selectiveScan/chunkcumsum.h
+++ b/cpp/tensorrt_llm/kernels/selectiveScan/chunkcumsum.h
@@ -15,8 +15,9 @@
  */
 
 #pragma once
-
+#if defined(ENABLE_FP8)
 #include <cuda_fp8.h>
+#endif
 #include <mma.h>
 
 #include "tensorrt_llm/common/cudaBf16Fallbacks.cuh"
diff --git a/cpp/tensorrt_llm/kernels/selectiveScan/chunkscan.h b/cpp/tensorrt_llm/kernels/selectiveScan/chunkscan.h
index 28cd7fde..0205be20 100644
--- a/cpp/tensorrt_llm/kernels/selectiveScan/chunkscan.h
+++ b/cpp/tensorrt_llm/kernels/selectiveScan/chunkscan.h
@@ -17,7 +17,9 @@
 #pragma once
 
 #include <cuda.h>
+#if defined(ENABLE_FP8)
 #include <cuda_fp8.h>
+#endif
 #include <mma.h>
 
 #include "tensorrt_llm/common/cudaBf16Fallbacks.cuh"
diff --git a/cpp/tensorrt_llm/kernels/selectiveScan/chunkstate.h b/cpp/tensorrt_llm/kernels/selectiveScan/chunkstate.h
index 030f99a2..1a9a6d47 100644
--- a/cpp/tensorrt_llm/kernels/selectiveScan/chunkstate.h
+++ b/cpp/tensorrt_llm/kernels/selectiveScan/chunkstate.h
@@ -17,7 +17,9 @@
 #pragma once
 
 #include <cuda.h>
+#if defined(ENABLE_FP8)
 #include <cuda_fp8.h>
+#endif
 #include <mma.h>
 
 #include "tensorrt_llm/common/cudaBf16Fallbacks.cuh"
diff --git a/cpp/tensorrt_llm/kernels/selectiveScan/statepassing.h b/cpp/tensorrt_llm/kernels/selectiveScan/statepassing.h
index 45cfced4..c55ec905 100644
--- a/cpp/tensorrt_llm/kernels/selectiveScan/statepassing.h
+++ b/cpp/tensorrt_llm/kernels/selectiveScan/statepassing.h
@@ -15,8 +15,9 @@
  */
 
 #pragma once
-
+#if defined(ENABLE_FP8)
 #include <cuda_fp8.h>
+#endif
 #include <mma.h>
 
 #include "tensorrt_llm/common/cudaBf16Fallbacks.cuh"
diff --git a/cpp/tensorrt_llm/kernels/speculativeDecoding/common.cu b/cpp/tensorrt_llm/kernels/speculativeDecoding/common.cu
index b76b94ae..2ab348f8 100644
--- a/cpp/tensorrt_llm/kernels/speculativeDecoding/common.cu
+++ b/cpp/tensorrt_llm/kernels/speculativeDecoding/common.cu
@@ -22,7 +22,7 @@
 #include "tensorrt_llm/kernels/speculativeDecoding/common.h"
 #ifndef CUDART_VERSION
 #error CUDART_VERSION Undefined!
-#elif (CUDART_VERSION >= 11050)
+#elif (CUDART_VERSION >= 11040)
 #include <cub/cub.cuh>
 #else
 #include "3rdparty/cub/cub.cuh"
diff --git a/cpp/tensorrt_llm/kernels/speculativeDecoding/explicitDraftTokensKernels.cu b/cpp/tensorrt_llm/kernels/speculativeDecoding/explicitDraftTokensKernels.cu
index b04754db..eab84fe9 100644
--- a/cpp/tensorrt_llm/kernels/speculativeDecoding/explicitDraftTokensKernels.cu
+++ b/cpp/tensorrt_llm/kernels/speculativeDecoding/explicitDraftTokensKernels.cu
@@ -18,7 +18,7 @@
 #include "tensorrt_llm/kernels/speculativeDecoding/explicitDraftTokensKernels.h"
 #ifndef CUDART_VERSION
 #error CUDART_VERSION Undefined!
-#elif (CUDART_VERSION >= 11050)
+#elif (CUDART_VERSION >= 11040)
 #include <cub/cub.cuh>
 #else
 #include "3rdparty/cub/cub.cuh"
diff --git a/cpp/tensorrt_llm/kernels/speculativeDecoding/externalDraftTokensKernels.cu b/cpp/tensorrt_llm/kernels/speculativeDecoding/externalDraftTokensKernels.cu
index 6036695c..607ffd19 100644
--- a/cpp/tensorrt_llm/kernels/speculativeDecoding/externalDraftTokensKernels.cu
+++ b/cpp/tensorrt_llm/kernels/speculativeDecoding/externalDraftTokensKernels.cu
@@ -22,7 +22,7 @@
 #include "tensorrt_llm/kernels/speculativeDecoding/externalDraftTokensKernels.h"
 #ifndef CUDART_VERSION
 #error CUDART_VERSION Undefined!
-#elif (CUDART_VERSION >= 11050)
+#elif (CUDART_VERSION >= 11040)
 #include <cub/cub.cuh>
 #else
 #include "3rdparty/cub/cub.cuh"
diff --git a/cpp/tensorrt_llm/kernels/speculativeDecoding/medusaDecodingKernels.cu b/cpp/tensorrt_llm/kernels/speculativeDecoding/medusaDecodingKernels.cu
index 7a6d8540..a1fd7379 100644
--- a/cpp/tensorrt_llm/kernels/speculativeDecoding/medusaDecodingKernels.cu
+++ b/cpp/tensorrt_llm/kernels/speculativeDecoding/medusaDecodingKernels.cu
@@ -22,7 +22,7 @@
 #include "tensorrt_llm/kernels/speculativeDecoding/medusaDecodingKernels.h"
 #ifndef CUDART_VERSION
 #error CUDART_VERSION Undefined!
-#elif (CUDART_VERSION >= 11050)
+#elif (CUDART_VERSION >= 11040)
 #include <cub/cub.cuh>
 #else
 #include "3rdparty/cub/cub.cuh"
diff --git a/cpp/tensorrt_llm/kernels/unfusedAttentionKernels.cu b/cpp/tensorrt_llm/kernels/unfusedAttentionKernels.cu
index b12a2f07..8e415169 100644
--- a/cpp/tensorrt_llm/kernels/unfusedAttentionKernels.cu
+++ b/cpp/tensorrt_llm/kernels/unfusedAttentionKernels.cu
@@ -1836,7 +1836,7 @@ __global__ void shiftKCache(KVCacheBuffer kvCacheBuffer, KVLinearBuffer shiftKCa
     // Use 8bit cache.
     static constexpr bool ENABLE_8BITS_CACHE = sizeof(T_cache) == 1;
     // FP8 KV Cache.
-    [[maybe_unused]] static constexpr bool FP8_K_CACHE = std::is_same<T_cache, __nv_fp8_e4m3>::value;
+    [[maybe_unused]] static constexpr bool FP8_K_CACHE = false; // std::is_same<T_cache, __nv_fp8_e4m3>::value;
     // INT8 KV Cache.
     static constexpr bool INT8_K_CACHE = std::is_same<T_cache, int8_t>::value;
 
@@ -2132,8 +2132,11 @@ void invokeConversion(Dst* dst, Src const* src, int64_t size, float const* __res
 #define INSTANTIATE_invokeConversion(Dst, Src)                                                                         \
     template void invokeConversion<Dst, Src>(                                                                          \
         Dst * dst, Src const* src, int64_t size, float const* __restrict__ scale, cudaStream_t stream)
+#ifdef ENABLE_FP8
 INSTANTIATE_invokeConversion(__nv_fp8_e4m3, half);
 INSTANTIATE_invokeConversion(__nv_fp8_e4m3, __nv_bfloat16);
+#endif
+
 #undef INSTANTIATE_invokeConversion
 
 } // namespace kernels
diff --git a/cpp/tensorrt_llm/kernels/unfusedAttentionKernels/unfusedAttentionKernels_2_bf16_fp8.cu b/cpp/tensorrt_llm/kernels/unfusedAttentionKernels/unfusedAttentionKernels_2_bf16_fp8.cu
index 14645db9..35951430 100644
--- a/cpp/tensorrt_llm/kernels/unfusedAttentionKernels/unfusedAttentionKernels_2_bf16_fp8.cu
+++ b/cpp/tensorrt_llm/kernels/unfusedAttentionKernels/unfusedAttentionKernels_2_bf16_fp8.cu
@@ -22,7 +22,7 @@ namespace tensorrt_llm
 namespace kernels
 {
 
-#ifdef ENABLE_BF16
+#if defined(ENABLE_BF16) && defined(ENABLE_FP8)
 INSTANTIATE_ADDFUSEDQKVBIAS_TRANSPOSE(__nv_bfloat16, __nv_fp8_e4m3, KVBlockArray);
 INSTANTIATE_ADDFUSEDQKVBIAS_TRANSPOSE(__nv_bfloat16, __nv_fp8_e4m3, KVLinearBuffer);
 #endif
diff --git a/cpp/tensorrt_llm/kernels/unfusedAttentionKernels/unfusedAttentionKernels_2_float_fp8.cu b/cpp/tensorrt_llm/kernels/unfusedAttentionKernels/unfusedAttentionKernels_2_float_fp8.cu
index 8d48ecae..3a33fb0d 100644
--- a/cpp/tensorrt_llm/kernels/unfusedAttentionKernels/unfusedAttentionKernels_2_float_fp8.cu
+++ b/cpp/tensorrt_llm/kernels/unfusedAttentionKernels/unfusedAttentionKernels_2_float_fp8.cu
@@ -21,9 +21,9 @@ namespace tensorrt_llm
 {
 namespace kernels
 {
-
+#if defined(ENABLE_FP8)
 INSTANTIATE_ADDFUSEDQKVBIAS_TRANSPOSE(float, __nv_fp8_e4m3, KVBlockArray);
 INSTANTIATE_ADDFUSEDQKVBIAS_TRANSPOSE(float, __nv_fp8_e4m3, KVLinearBuffer);
-
+#endif
 } // namespace kernels
 } // namespace tensorrt_llm
diff --git a/cpp/tensorrt_llm/kernels/unfusedAttentionKernels/unfusedAttentionKernels_2_half_fp8.cu b/cpp/tensorrt_llm/kernels/unfusedAttentionKernels/unfusedAttentionKernels_2_half_fp8.cu
index b1fe8d13..f43e7be5 100644
--- a/cpp/tensorrt_llm/kernels/unfusedAttentionKernels/unfusedAttentionKernels_2_half_fp8.cu
+++ b/cpp/tensorrt_llm/kernels/unfusedAttentionKernels/unfusedAttentionKernels_2_half_fp8.cu
@@ -21,9 +21,9 @@ namespace tensorrt_llm
 {
 namespace kernels
 {
-
+#if defined(ENABLE_FP8)
 INSTANTIATE_ADDFUSEDQKVBIAS_TRANSPOSE(half, __nv_fp8_e4m3, KVBlockArray);
 INSTANTIATE_ADDFUSEDQKVBIAS_TRANSPOSE(half, __nv_fp8_e4m3, KVLinearBuffer);
-
+#endif
 } // namespace kernels
 } // namespace tensorrt_llm
diff --git a/cpp/tensorrt_llm/kernels/unfusedAttentionKernels/unfusedAttentionKernels_2_template.h b/cpp/tensorrt_llm/kernels/unfusedAttentionKernels/unfusedAttentionKernels_2_template.h
index c53510d3..1160f39c 100644
--- a/cpp/tensorrt_llm/kernels/unfusedAttentionKernels/unfusedAttentionKernels_2_template.h
+++ b/cpp/tensorrt_llm/kernels/unfusedAttentionKernels/unfusedAttentionKernels_2_template.h
@@ -51,7 +51,7 @@ struct Rotary_vec_t
     using Type = T;
     using BaseType = T;
     // Quantized output type only supports fp8 currently.
-    using QuantizedType = __nv_fp8_e4m3;
+    using QuantizedType = void; // __nv_fp8_e4m3;
     static constexpr int size = 1;
 };
 
@@ -62,7 +62,7 @@ struct Rotary_vec_t<float, 32>
 {
     using Type = float;
     using BaseType = float;
-    using QuantizedType = __nv_fp8_e4m3;
+    using QuantizedType = void; // __nv_fp8_e4m3;
     static constexpr int size = 1;
 };
 
@@ -71,7 +71,7 @@ struct Rotary_vec_t<float, 64>
 {
     using Type = float2;
     using BaseType = float;
-    using QuantizedType = mmha::fp8_2_t;
+    using QuantizedType = void; // mmha::fp8_2_t;
     static constexpr int size = 2;
 };
 
@@ -80,7 +80,7 @@ struct Rotary_vec_t<float, 128>
 {
     using Type = float4;
     using BaseType = float;
-    using QuantizedType = mmha::fp8_4_t;
+    using QuantizedType = void; // mmha::fp8_4_t;
     static constexpr int size = 4;
 };
 
@@ -89,7 +89,7 @@ struct Rotary_vec_t<float, 256>
 {
     using Type = mmha::Float8_;
     using BaseType = float;
-    using QuantizedType = mmha::fp8_8_t;
+    using QuantizedType = void; // mmha::fp8_8_t;
     static constexpr int size = 8;
 };
 
@@ -100,7 +100,7 @@ struct Rotary_vec_t<half, 32>
 {
     using Type = uint16_t;
     using BaseType = uint16_t;
-    using QuantizedType = __nv_fp8_e4m3;
+    using QuantizedType = void; // __nv_fp8_e4m3;
     static constexpr int size = 2;
 };
 
@@ -109,7 +109,7 @@ struct Rotary_vec_t<half, 64>
 {
     using Type = uint32_t;
     using BaseType = uint16_t;
-    using QuantizedType = mmha::fp8_2_t;
+    using QuantizedType = void; // mmha::fp8_2_t;
     static constexpr int size = 2;
 };
 
@@ -118,7 +118,7 @@ struct Rotary_vec_t<half, 128>
 {
     using Type = uint2;
     using BaseType = uint16_t;
-    using QuantizedType = mmha::fp8_4_t;
+    using QuantizedType = void; // mmha::fp8_4_t;
     static constexpr int size = 4;
 };
 
@@ -127,7 +127,7 @@ struct Rotary_vec_t<half, 256>
 {
     using Type = uint4;
     using BaseType = uint16_t;
-    using QuantizedType = mmha::fp8_8_t;
+    using QuantizedType = void; // mmha::fp8_8_t;
     static constexpr int size = 8;
 };
 
@@ -140,7 +140,7 @@ struct Rotary_vec_t<__nv_bfloat16, 32>
 {
     using Type = __nv_bfloat16;
     using BaseType = __nv_bfloat16;
-    using QuantizedType = __nv_fp8_e4m3;
+    using QuantizedType = void; // __nv_fp8_e4m3;
     static constexpr int size = 1;
 };
 
@@ -149,7 +149,7 @@ struct Rotary_vec_t<__nv_bfloat16, 64>
 {
     using Type = __nv_bfloat162;
     using BaseType = __nv_bfloat16;
-    using QuantizedType = mmha::fp8_2_t;
+    using QuantizedType = void; // mmha::fp8_2_t;
     static constexpr int size = 2;
 };
 
@@ -158,7 +158,7 @@ struct Rotary_vec_t<__nv_bfloat16, 128>
 {
     using Type = mmha::bf16_4_t;
     using BaseType = __nv_bfloat16;
-    using QuantizedType = mmha::fp8_4_t;
+    using QuantizedType = void; // mmha::fp8_4_t;
     static constexpr int size = 4;
 };
 
@@ -167,7 +167,7 @@ struct Rotary_vec_t<__nv_bfloat16, 256>
 {
     using Type = mmha::bf16_8_t;
     using BaseType = __nv_bfloat16;
-    using QuantizedType = mmha::fp8_8_t;
+    using QuantizedType = void; // mmha::fp8_8_t;
     static constexpr int size = 8;
 };
 
@@ -335,8 +335,8 @@ __global__ void applyBiasRopeUpdateKVCache(QKVPreprocessingParams<T, KVCacheBuff
     // The base type will share the rotary coefficient.
     using BaseType = typename Rotary_vec_t<T, Dh_MAX>::BaseType;
     // Quantized output only supports fp8 currently.
-    using QuantizedEltType = __nv_fp8_e4m3;
-    using QuantizedVecType = typename Rotary_vec_t<T, Dh_MAX>::QuantizedType;
+    // using QuantizedEltType = __nv_fp8_e4m3;
+    // using QuantizedVecType = typename Rotary_vec_t<T, Dh_MAX>::QuantizedType;
     // GPTJ rotary embedding: two elements share the same rotary coefficient.
     constexpr int ROTARY_COEF_VEC_SIZE = ROTARY_TYPE == RotaryPositionEmbeddingType::GPTJ ? VEC_SIZE / 2 : VEC_SIZE;
 
@@ -507,9 +507,9 @@ __global__ void applyBiasRopeUpdateKVCache(QKVPreprocessingParams<T, KVCacheBuff
                 VecType k_to_cache = params.position_shift_enabled ? k_wo_pos : k;
 
                 auto const dst_q_idx = static_cast<size_t>(global_token_idx) * params.q_hidden_size + hidden_idx;
-                QuantizedEltType* quantized_q_ptr = STORE_QKV
-                    ? reinterpret_cast<QuantizedEltType*>(params.QuantizedQKV) + src_q_idx
-                    : reinterpret_cast<QuantizedEltType*>(params.Q) + dst_q_idx;
+                // QuantizedEltType* quantized_q_ptr = STORE_QKV
+                //    ? reinterpret_cast<QuantizedEltType*>(params.QuantizedQKV) + src_q_idx
+                //    : reinterpret_cast<QuantizedEltType*>(params.Q) + dst_q_idx;
                 VecType* q_ptr = STORE_QKV ? reinterpret_ptr<T, VecType>(params.QKV, src_q_idx)
                                            : reinterpret_ptr<T, VecType>(params.Q, dst_q_idx);
 
@@ -522,35 +522,35 @@ __global__ void applyBiasRopeUpdateKVCache(QKVPreprocessingParams<T, KVCacheBuff
                         &scaleOrigQuant, params.kvScaleOrigQuant ? params.kvScaleOrigQuant[0] : 1.0f);
                 }
 
-                if constexpr (FP8_OUTPUT)
-                {
+                // if constexpr (FP8_OUTPUT)
+                // {
                     // Quant the vec to fp8 vec with the scale.
-                    mmha::store_8bits_vec(quantized_q_ptr, q, 0, scaleOrigQuant);
-                }
-                else
-                {
+                //    mmha::store_8bits_vec(quantized_q_ptr, q, 0, scaleOrigQuant);
+                //}
+                //else
+                //{
                     *q_ptr = q;
-                }
+                //}
                 if ((params.head_num == params.kv_head_num) || (head_idx == (kv_head_idx * params.qheads_per_kv_head)))
                 {
                     if constexpr (STORE_QKV)
                     {
-                        if constexpr (FP8_OUTPUT)
-                        {
+                        //if constexpr (FP8_OUTPUT)
+                        //{
                             // Quant the vec to fp8 vec with the scale.
-                            mmha::store_8bits_vec(
-                                reinterpret_cast<QuantizedEltType*>(params.QuantizedQKV), k, src_k_idx, scaleOrigQuant);
-                            mmha::store_8bits_vec(
-                                reinterpret_cast<QuantizedEltType*>(params.QuantizedQKV), v, src_v_idx, scaleOrigQuant);
-                        }
-                        else
-                        {
+                        //    mmha::store_8bits_vec(
+                        //        reinterpret_cast<QuantizedEltType*>(params.QuantizedQKV), k, src_k_idx, scaleOrigQuant);
+                        //    mmha::store_8bits_vec(
+                        //        reinterpret_cast<QuantizedEltType*>(params.QuantizedQKV), v, src_v_idx, scaleOrigQuant);
+                        //}
+                        //else
+                        //{
                             *reinterpret_cast<VecType*>(&params.QKV[src_k_idx]) = k;
                             if constexpr (ADD_BIAS)
                             {
                                 *reinterpret_cast<VecType*>(&params.QKV[src_v_idx]) = v;
                             }
-                        }
+                        //}
                     }
 
                     if (valid_kv_cache_pos)
@@ -582,7 +582,7 @@ template <typename T>
 struct VecType
 {
     using Type = T;
-    using QuantizedType = mmha::fp8_8_t;
+    using QuantizedType = void; // mmha::fp8_8_t;
     using GPTNeoXEltType = T;
     using GPTJEltType = T;
 };
@@ -591,7 +591,7 @@ template <>
 struct VecType<float>
 {
     using Type = float4;
-    using QuantizedType = mmha::fp8_4_t;
+    using QuantizedType = void; // mmha::fp8_4_t;
     using GPTNeoXEltType = float;
     using GPTJEltType = float2;
 };
@@ -600,7 +600,7 @@ template <>
 struct VecType<half>
 {
     using Type = uint4;
-    using QuantizedType = mmha::fp8_8_t;
+    using QuantizedType = void; // mmha::fp8_8_t;
     using GPTNeoXEltType = uint16_t;
     using GPTJEltType = uint32_t;
 };
@@ -609,7 +609,7 @@ template <>
 struct VecType<__nv_bfloat16>
 {
     using Type = mmha::bf16_8_t;
-    using QuantizedType = mmha::fp8_8_t;
+    using QuantizedType = void; // mmha::fp8_8_t;
     using GPTNeoXEltType = __nv_bfloat16;
     using GPTJEltType = __nv_bfloat162;
 };
@@ -648,8 +648,8 @@ __global__ void applyBiasRopeUpdateKVCacheV2(QKVPreprocessingParams<T, KVCacheBu
     // Constants.
     using VecT = typename VecType<T>::Type;
     // Quantized output only supports fp8 currently.
-    using QuantizedEltType = __nv_fp8_e4m3;
-    using QuantizedVecType = typename VecType<T>::QuantizedType;
+    // using QuantizedEltType = __nv_fp8_e4m3;
+    // using QuantizedVecType = typename VecType<T>::QuantizedType;
     using GPTNeoXEltT = typename VecType<T>::GPTNeoXEltType;
     using GPTJEltT = typename VecType<T>::GPTJEltType;
     constexpr auto HEAD_SIZE = Dh;
@@ -813,9 +813,9 @@ __global__ void applyBiasRopeUpdateKVCacheV2(QKVPreprocessingParams<T, KVCacheBu
         if (valid_token)
         {
             auto const dst_q_idx = static_cast<size_t>(global_token_idx) * params.q_hidden_size + hidden_idx;
-            QuantizedEltType* quantized_q_ptr = STORE_QKV
-                ? reinterpret_cast<QuantizedEltType*>(params.QuantizedQKV) + src_q_idx
-                : reinterpret_cast<QuantizedEltType*>(params.Q) + dst_q_idx;
+            // QuantizedEltType* quantized_q_ptr = STORE_QKV
+            //     ? reinterpret_cast<QuantizedEltType*>(params.QuantizedQKV) + src_q_idx
+            //     : reinterpret_cast<QuantizedEltType*>(params.Q) + dst_q_idx;
             VecT* q_ptr = STORE_QKV ? reinterpret_ptr<T, VecT>(params.QKV, src_q_idx)
                                     : reinterpret_ptr<T, VecT>(params.Q, dst_q_idx);
 
@@ -827,35 +827,35 @@ __global__ void applyBiasRopeUpdateKVCacheV2(QKVPreprocessingParams<T, KVCacheBu
                 mmha::convert_from_float(&scaleOrigQuant, params.kvScaleOrigQuant ? params.kvScaleOrigQuant[0] : 1.0f);
             }
 
-            if constexpr (FP8_OUTPUT)
-            {
+            // if constexpr (FP8_OUTPUT)
+            // {
                 // Quant the vec to fp8 vec with the scale.
-                mmha::store_8bits_vec(quantized_q_ptr, q, 0, scaleOrigQuant);
-            }
-            else
-            {
+            //     mmha::store_8bits_vec(quantized_q_ptr, q, 0, scaleOrigQuant);
+            // }
+            // else
+            // {
                 *q_ptr = q;
-            }
+            // }
             if ((params.head_num == params.kv_head_num) || (head_idx == (kv_head_idx * params.qheads_per_kv_head)))
             {
                 if constexpr (STORE_QKV)
                 {
-                    if constexpr (FP8_OUTPUT)
-                    {
+                    // if constexpr (FP8_OUTPUT)
+                    //{
                         // Quant the vec to fp8 vec with the scale.
-                        mmha::store_8bits_vec(
-                            reinterpret_cast<QuantizedEltType*>(params.QuantizedQKV), k, src_k_idx, scaleOrigQuant);
-                        mmha::store_8bits_vec(
-                            reinterpret_cast<QuantizedEltType*>(params.QuantizedQKV), v, src_v_idx, scaleOrigQuant);
-                    }
-                    else
-                    {
+                    //    mmha::store_8bits_vec(
+                    //        reinterpret_cast<QuantizedEltType*>(params.QuantizedQKV), k, src_k_idx, scaleOrigQuant);
+                    //    mmha::store_8bits_vec(
+                    //        reinterpret_cast<QuantizedEltType*>(params.QuantizedQKV), v, src_v_idx, scaleOrigQuant);
+                    //}
+                    //else
+                    //{
                         *reinterpret_cast<VecT*>(&params.QKV[src_k_idx]) = k;
                         if constexpr (ADD_BIAS)
                         {
                             *reinterpret_cast<VecT*>(&params.QKV[src_v_idx]) = v;
                         }
-                    }
+                    //}
                 }
 
                 if (valid_kv_cache_pos)
diff --git a/cpp/tensorrt_llm/kernels/weightOnlyBatchedGemv/cudaCoreGemm.cu b/cpp/tensorrt_llm/kernels/weightOnlyBatchedGemv/cudaCoreGemm.cu
index d131488c..e39b5a11 100644
--- a/cpp/tensorrt_llm/kernels/weightOnlyBatchedGemv/cudaCoreGemm.cu
+++ b/cpp/tensorrt_llm/kernels/weightOnlyBatchedGemv/cudaCoreGemm.cu
@@ -159,6 +159,7 @@ bool cudaCoreGemmDispatcher(Params const& params, cudaStream_t stream)
     {
         dispatched = false;
     }
+#if defined(ENABLE_FP8)
     else if (params.inputType == nvinfer1::DataType::kFP8)
     {
         if (params.k % 16 != 0)
@@ -183,6 +184,7 @@ bool cudaCoreGemmDispatcher(Params const& params, cudaStream_t stream)
             dispatched = false;
         }
     }
+#endif
     else if (params.inputType == nvinfer1::DataType::kHALF)
     {
         if (params.k % 8 != 0)
diff --git a/cpp/tensorrt_llm/kernels/weightOnlyBatchedGemv/cudaCoreGemm.h b/cpp/tensorrt_llm/kernels/weightOnlyBatchedGemv/cudaCoreGemm.h
index 4daad747..5735c846 100644
--- a/cpp/tensorrt_llm/kernels/weightOnlyBatchedGemv/cudaCoreGemm.h
+++ b/cpp/tensorrt_llm/kernels/weightOnlyBatchedGemv/cudaCoreGemm.h
@@ -28,7 +28,9 @@
 #include <cstdint>
 #include <cuda_bf16.h>
 #include <cuda_fp16.h>
+#if defined(ENABLE_FP8)
 #include <cuda_fp8.h>
+#endif
 #include <cuda_runtime.h>
 #include <cuda_runtime_api.h>
 #include <iostream>
diff --git a/cpp/tensorrt_llm/plugins/CMakeLists.txt b/cpp/tensorrt_llm/plugins/CMakeLists.txt
index 604c656e..c48cab57 100755
--- a/cpp/tensorrt_llm/plugins/CMakeLists.txt
+++ b/cpp/tensorrt_llm/plugins/CMakeLists.txt
@@ -39,7 +39,7 @@ set(PLUGIN_LISTS
     identityPlugin
     gemmPlugin
     gemmSwigluPlugin
-    fp8RowwiseGemmPlugin
+    #fp8RowwiseGemmPlugin
     smoothQuantGemmPlugin
     quantizePerTokenPlugin
     quantizeTensorPlugin
@@ -50,11 +50,11 @@ set(PLUGIN_LISTS
     lookupPlugin
     loraPlugin
     mixtureOfExperts
-    selectiveScanPlugin
+    #selectiveScanPlugin
     mambaConv1dPlugin
-    lruPlugin
-    cumsumLastDimPlugin
-    lowLatencyGemmPlugin)
+    #lruPlugin
+    cumsumLastDimPlugin)
+    #lowLatencyGemmPlugin
 
 foreach(PLUGIN_ITER ${PLUGIN_LISTS})
   include_directories(${PLUGIN_ITER})
@@ -130,7 +130,7 @@ target_link_libraries(
   ${CUBLASLT_LIB}
   nvinfer
   ${CUDA_DRV_LIB}
-  ${CUDA_NVML_LIB}
+  #${CUDA_NVML_LIB}
   ${CUDA_RT_LIB}
   ${CMAKE_DL_LIBS}
   ${SHARED_TARGET})
diff --git a/cpp/tensorrt_llm/plugins/api/tllmPlugin.cpp b/cpp/tensorrt_llm/plugins/api/tllmPlugin.cpp
index 8a9d6784..9ec34ecd 100644
--- a/cpp/tensorrt_llm/plugins/api/tllmPlugin.cpp
+++ b/cpp/tensorrt_llm/plugins/api/tllmPlugin.cpp
@@ -20,7 +20,7 @@
 #include "tensorrt_llm/runtime/tllmLogger.h"
 
 #include "tensorrt_llm/plugins/bertAttentionPlugin/bertAttentionPlugin.h"
-#include "tensorrt_llm/plugins/fp8RowwiseGemmPlugin/fp8RowwiseGemmPlugin.h"
+// #include "tensorrt_llm/plugins/fp8RowwiseGemmPlugin/fp8RowwiseGemmPlugin.h"
 #include "tensorrt_llm/plugins/gemmPlugin/gemmPlugin.h"
 #include "tensorrt_llm/plugins/gemmSwigluPlugin/gemmSwigluPlugin.h"
 #include "tensorrt_llm/plugins/gptAttentionPlugin/gptAttentionPlugin.h"
@@ -39,7 +39,7 @@
 #include "tensorrt_llm/plugins/ncclPlugin/sendPlugin.h"
 #endif // ENABLE_MULTI_DEVICE
 #include "tensorrt_llm/plugins/cumsumLastDimPlugin/cumsumLastDimPlugin.h"
-#include "tensorrt_llm/plugins/lowLatencyGemmPlugin/lowLatencyGemmPlugin.h"
+// #include "tensorrt_llm/plugins/lowLatencyGemmPlugin/lowLatencyGemmPlugin.h"
 #include "tensorrt_llm/plugins/quantizePerTokenPlugin/quantizePerTokenPlugin.h"
 #include "tensorrt_llm/plugins/quantizeTensorPlugin/quantizeTensorPlugin.h"
 #include "tensorrt_llm/plugins/rmsnormQuantizationPlugin/rmsnormQuantizationPlugin.h"
@@ -177,7 +177,7 @@ extern "C"
         static tensorrt_llm::plugins::GPTAttentionPluginCreator gptAttentionPluginCreator;
         static tensorrt_llm::plugins::GemmPluginCreator gemmPluginCreator;
         static tensorrt_llm::plugins::GemmSwigluPluginCreator gemmSwigluPluginCreator;
-        static tensorrt_llm::plugins::Fp8RowwiseGemmPluginCreator fp8RowwiseGemmPluginCreator;
+        // static tensorrt_llm::plugins::Fp8RowwiseGemmPluginCreator fp8RowwiseGemmPluginCreator;
         static tensorrt_llm::plugins::MixtureOfExpertsPluginCreator moePluginCreator;
 #if ENABLE_MULTI_DEVICE
         static tensorrt_llm::plugins::SendPluginCreator sendPluginCreator;
@@ -196,11 +196,11 @@ extern "C"
         static tensorrt_llm::plugins::WeightOnlyQuantMatmulPluginCreator weightOnlyQuantMatmulPluginCreator;
         static tensorrt_llm::plugins::LookupPluginCreator lookupPluginCreator;
         static tensorrt_llm::plugins::LoraPluginCreator loraPluginCreator;
-        static tensorrt_llm::plugins::SelectiveScanPluginCreator selectiveScanPluginCreator;
+        // static tensorrt_llm::plugins::SelectiveScanPluginCreator selectiveScanPluginCreator;
         static tensorrt_llm::plugins::MambaConv1dPluginCreator mambaConv1DPluginCreator;
-        static tensorrt_llm::plugins::lruPluginCreator lruPluginCreator;
+        // static tensorrt_llm::plugins::lruPluginCreator lruPluginCreator;
         static tensorrt_llm::plugins::CumsumLastDimPluginCreator cumsumLastDimPluginCreator;
-        static tensorrt_llm::plugins::LowLatencyGemmPluginCreator lowLatencyGemmPluginCreator;
+        // static tensorrt_llm::plugins::LowLatencyGemmPluginCreator lowLatencyGemmPluginCreator;
 
         static std::array pluginCreators
             = { creatorPtr(identityPluginCreator),
@@ -208,7 +208,7 @@ extern "C"
                   creatorPtr(gptAttentionPluginCreator),
                   creatorPtr(gemmPluginCreator),
                   creatorPtr(gemmSwigluPluginCreator),
-                  creatorPtr(fp8RowwiseGemmPluginCreator),
+                  // creatorPtr(fp8RowwiseGemmPluginCreator),
                   creatorPtr(moePluginCreator),
 #if ENABLE_MULTI_DEVICE
                   creatorPtr(sendPluginCreator),
@@ -226,11 +226,11 @@ extern "C"
                   creatorPtr(weightOnlyQuantMatmulPluginCreator),
                   creatorPtr(lookupPluginCreator),
                   creatorPtr(loraPluginCreator),
-                  creatorPtr(selectiveScanPluginCreator),
+                  // creatorPtr(selectiveScanPluginCreator),
                   creatorPtr(mambaConv1DPluginCreator),
-                  creatorPtr(lruPluginCreator),
+                  // creatorPtr(lruPluginCreator),
                   creatorPtr(cumsumLastDimPluginCreator),
-                  creatorPtr(lowLatencyGemmPluginCreator),
+                  // creatorPtr(lowLatencyGemmPluginCreator),
               };
         nbCreators = pluginCreators.size();
         return pluginCreators.data();
diff --git a/cpp/tensorrt_llm/plugins/common/gemmPluginProfiler.cpp b/cpp/tensorrt_llm/plugins/common/gemmPluginProfiler.cpp
index 3eafe594..c1a83019 100644
--- a/cpp/tensorrt_llm/plugins/common/gemmPluginProfiler.cpp
+++ b/cpp/tensorrt_llm/plugins/common/gemmPluginProfiler.cpp
@@ -17,11 +17,13 @@
 
 #include "tensorrt_llm/plugins/common/gemmPluginProfiler.h"
 #include "tensorrt_llm/common/cublasMMWrapper.h"
+#ifdef ENABLE_FP8
 #include "tensorrt_llm/kernels/cutlass_kernels/fp8_rowwise_gemm/fp8_rowwise_gemm.h"
+#endif
 #include "tensorrt_llm/kernels/cutlass_kernels/fpA_intB_gemm/fpA_intB_gemm.h"
 #include "tensorrt_llm/kernels/cutlass_kernels/fused_gated_gemm/fused_gated_gemm.h"
 #include "tensorrt_llm/kernels/cutlass_kernels/int8_gemm/int8_gemm.h"
-#include "tensorrt_llm/plugins/lowLatencyGemmPlugin/lowLatencyGemmPlugin.h"
+// #include "tensorrt_llm/plugins/lowLatencyGemmPlugin/lowLatencyGemmPlugin.h"
 #include "tensorrt_llm/plugins/mixtureOfExperts/mixtureOfExpertsPlugin.h"
 
 namespace tensorrt_llm::plugins
@@ -327,11 +329,11 @@ template class GemmPluginProfiler<tensorrt_llm::cutlass_extensions::CutlassGemmC
     std::shared_ptr<tensorrt_llm::kernels::cutlass_kernels::CutlassFusedGatedGemmRunnerInterface>, GemmIdCore,
     GemmIdCoreHash>;
 
-template class GemmPluginProfiler<tensorrt_llm::cutlass_extensions::CutlassGemmConfig,
-    std::shared_ptr<tensorrt_llm::kernels::cutlass_kernels::CutlassFp8RowwiseGemmRunnerInterface>, GemmIdCore,
-    GemmIdCoreHash>;
+// template class GemmPluginProfiler<tensorrt_llm::cutlass_extensions::CutlassGemmConfig,
+//    std::shared_ptr<tensorrt_llm::kernels::cutlass_kernels::CutlassFp8RowwiseGemmRunnerInterface>, GemmIdCore,
+//    GemmIdCoreHash>;
 
-template class GemmPluginProfiler<LowLatencyGemmPluginProfiler::Config, LowLatencyGemmRunnerPtr, GemmIdCore,
-    GemmIdCoreHash>;
+//template class GemmPluginProfiler<LowLatencyGemmPluginProfiler::Config, LowLatencyGemmRunnerPtr, GemmIdCore,
+//    GemmIdCoreHash>;
 
 } // namespace tensorrt_llm::plugins
diff --git a/cpp/tensorrt_llm/plugins/common/plugin.cpp b/cpp/tensorrt_llm/plugins/common/plugin.cpp
index 95401ade..59d8bef6 100644
--- a/cpp/tensorrt_llm/plugins/common/plugin.cpp
+++ b/cpp/tensorrt_llm/plugins/common/plugin.cpp
@@ -23,7 +23,9 @@
 #include <cstdint>
 #include <cuda_bf16.h>
 #include <cuda_fp16.h>
+#if defined(ENABLE_FP8)
 #include <cuda_fp8.h>
+#endif
 #include <functional>
 #include <mutex>
 #include <thread>
diff --git a/cpp/tensorrt_llm/plugins/common/plugin.h b/cpp/tensorrt_llm/plugins/common/plugin.h
index 96bd1ef4..f0bfd606 100644
--- a/cpp/tensorrt_llm/plugins/common/plugin.h
+++ b/cpp/tensorrt_llm/plugins/common/plugin.h
@@ -33,7 +33,7 @@
 #include <cstring>
 #include <map>
 #include <memory>
-#include <nvml.h>
+// #include <nvml.h>
 #include <optional>
 #include <set>
 #include <string>
@@ -302,13 +302,13 @@ private:
     std::unordered_map<std::string_view, Record> mMap;
 };
 
-#define NVML_CHECK(cmd)                                                                                                \
-    do                                                                                                                 \
-    {                                                                                                                  \
-        nvmlReturn_t r = cmd;                                                                                          \
-        if (r != NVML_SUCCESS)                                                                                         \
-        {                                                                                                              \
-            printf("Failed, NVML error %s:%d '%s'\n", __FILE__, __LINE__, nvmlErrorString(r));                         \
-            exit(EXIT_FAILURE);                                                                                        \
-        }                                                                                                              \
-    } while (0)
+// #define NVML_CHECK(cmd)                                                                                                \
+//    do                                                                                                                 \
+//    {                                                                                                                  \
+//        nvmlReturn_t r = cmd;                                                                                          \
+//        if (r != NVML_SUCCESS)                                                                                         \
+//        {                                                                                                              \
+//            printf("Failed, NVML error %s:%d '%s'\n", __FILE__, __LINE__, nvmlErrorString(r));                         \
+//            exit(EXIT_FAILURE);                                                                                        \
+//        }                                                                                                              \
+//    } while (0)
diff --git a/cpp/tensorrt_llm/plugins/fp8RowwiseGemmPlugin/fp8RowwiseGemmPlugin.cpp b/cpp/tensorrt_llm/plugins/fp8RowwiseGemmPlugin/fp8RowwiseGemmPlugin.cpp
deleted file mode 100644
index e741f203..00000000
--- a/cpp/tensorrt_llm/plugins/fp8RowwiseGemmPlugin/fp8RowwiseGemmPlugin.cpp
+++ /dev/null
@@ -1,437 +0,0 @@
-/*
- * SPDX-FileCopyrightText: Copyright (c) 1993-2022 NVIDIA CORPORATION &
- * AFFILIATES. All rights reserved. SPDX-License-Identifier: Apache-2.0
- *
- * Licensed under the Apache License, Version 2.0 (the "License");
- * you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- * http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-#include "fp8RowwiseGemmPlugin.h"
-#include "cutlass_extensions/gemm_configs.h"
-
-#include <NvInferRuntimeBase.h>
-#include <numeric>
-
-using namespace nvinfer1;
-using namespace tensorrt_llm::common;
-using namespace tensorrt_llm::kernels::cutlass_kernels;
-using tensorrt_llm::plugins::Fp8RowwiseGemmPluginCreator;
-using tensorrt_llm::plugins::Fp8RowwiseGemmPlugin;
-using tensorrt_llm::plugins::Fp8RowwiseGemmPluginProfiler;
-using tensorrt_llm::plugins::read;
-using tensorrt_llm::plugins::write;
-
-static char const* FP8_ROWWISE_GEMM_PLUGIN_VERSION{"1"};
-static char const* FP8_ROWWISE_GEMM_PLUGIN_NAME{"Fp8RowwiseGemm"};
-PluginFieldCollection Fp8RowwiseGemmPluginCreator::mFC{};
-std::vector<nvinfer1::PluginField> Fp8RowwiseGemmPluginCreator::mPluginAttributes;
-
-size_t Fp8RowwiseGemmPluginProfiler::getBytePerElement(nvinfer1::DataType type)
-{
-    size_t bpe;
-    if (type == nvinfer1::DataType::kHALF || type == nvinfer1::DataType::kBF16)
-    {
-        bpe = 2;
-    }
-    else if (type == nvinfer1::DataType::kINT8 || type == nvinfer1::DataType::kFP8)
-    {
-        bpe = 1;
-    }
-    else
-    {
-        TLLM_THROW("Not recognized/implemented");
-    }
-    return bpe;
-}
-
-void Fp8RowwiseGemmPluginProfiler::setQuantMode(tensorrt_llm::common::QuantMode const& quantMode)
-{
-    mQuantMode = quantMode;
-}
-
-void Fp8RowwiseGemmPluginProfiler::runTactic(int m, int n, int k, Fp8RowwiseGemmPluginProfiler::Config const& tactic,
-    char* workspace, cudaStream_t const& stream)
-{
-    size_t bpeIn = getBytePerElement(nvinfer1::DataType::kFP8);
-    size_t bpeOut = getBytePerElement(mType);
-
-    // Workspace size required by gemm runner
-    // NB: this function will throw exception when selected tactic exceeds SMEM, which is then
-    // caught by gemmPluginProfiler and it will register this tactic as invalid
-    size_t wsSizeRunner = mRunner->getWorkspaceSize(m, n, k);
-
-    // Workspace size required by profiling
-    size_t wsByteOffset = 0;
-    int8_t* wsBytePointer = reinterpret_cast<int8_t*>(workspace);
-    void* aTmp = reinterpret_cast<void*>(nextWorkspacePtr(wsBytePointer, wsByteOffset, m * k * bpeIn));
-    void* bTmp = reinterpret_cast<void*>(nextWorkspacePtr(wsBytePointer, wsByteOffset, n * k * bpeIn));
-    // void* cTmp = reinterpret_cast<void*>(nextWorkspacePtr(wsBytePointer, wsByteOffset,  n * bpeOut));
-    void* dTmp = reinterpret_cast<void*>(nextWorkspacePtr(wsBytePointer, wsByteOffset, m * n * bpeOut));
-    float* scaleD0Tmp = reinterpret_cast<float*>(nextWorkspacePtr(wsBytePointer, wsByteOffset, m * sizeof(float)));
-    float* scaleD1Tmp = reinterpret_cast<float*>(nextWorkspacePtr(wsBytePointer, wsByteOffset, n * sizeof(float)));
-    char* workspaceTmp = reinterpret_cast<char*>(nextWorkspacePtr(wsBytePointer, wsByteOffset, wsSizeRunner));
-
-    // Run profiling
-    mRunner->gemm(dTmp, aTmp, bTmp, nullptr, mQuantMode, m, n, k, scaleD0Tmp, scaleD1Tmp, tactic, workspaceTmp,
-        wsSizeRunner, stream);
-    sync_check_cuda_error();
-}
-
-int Fp8RowwiseGemmPluginProfiler::getMaxProfileM() const
-{
-    // Max_num_tokens are not suggested to be set larger than 16k.
-    return 16384;
-}
-
-void Fp8RowwiseGemmPluginProfiler::computeTmpSize(size_t maxM, size_t n, size_t k)
-{
-    std::vector<size_t> workspaces = {
-        maxM * k * getBytePerElement(nvinfer1::DataType::kFP8), // A
-        n * k * getBytePerElement(nvinfer1::DataType::kFP8),    // B
-        // n * getBytePerElement(mType),          // C_bias
-        maxM * n * getBytePerElement(mType),  // D
-        maxM * sizeof(float),                 // alphaRow
-        n * sizeof(float),                    // alphaCol
-        maxM * sizeof(float),                 // alphaOutput
-        mRunner->getWorkspaceSize(maxM, n, k) // workspace
-    };
-    size_t bytes = calculateTotalWorkspaceSize(workspaces.data(), workspaces.size());
-    setTmpWorkspaceSizeInBytes(bytes);
-}
-
-std::vector<Fp8RowwiseGemmPluginProfiler::Config> Fp8RowwiseGemmPluginProfiler::getTactics(int m, int n, int k) const
-{
-    return mRunner->getConfigs();
-}
-
-Fp8RowwiseGemmPlugin::Fp8RowwiseGemmPlugin(
-    QuantMode quantMode, nvinfer1::DataType type, Fp8RowwiseGemmPlugin::PluginProfilerPtr const& pluginProfiler)
-    : mQuantMode(quantMode)
-    , mPluginProfiler(pluginProfiler)
-{
-    init(type);
-}
-
-// Parameterized constructor
-Fp8RowwiseGemmPlugin::Fp8RowwiseGemmPlugin(
-    void const* data, size_t length, Fp8RowwiseGemmPlugin::PluginProfilerPtr const& pluginProfiler)
-    : mPluginProfiler(pluginProfiler)
-{
-    char const *d = reinterpret_cast<char const*>(data), *a = d;
-    nvinfer1::DataType type;
-    unsigned int quantMode;
-    read(d, quantMode);
-    read(d, type);
-    read(d, mDims);
-
-    mQuantMode = QuantMode(quantMode);
-
-    init(type);
-
-    mPluginProfiler->deserialize(d, mDims, mGemmId);
-
-    TLLM_CHECK(d == a + length);
-}
-
-void Fp8RowwiseGemmPlugin::init(nvinfer1::DataType type)
-{
-    mType = type;
-    if (mType == nvinfer1::DataType::kHALF)
-    {
-        mGemmRunner = std::make_shared<CutlassFp8RowwiseGemmRunner<half>>();
-    }
-#ifdef ENABLE_BF16
-    else if (mType == nvinfer1::DataType::kBF16)
-    {
-        mGemmRunner = std::make_shared<CutlassFp8RowwiseGemmRunner<__nv_bfloat16>>();
-    }
-#endif
-    else
-    {
-        TLLM_THROW("Fp8 Rowwise Gemm plugin doesn't support this type now");
-    }
-
-    mPluginProfiler->setQuantMode(mQuantMode);
-
-    mGemmId = GemmIdCore(mDims.n, mDims.k, mType);
-}
-
-// IPluginV2DynamicExt Methods
-nvinfer1::IPluginV2DynamicExt* Fp8RowwiseGemmPlugin::clone() const noexcept
-{
-    auto* plugin = new Fp8RowwiseGemmPlugin(*this);
-    return plugin;
-}
-
-nvinfer1::DimsExprs Fp8RowwiseGemmPlugin::getOutputDimensions(
-    int outputIndex, nvinfer1::DimsExprs const* inputs, int nbInputs, nvinfer1::IExprBuilder& exprBuilder) noexcept
-{
-    try
-    {
-        TLLM_CHECK(nbInputs == 4);
-        TLLM_CHECK(outputIndex == 0);
-        int const nbDimsA = inputs[0].nbDims;
-        TLLM_CHECK(nbDimsA >= 2);
-        DimsExprs ret;
-        ret.nbDims = nbDimsA;
-        for (int ii = 0; ii < nbDimsA - 1; ++ii)
-        {
-            ret.d[ii] = inputs[0].d[ii];
-        }
-        ret.d[nbDimsA - 1] = inputs[1].d[0];
-        return ret;
-    }
-    catch (std::exception const& e)
-    {
-        caughtError(e);
-    }
-    return DimsExprs{};
-}
-
-bool Fp8RowwiseGemmPlugin::supportsFormatCombination(
-    int pos, nvinfer1::PluginTensorDesc const* inOut, int nbInputs, int nbOutputs) noexcept
-{
-    switch (pos)
-    {
-    case 0:
-        // activation
-        return inOut[pos].type == nvinfer1::DataType::kFP8 && inOut[pos].format == TensorFormat::kLINEAR;
-    case 1:
-        // weights
-        // Weights stored in checkpoint must have fp8 type
-        return inOut[pos].type == nvinfer1::DataType::kFP8 && inOut[pos].format == TensorFormat::kLINEAR;
-    case 2:
-        // scales channels
-    case 3:
-        // scales tokens
-        return inOut[pos].type == nvinfer1::DataType::kFLOAT && inOut[pos].format == TensorFormat::kLINEAR;
-    case 4:
-        // out
-        return inOut[pos].type == mType && inOut[pos].format == TensorFormat::kLINEAR;
-    default:
-        // Never should be here
-        TLLM_THROW("Fp8 Rowwise Gemm plugin doesn't support this type now");
-        assert(false);
-        return false;
-    }
-}
-
-void Fp8RowwiseGemmPlugin::configurePlugin(nvinfer1::DynamicPluginTensorDesc const* in, int nbInputs,
-    nvinfer1::DynamicPluginTensorDesc const* out, int nbOutputs) noexcept
-{
-    auto const minM = std::accumulate(in[0].min.d, in[0].min.d + in[0].min.nbDims - 1, 1, std::multiplies<int>());
-    auto const maxM = std::accumulate(in[0].max.d, in[0].max.d + in[0].max.nbDims - 1, 1, std::multiplies<int>());
-
-    int const maxK = in[0].max.d[in[0].max.nbDims - 1];
-    int const maxN = in[1].max.d[0];
-    int const minK = in[0].min.d[in[0].min.nbDims - 1];
-    int const minN = in[1].min.d[0];
-
-    TLLM_CHECK_WITH_INFO(minN == maxN, "Variable out channels is not allowed");
-    TLLM_CHECK_WITH_INFO(minK == maxK, "Variable in channels is not allowed");
-
-    if (!mDims.isInitialized())
-    {
-        mDims = {minM, maxM, maxN, maxK};
-    }
-    mGemmId = {maxN, maxK, mType};
-
-    mWorkspaceMaxSize = mGemmRunner->getWorkspaceSize(maxM, maxN, maxK);
-}
-
-size_t Fp8RowwiseGemmPlugin::getWorkspaceSize(nvinfer1::PluginTensorDesc const* inputs, int nbInputs,
-    nvinfer1::PluginTensorDesc const* outputs, int nbOutputs) const noexcept
-{
-    return mWorkspaceMaxSize;
-}
-
-int Fp8RowwiseGemmPlugin::enqueue(nvinfer1::PluginTensorDesc const* inputDesc,
-    nvinfer1::PluginTensorDesc const* outputDesc, void const* const* inputs, void* const* outputs, void* workspace,
-    cudaStream_t stream) noexcept
-{
-    // inputs
-    //     mat1           [M(*), K]
-    //     mat2           [N, K]
-    //     scale_tokens   [M, 1] if has_per_token_scaling else [1, 1]
-    //     scale_channels [1, N] if has_per_channel_scaling else [1, 1]
-    // outputs
-    //     mat [M(*), N]
-    int m = 1;
-    for (int ii = 0; ii < inputDesc[0].dims.nbDims - 1; ++ii)
-    {
-        m *= inputDesc[0].dims.d[ii];
-    }
-    int const n = inputDesc[1].dims.d[0];
-    int const k = inputDesc[0].dims.d[inputDesc[0].dims.nbDims - 1];
-    size_t const wsSize = mGemmRunner->getWorkspaceSize(m, n, k);
-
-    auto const bestTactic = mPluginProfiler->getBestConfig(m, mGemmId);
-    TLLM_CHECK_WITH_INFO(bestTactic, "No valid GEMM tactic");
-    mGemmRunner->gemm(outputs[0], inputs[0], inputs[1], nullptr, mQuantMode, m, n, k,
-        reinterpret_cast<float const*>(inputs[2]), reinterpret_cast<float const*>(inputs[3]), *bestTactic,
-        reinterpret_cast<char*>(workspace), wsSize, stream);
-    sync_check_cuda_error();
-
-    return 0;
-}
-
-// IPluginV2Ext Methods
-nvinfer1::DataType Fp8RowwiseGemmPlugin::getOutputDataType(
-    int index, nvinfer1::DataType const* inputTypes, int nbInputs) const noexcept
-{
-    TLLM_CHECK(index == 0);
-    return mType;
-}
-
-// IPluginV2 Methods
-
-char const* Fp8RowwiseGemmPlugin::getPluginType() const noexcept
-{
-    return FP8_ROWWISE_GEMM_PLUGIN_NAME;
-}
-
-char const* Fp8RowwiseGemmPlugin::getPluginVersion() const noexcept
-{
-    return FP8_ROWWISE_GEMM_PLUGIN_VERSION;
-}
-
-int Fp8RowwiseGemmPlugin::getNbOutputs() const noexcept
-{
-    return 1;
-}
-
-int Fp8RowwiseGemmPlugin::initialize() noexcept
-{
-    configGemm(); // gemm profiler in action
-    return 0;
-}
-
-void Fp8RowwiseGemmPlugin::terminate() noexcept {}
-
-size_t Fp8RowwiseGemmPlugin::getSerializationSize() const noexcept
-{
-    return sizeof(unsigned int) +                       // QuantMode
-        sizeof(nvinfer1::DataType) +                    // dtype
-        sizeof(mDims) +                                 // Dimensions
-        mPluginProfiler->getSerializationSize(mGemmId); // selected tactics container size
-}
-
-void Fp8RowwiseGemmPlugin::serialize(void* buffer) const noexcept
-{
-    char *d = static_cast<char*>(buffer), *a = d;
-    write(d, mQuantMode.value());
-    write(d, mType);
-    write(d, mDims);
-
-    mPluginProfiler->serialize(d, mGemmId);
-    TLLM_CHECK(d == a + getSerializationSize());
-}
-
-void Fp8RowwiseGemmPlugin::destroy() noexcept
-{
-    // This gets called when the network containing plugin is destroyed
-    delete this;
-}
-
-void Fp8RowwiseGemmPlugin::configGemm()
-{
-    mPluginProfiler->profileTactics(mGemmRunner, mType, mDims, mGemmId);
-}
-
-Fp8RowwiseGemmPluginCreator::Fp8RowwiseGemmPluginCreator()
-{
-    // Fill PluginFieldCollection with PluginField arguments metadata
-    mPluginAttributes.clear();
-    mPluginAttributes.emplace_back(PluginField("has_per_channel_scaling", nullptr, PluginFieldType::kINT32, 1));
-    mPluginAttributes.emplace_back(PluginField("has_per_token_scaling", nullptr, PluginFieldType::kINT32, 1));
-    mPluginAttributes.emplace_back(PluginField("type_id", nullptr, PluginFieldType::kINT32, 1));
-    mFC.nbFields = mPluginAttributes.size();
-    mFC.fields = mPluginAttributes.data();
-}
-
-char const* Fp8RowwiseGemmPluginCreator::getPluginName() const noexcept
-{
-    return FP8_ROWWISE_GEMM_PLUGIN_NAME;
-}
-
-char const* Fp8RowwiseGemmPluginCreator::getPluginVersion() const noexcept
-{
-    return FP8_ROWWISE_GEMM_PLUGIN_VERSION;
-}
-
-PluginFieldCollection const* Fp8RowwiseGemmPluginCreator::getFieldNames() noexcept
-{
-    return &mFC;
-}
-
-IPluginV2* Fp8RowwiseGemmPluginCreator::createPlugin(char const* name, PluginFieldCollection const* fc) noexcept
-{
-    PluginField const* fields = fc->fields;
-    TLLM_CHECK(fc->nbFields == 3);
-    bool perTokenScaling, perChannelScaling;
-    nvinfer1::DataType type;
-    // Read configurations from each fields
-    for (int i = 0; i < fc->nbFields; ++i)
-    {
-        char const* attrName = fields[i].name;
-        if (!strcmp(attrName, "has_per_channel_scaling"))
-        {
-            TLLM_CHECK(fields[i].type == PluginFieldType::kINT32);
-            perChannelScaling = static_cast<bool>(*(static_cast<int const*>(fields[i].data)));
-        }
-        else if (!strcmp(attrName, "has_per_token_scaling"))
-        {
-            TLLM_CHECK(fields[i].type == PluginFieldType::kINT32);
-            perTokenScaling = static_cast<bool>(*(static_cast<int const*>(fields[i].data)));
-        }
-        else if (!strcmp(attrName, "type_id"))
-        {
-            TLLM_CHECK(fields[i].type == PluginFieldType::kINT32);
-            type = static_cast<nvinfer1::DataType>(*(static_cast<nvinfer1::DataType const*>(fields[i].data)));
-        }
-    }
-    try
-    {
-        // Fp8RowwiseGemmPluginCreator is unique and shared for an engine generation
-        // Create plugin profiler with shared tactics map
-        auto pluginProfiler = mGemmPluginProfileManager.createGemmPluginProfiler(/* inference */ false);
-        QuantMode quantMode = QuantMode::fromDescription();
-        auto* obj = new Fp8RowwiseGemmPlugin(quantMode, type, pluginProfiler);
-        obj->setPluginNamespace(mNamespace.c_str());
-        return obj;
-    }
-    catch (std::exception const& e)
-    {
-        caughtError(e);
-    }
-    return nullptr;
-}
-
-IPluginV2* Fp8RowwiseGemmPluginCreator::deserializePlugin(
-    char const* name, void const* serialData, size_t serialLength) noexcept
-{
-    // This object will be deleted when the network is destroyed, which will
-    // call Fp8RowwiseGemmPlugin::destroy()
-    try
-    {
-        // Create plugin profiler with private tactics map which is read from the serialized engine
-        auto pluginProfiler = mGemmPluginProfileManager.createGemmPluginProfiler(/* inference */ true);
-        auto* obj = new Fp8RowwiseGemmPlugin(serialData, serialLength, pluginProfiler);
-        obj->setPluginNamespace(mNamespace.c_str());
-        return obj;
-    }
-    catch (std::exception const& e)
-    {
-        caughtError(e);
-    }
-    return nullptr;
-}
diff --git a/cpp/tensorrt_llm/plugins/fp8RowwiseGemmPlugin/fp8RowwiseGemmPlugin.h b/cpp/tensorrt_llm/plugins/fp8RowwiseGemmPlugin/fp8RowwiseGemmPlugin.h
deleted file mode 100644
index 36f22ad5..00000000
--- a/cpp/tensorrt_llm/plugins/fp8RowwiseGemmPlugin/fp8RowwiseGemmPlugin.h
+++ /dev/null
@@ -1,140 +0,0 @@
-/*
- * SPDX-FileCopyrightText: Copyright (c) 1993-2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
- * SPDX-License-Identifier: Apache-2.0
- *
- * Licensed under the Apache License, Version 2.0 (the "License");
- * you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- * http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-#pragma once
-
-#include "tensorrt_llm/kernels/cutlass_kernels/fp8_rowwise_gemm/fp8_rowwise_gemm.h"
-#include "tensorrt_llm/plugins/common/gemmPluginProfiler.h"
-#include "tensorrt_llm/plugins/common/plugin.h"
-#include <cassert>
-#include <set>
-#include <string>
-#include <vector>
-
-namespace tensorrt_llm::plugins
-{
-
-using Fp8RowwiseGemmRunnerPtr
-    = std::shared_ptr<tensorrt_llm::kernels::cutlass_kernels::CutlassFp8RowwiseGemmRunnerInterface>;
-
-class Fp8RowwiseGemmPluginProfiler : public GemmPluginProfiler<tensorrt_llm::cutlass_extensions::CutlassGemmConfig,
-                                         Fp8RowwiseGemmRunnerPtr, GemmIdCore, GemmIdCoreHash>
-
-{
-public:
-    using Config = tensorrt_llm::cutlass_extensions::CutlassGemmConfig;
-
-    void setQuantMode(tensorrt_llm::common::QuantMode const& quantMode);
-
-    virtual int getMaxProfileM() const override;
-
-protected:
-    void runTactic(int m, int n, int k, Config const& tactic, char* workspace, cudaStream_t const& stream) override;
-
-    void computeTmpSize(size_t maxM, size_t n, size_t k) override;
-
-    std::vector<Config> getTactics(int m, int n, int k) const override;
-
-private:
-    size_t getBytePerElement(nvinfer1::DataType type);
-
-    tensorrt_llm::common::QuantMode mQuantMode;
-};
-
-class Fp8RowwiseGemmPlugin : public BasePlugin
-{
-public:
-    using PluginProfilerPtr = std::shared_ptr<Fp8RowwiseGemmPluginProfiler>;
-
-    Fp8RowwiseGemmPlugin() = delete;
-
-    Fp8RowwiseGemmPlugin(
-        tensorrt_llm::common::QuantMode quantMode, nvinfer1::DataType type, PluginProfilerPtr const& pluginProfiler);
-
-    Fp8RowwiseGemmPlugin(void const* data, size_t length, PluginProfilerPtr const& profiler);
-
-    ~Fp8RowwiseGemmPlugin() override = default;
-
-    // IPluginV2DynamicExt Methods
-    nvinfer1::IPluginV2DynamicExt* clone() const noexcept override;
-    nvinfer1::DimsExprs getOutputDimensions(int outputIndex, nvinfer1::DimsExprs const* inputs, int nbInputs,
-        nvinfer1::IExprBuilder& exprBuilder) noexcept override;
-    bool supportsFormatCombination(
-        int pos, nvinfer1::PluginTensorDesc const* inOut, int nbInputs, int nbOutputs) noexcept override;
-    void configurePlugin(nvinfer1::DynamicPluginTensorDesc const* in, int nbInputs,
-        nvinfer1::DynamicPluginTensorDesc const* out, int nbOutputs) noexcept override;
-    size_t getWorkspaceSize(nvinfer1::PluginTensorDesc const* inputs, int nbInputs,
-        nvinfer1::PluginTensorDesc const* outputs, int nbOutputs) const noexcept override;
-    int enqueue(nvinfer1::PluginTensorDesc const* inputDesc, nvinfer1::PluginTensorDesc const* outputDesc,
-        void const* const* inputs, void* const* outputs, void* workspace, cudaStream_t stream) noexcept override;
-
-    // IPluginV2Ext Methods
-    nvinfer1::DataType getOutputDataType(
-        int index, nvinfer1::DataType const* inputTypes, int nbInputs) const noexcept override;
-
-    // IPluginV2 Methods
-    char const* getPluginType() const noexcept override;
-    char const* getPluginVersion() const noexcept override;
-    int getNbOutputs() const noexcept override;
-    int initialize() noexcept override;
-    void terminate() noexcept override;
-    size_t getSerializationSize() const noexcept override;
-    void serialize(void* buffer) const noexcept override;
-    void destroy() noexcept override;
-
-private:
-    void init(nvinfer1::DataType type);
-
-    void configGemm();
-
-private:
-    const std::string mLayerName;
-
-    Fp8RowwiseGemmRunnerPtr mGemmRunner;
-    tensorrt_llm::common::QuantMode mQuantMode; // not configurable yet
-    size_t mWorkspaceMaxSize;
-
-    GemmDims mDims{};
-    GemmIdCore mGemmId{};
-
-    PluginProfilerPtr mPluginProfiler;
-
-    nvinfer1::DataType mType;
-};
-
-class Fp8RowwiseGemmPluginCreator : public BaseCreator
-{
-public:
-    Fp8RowwiseGemmPluginCreator();
-
-    char const* getPluginName() const noexcept override;
-
-    char const* getPluginVersion() const noexcept override;
-
-    nvinfer1::PluginFieldCollection const* getFieldNames() noexcept override;
-
-    nvinfer1::IPluginV2* createPlugin(char const* name, nvinfer1::PluginFieldCollection const* fc) noexcept override;
-
-    nvinfer1::IPluginV2* deserializePlugin(
-        char const* name, void const* serialData, size_t serialLength) noexcept override;
-
-private:
-    GemmPluginProfilerManager<Fp8RowwiseGemmPluginProfiler> mGemmPluginProfileManager;
-    static nvinfer1::PluginFieldCollection mFC;
-    static std::vector<nvinfer1::PluginField> mPluginAttributes;
-};
-
-} // namespace tensorrt_llm::plugins
diff --git a/cpp/tensorrt_llm/plugins/gemmSwigluPlugin/gemmSwigluPlugin.cpp b/cpp/tensorrt_llm/plugins/gemmSwigluPlugin/gemmSwigluPlugin.cpp
index 3b2142c8..27966e74 100644
--- a/cpp/tensorrt_llm/plugins/gemmSwigluPlugin/gemmSwigluPlugin.cpp
+++ b/cpp/tensorrt_llm/plugins/gemmSwigluPlugin/gemmSwigluPlugin.cpp
@@ -147,7 +147,9 @@ void GemmSwigluPlugin::init(nvinfer1::DataType type)
     mType = type;
     if (mType == nvinfer1::DataType::kFP8)
     {
+        #if defined(ENABLE_FP8)
         mGemmRunner = std::make_shared<CutlassFusedGatedGemmRunner<__nv_fp8_e4m3>>();
+        #endif
     }
     else
     {
diff --git a/cpp/tensorrt_llm/plugins/gptAttentionCommon/gptAttentionCommon.cpp b/cpp/tensorrt_llm/plugins/gptAttentionCommon/gptAttentionCommon.cpp
index 29190010..33218cf7 100644
--- a/cpp/tensorrt_llm/plugins/gptAttentionCommon/gptAttentionCommon.cpp
+++ b/cpp/tensorrt_llm/plugins/gptAttentionCommon/gptAttentionCommon.cpp
@@ -827,8 +827,10 @@ int GPTAttentionPluginCommon::enqueueContext(EnqueueContextParams<T, KVCacheBuff
     T* qk_buf_ = reinterpret_cast<T*>(nextWorkspacePtr(workspace_byte_ptr, offset, qk_buf_size));
     T* qkv_buf_2_ = reinterpret_cast<T*>(nextWorkspacePtr(workspace_byte_ptr, offset, qkv_buf_2_size));
     float* qk_buf_float_ = reinterpret_cast<float*>(nextWorkspacePtr(workspace_byte_ptr, offset, qk_buf_float_size));
-    __nv_fp8_e4m3* fp8_qkv_buffer
-        = reinterpret_cast<__nv_fp8_e4m3*>(nextWorkspacePtr(workspace_byte_ptr, offset, fp8_qkv_buffer_size));
+    // __nv_fp8_e4m3* fp8_qkv_buffer
+    //   = reinterpret_cast<__nv_fp8_e4m3*>(nextWorkspacePtr(workspace_byte_ptr, offset, fp8_qkv_buffer_size));
+    T* fp8_qkv_buffer
+        = reinterpret_cast<T*>(nextWorkspacePtr(workspace_byte_ptr, offset, fp8_qkv_buffer_size));
     int* padding_offset = mEnableContextFMHA
         ? nullptr
         : reinterpret_cast<int*>(nextWorkspacePtr(workspace_byte_ptr, offset, padding_offset_size));
diff --git a/cpp/tensorrt_llm/plugins/lowLatencyGemmPlugin/lowLatencyGemmPlugin.cpp b/cpp/tensorrt_llm/plugins/lowLatencyGemmPlugin/lowLatencyGemmPlugin.cpp
deleted file mode 100644
index e6ba4a2f..00000000
--- a/cpp/tensorrt_llm/plugins/lowLatencyGemmPlugin/lowLatencyGemmPlugin.cpp
+++ /dev/null
@@ -1,424 +0,0 @@
-
-/*
- * SPDX-FileCopyrightText: Copyright (c) 1993-2024 NVIDIA CORPORATION &
- * AFFILIATES. All rights reserved. SPDX-License-Identifier: Apache-2.0
- *
- * Licensed under the Apache License, Version 2.0 (the "License");
- * you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- * http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-#include "lowLatencyGemmPlugin.h"
-#include "tensorrt_llm/common/assert.h"
-#include "tensorrt_llm/common/cudaFp8Utils.h"
-#include "tensorrt_llm/common/logger.h"
-#include "tensorrt_llm/kernels/internal_cutlass_kernels/include/low_latency_gemm.h"
-#include <NvInferRuntime.h>
-#include <NvInferRuntimeBase.h>
-#include <NvInferRuntimePlugin.h>
-#include <cstddef>
-#include <cstdint>
-#include <cstdio>
-#include <numeric>
-#include <optional>
-#include <vector>
-
-using namespace nvinfer1;
-using namespace tensorrt_llm::common;
-using namespace tensorrt_llm::kernels::internal_cutlass_kernels;
-using tensorrt_llm::plugins::LowLatencyGemmPluginCreator;
-using tensorrt_llm::plugins::LowLatencyGemmPlugin;
-using tensorrt_llm::plugins::LowLatencyGemmPluginProfiler;
-using tensorrt_llm::plugins::read;
-using tensorrt_llm::plugins::write;
-
-static char const* LOW_LATENCY_GEMM_PLUGIN_VERSION{"1"};
-static char const* LOW_LATENCY_GEMM_PLUGIN_NAME{"LowLatencyGemm"};
-
-PluginFieldCollection LowLatencyGemmPluginCreator::mFC{};
-std::vector<nvinfer1::PluginField> LowLatencyGemmPluginCreator::mPluginAttributes;
-
-using FP8Type = __nv_fp8_e4m3;
-
-static std::optional<float> getFloatEnv(char const* name)
-{
-    char const* const env = std::getenv(name);
-    if (env == nullptr)
-    {
-        return std::nullopt;
-    }
-    try
-    {
-        float value = std::stof(env);
-        return {value};
-    }
-    catch (std::invalid_argument const& e)
-    {
-        return std::nullopt;
-    }
-    catch (std::out_of_range const& e)
-    {
-        return std::nullopt;
-    }
-};
-
-void LowLatencyGemmPluginProfiler::runTactic(int m, int n, int k, LowLatencyGemmPluginProfiler::Config const& tactic,
-    char* workspace, cudaStream_t const& stream)
-{
-
-    float default_pdl_overlap_ratio = 0.5;
-    float default_prefetch_ratio = -1.0;
-    FP8Type* aTmp = reinterpret_cast<FP8Type*>(workspace);
-    FP8Type* bTmp
-        = reinterpret_cast<FP8Type*>(nextWorkspacePtr(reinterpret_cast<int8_t*>(aTmp), m * k * sizeof(FP8Type)));
-    void* cTmp = reinterpret_cast<void*>(nextWorkspacePtr(reinterpret_cast<int8_t*>(bTmp), n * k * sizeof(FP8Type)));
-    size_t workspaceSize = mRunner->getWorkspaceSize(m, n, k);
-    char* workspaceTmp = reinterpret_cast<char*>(nextWorkspacePtr(reinterpret_cast<int8_t*>(cTmp), workspaceSize));
-    mRunner->gemm(aTmp, bTmp, 1.0f, 0.0f, nullptr, cTmp, m, n, k, default_pdl_overlap_ratio, default_prefetch_ratio,
-        tactic, workspaceTmp, workspaceSize, stream);
-}
-
-void LowLatencyGemmPluginProfiler::computeTmpSize(size_t maxM, size_t n, size_t k)
-{
-
-    std::vector<size_t> workspaces = {maxM * k * sizeof(FP8Type), n * k * sizeof(FP8Type),
-        maxM * n * (mType == nvinfer1::DataType::kFLOAT ? sizeof(float) : sizeof(half)),
-        mRunner->getWorkspaceSize(maxM, n, k)};
-
-    size_t bytes = calculateTotalWorkspaceSize(workspaces.data(), workspaces.size());
-    setTmpWorkspaceSizeInBytes(bytes);
-}
-
-std::vector<LowLatencyGemmPluginProfiler::Config> LowLatencyGemmPluginProfiler::getTactics(int m, int n, int k) const
-{
-    return mRunner->getConfigs();
-}
-
-LowLatencyGemmPlugin::LowLatencyGemmPlugin(
-    nvinfer1::DataType type, float alpha, PluginProfilerPtr const& pluginProfiler)
-    : mAplha(alpha)
-    , mPluginProfiler(pluginProfiler)
-{
-    init(type);
-}
-
-LowLatencyGemmPlugin::LowLatencyGemmPlugin(void const* data, size_t length, PluginProfilerPtr const& pluginProfiler)
-    : mPluginProfiler(pluginProfiler)
-{
-
-    char const *d = reinterpret_cast<char const*>(data), *a = d;
-    nvinfer1::DataType type;
-    read(d, type);
-    read(d, mAplha);
-    read(d, mDims);
-    init(type);
-    mPluginProfiler->deserialize(d, mDims, mGemmId);
-    TLLM_CHECK_WITH_INFO(d == a + length,
-        "Expected length (%d) != real length (%d). This is often "
-        "caused by using different TensorRT-LLM version to build "
-        "engine and run engine.",
-        (int) length, (int) (d - a));
-}
-
-void LowLatencyGemmPlugin::init(nvinfer1::DataType type)
-{
-
-    mType = type;
-
-    if (mType == nvinfer1::DataType::kFLOAT)
-    {
-        m_lowLatencyGemmRunner = std::make_shared<CutlassLowLatencyFp8GemmRunner<float>>();
-    }
-    else if (mType == nvinfer1::DataType::kHALF)
-    {
-        m_lowLatencyGemmRunner = std::make_shared<CutlassLowLatencyFp8GemmRunner<half>>();
-    }
-#ifdef ENABLE_BF16
-
-    else if (mType == nvinfer1::DataType::kBF16)
-    {
-        m_lowLatencyGemmRunner = std::make_shared<CutlassLowLatencyFp8GemmRunner<__nv_bfloat16>>();
-    }
-#endif
-    else
-    {
-        TLLM_THROW("Unsupported data type");
-    }
-    mGemmId = GemmIdCore(mDims.n, mDims.k, mType);
-}
-
-nvinfer1::DimsExprs LowLatencyGemmPlugin::getOutputDimensions(
-    int outputIndex, nvinfer1::DimsExprs const* inputs, int nbInputs, nvinfer1::IExprBuilder& exprBuilder) noexcept
-{
-    try
-    {
-        TLLM_CHECK(nbInputs == 2);
-        TLLM_CHECK(outputIndex == 0);
-        int const nbDimsA = inputs[0].nbDims;
-        TLLM_CHECK(nbDimsA >= 2);
-        DimsExprs ret;
-        ret.nbDims = nbDimsA;
-        for (int ii = 0; ii < nbDimsA - 1; ++ii)
-        {
-            ret.d[ii] = inputs[0].d[ii];
-        }
-        // input[1] , weights [n,k]
-        ret.d[nbDimsA - 1] = inputs[1].d[0];
-        return ret;
-    }
-    catch (std::exception const& e)
-    {
-        caughtError(e);
-    }
-    return DimsExprs{};
-}
-
-bool LowLatencyGemmPlugin::supportsFormatCombination(
-    int pos, nvinfer1::PluginTensorDesc const* inOut, int nbInputs, int nbOutputs) noexcept
-{
-    switch (pos)
-    {
-    case 0:
-        // activation
-        return inOut[pos].type == nvinfer1::DataType::kFP8 && inOut[pos].format == TensorFormat::kLINEAR;
-    case 1:
-        // weights
-        // Weights stored in checkpoint must have fp8 type
-        return inOut[pos].type == nvinfer1::DataType::kFP8 && inOut[pos].format == TensorFormat::kLINEAR;
-    case 2:
-        // out
-        return inOut[pos].type == mType && inOut[pos].format == TensorFormat::kLINEAR;
-    default:
-        // Never should be here
-        assert(false);
-        return false;
-    }
-}
-
-void LowLatencyGemmPlugin::configurePlugin(nvinfer1::DynamicPluginTensorDesc const* in, int nbInputs,
-    nvinfer1::DynamicPluginTensorDesc const* out, int nbOutputs) noexcept
-{
-    auto const minM = std::accumulate(in[0].min.d, in[0].min.d + in[0].min.nbDims - 1, 1, std::multiplies<int>());
-    auto const maxM = std::accumulate(in[0].max.d, in[0].max.d + in[0].max.nbDims - 1, 1, std::multiplies<int>());
-
-    int const maxK = in[0].max.d[in[0].max.nbDims - 1];
-    int const maxN = in[1].max.d[0];
-    int const minK = in[0].min.d[in[0].min.nbDims - 1];
-    int const minN = in[1].min.d[0];
-
-    TLLM_CHECK_WITH_INFO(minN == maxN, "Variable out channels is not allowed");
-    TLLM_CHECK_WITH_INFO(minK == maxK, "Variable in channels is not allowed");
-
-    if (!mDims.isInitialized())
-    {
-        mDims = {minM, maxM, maxN, maxK};
-    }
-    mGemmId = {maxN, maxK, mType};
-
-    m_workspaceMaxSize = m_lowLatencyGemmRunner->getWorkspaceSize(maxM, maxN, maxK);
-}
-
-size_t LowLatencyGemmPlugin::getWorkspaceSize(nvinfer1::PluginTensorDesc const* inputs, int nbInputs,
-    nvinfer1::PluginTensorDesc const* outputs, int nbOutputs) const noexcept
-{
-    return m_workspaceMaxSize;
-}
-
-int LowLatencyGemmPlugin::enqueue(nvinfer1::PluginTensorDesc const* inputDesc,
-    nvinfer1::PluginTensorDesc const* outputDesc, void const* const* inputs, void* const* outputs, void* workspace,
-    cudaStream_t stream) noexcept
-{
-
-    // input0 activation [M,K]
-    // input1 weights [N,K]
-    // output0 [M,N]
-
-    int64_t m64 = 1;
-    for (int ii = 0; ii < inputDesc[0].dims.nbDims - 1; ++ii)
-    {
-        m64 *= inputDesc[0].dims.d[ii];
-    }
-    int const m = TLLM_INT32_CAST(m64);
-    int const n = TLLM_INT32_CAST(inputDesc[1].dims.d[0]);
-    int const k = TLLM_INT32_CAST(inputDesc[0].dims.d[inputDesc[0].dims.nbDims - 1]);
-    int const wsSize = m_lowLatencyGemmRunner->getWorkspaceSize(m, n, k);
-    auto const& bestTactic = mPluginProfiler->getBestConfig(m, mGemmId);
-    TLLM_CHECK_WITH_INFO(bestTactic, "No valid Low Latency GEMM tactic");
-
-    auto env_pdl_overlap_ratio = getFloatEnv("TRTLLM_PDL_OVERLAP_RATIO");
-    auto env_prefetch_ratio = getFloatEnv("TRTLLM_PREFETCH_RATIO");
-    auto valid_ratio = [](std::optional<float>& env_val, float default_val)
-    {
-        if (env_val.has_value())
-        {
-            TLLM_CHECK_WITH_INFO(env_val.value() <= 1.0f, "Valid ratio should be less than or equal to 1.0");
-            return env_val.value();
-        }
-        return default_val;
-    };
-    float pdl_overlap_ratio = valid_ratio(env_pdl_overlap_ratio, /*default_val=*/0.5);
-    float prefetch_ratio = valid_ratio(env_prefetch_ratio, /*default_val=*/-1.0);
-    m_lowLatencyGemmRunner->gemm(const_cast<FP8Type*>(reinterpret_cast<FP8Type const*>(inputs[0])),
-        const_cast<FP8Type*>(reinterpret_cast<FP8Type const*>(inputs[1])), mAplha, 0.0F, nullptr, outputs[0], m, n, k,
-        pdl_overlap_ratio, prefetch_ratio, *bestTactic, reinterpret_cast<char*>(workspace), wsSize, stream);
-
-    return 0;
-}
-
-nvinfer1::DataType LowLatencyGemmPlugin::getOutputDataType(
-    int index, nvinfer1::DataType const* inputTypes, int nbInputs) const noexcept
-{
-    TLLM_CHECK(index == 0);
-    return mType;
-}
-
-// IPluginV2 Methods
-
-char const* LowLatencyGemmPlugin::getPluginType() const noexcept
-{
-    return LOW_LATENCY_GEMM_PLUGIN_NAME;
-}
-
-char const* LowLatencyGemmPlugin::getPluginVersion() const noexcept
-{
-    return LOW_LATENCY_GEMM_PLUGIN_VERSION;
-}
-
-int LowLatencyGemmPlugin::getNbOutputs() const noexcept
-{
-    return 1;
-}
-
-int LowLatencyGemmPlugin::initialize() noexcept
-{
-    configGemm();
-    return 0;
-}
-
-void LowLatencyGemmPlugin::terminate() noexcept {}
-
-nvinfer1::IPluginV2DynamicExt* LowLatencyGemmPlugin::clone() const noexcept
-{
-    auto* plugin = new LowLatencyGemmPlugin(*this);
-    return plugin;
-}
-
-size_t LowLatencyGemmPlugin::getSerializationSize() const noexcept
-{
-    return sizeof(nvinfer1::DataType) + // dtype
-        sizeof(float) * 1 +             // alpha
-        sizeof(mDims) + mPluginProfiler->getSerializationSize(mGemmId);
-}
-
-void LowLatencyGemmPlugin::serialize(void* buffer) const noexcept
-{
-    char *d = static_cast<char*>(buffer), *a = d;
-    write(d, mType);
-    write(d, mAplha);
-    write(d, mDims);
-    mPluginProfiler->serialize(d, mGemmId);
-    TLLM_CHECK(d == a + getSerializationSize());
-}
-
-void LowLatencyGemmPlugin::destroy() noexcept
-{
-    // This gets called when the network containing plugin is destroyed
-    delete this;
-}
-
-void LowLatencyGemmPlugin::configGemm()
-{
-    mPluginProfiler->profileTactics(m_lowLatencyGemmRunner, mType, mDims, mGemmId);
-}
-
-LowLatencyGemmPluginCreator::LowLatencyGemmPluginCreator()
-{
-
-    // Fill PluginFieldCollection with PluginField arguments metadata
-    mPluginAttributes.clear();
-    mPluginAttributes.emplace_back(PluginField("alpha", nullptr, PluginFieldType::kFLOAT32, 1));
-    mPluginAttributes.emplace_back(PluginField("type_id", nullptr, PluginFieldType::kINT32, 1));
-    mFC.nbFields = mPluginAttributes.size();
-    mFC.fields = mPluginAttributes.data();
-}
-
-char const* LowLatencyGemmPluginCreator::getPluginName() const noexcept
-{
-    return LOW_LATENCY_GEMM_PLUGIN_NAME;
-}
-
-char const* LowLatencyGemmPluginCreator::getPluginVersion() const noexcept
-{
-    return LOW_LATENCY_GEMM_PLUGIN_VERSION;
-}
-
-PluginFieldCollection const* LowLatencyGemmPluginCreator::getFieldNames() noexcept
-{
-    return &mFC;
-}
-
-IPluginV2* LowLatencyGemmPluginCreator::createPlugin(char const* name, PluginFieldCollection const* fc) noexcept
-{
-    PluginField const* fields = fc->fields;
-    float alpha;
-    nvinfer1::DataType type;
-    for (int i = 0; i < fc->nbFields; i++)
-    {
-        char const* attrName = fields[i].name;
-        if (!strcmp(attrName, "alpha"))
-        {
-
-            TLLM_CHECK(fields[i].type == PluginFieldType::kFLOAT32);
-            alpha = *(static_cast<float const*>(fields[i].data));
-        }
-        else if (!strcmp(attrName, "type_id"))
-        {
-            TLLM_CHECK(fields[i].type == PluginFieldType::kINT32);
-            type = static_cast<nvinfer1::DataType>(*(static_cast<nvinfer1::DataType const*>(fields[i].data)));
-        }
-    }
-
-    try
-    {
-
-        //
-        // GemmPluginCreator is unique and shared for an engine generation
-        // Create plugin profiler with shared tactics map
-
-        auto pluginProfiler = gemmPluginProfileManager.createGemmPluginProfiler(/*inference=*/false);
-        auto* obj = new LowLatencyGemmPlugin(type, alpha, pluginProfiler);
-        obj->setPluginNamespace(mNamespace.c_str());
-        return obj;
-    }
-
-    catch (std::exception const& e)
-    {
-        caughtError(e);
-    }
-    return nullptr;
-}
-
-IPluginV2* LowLatencyGemmPluginCreator::deserializePlugin(
-    char const* name, void const* serialData, size_t serialLength) noexcept
-{
-    try
-    {
-        auto pluginProfiler = gemmPluginProfileManager.createGemmPluginProfiler(/*inference=*/true);
-        auto* obj = new LowLatencyGemmPlugin(serialData, serialLength, pluginProfiler);
-        obj->setPluginNamespace(mNamespace.c_str());
-        return obj;
-    }
-    catch (std::exception const& e)
-    {
-        caughtError(e);
-    }
-    return nullptr;
-}
diff --git a/cpp/tensorrt_llm/plugins/lowLatencyGemmPlugin/lowLatencyGemmPlugin.h b/cpp/tensorrt_llm/plugins/lowLatencyGemmPlugin/lowLatencyGemmPlugin.h
deleted file mode 100644
index c780718a..00000000
--- a/cpp/tensorrt_llm/plugins/lowLatencyGemmPlugin/lowLatencyGemmPlugin.h
+++ /dev/null
@@ -1,135 +0,0 @@
-/*
- * SPDX-FileCopyrightText: Copyright (c) 1993-2024 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
- * SPDX-License-Identifier: Apache-2.0
- *
- * Licensed under the Apache License, Version 2.0 (the "License");
- * you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- * http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-#pragma once
-
-#include "tensorrt_llm/kernels/internal_cutlass_kernels/include/low_latency_gemm.h"
-
-#include "tensorrt_llm/plugins/common/gemmPluginProfiler.h"
-#include "tensorrt_llm/plugins/common/plugin.h"
-#include <cassert>
-#include <cstddef>
-#include <memory>
-#include <set>
-#include <string>
-#include <vector>
-
-namespace tensorrt_llm::plugins
-{
-
-using LowLatencyGemmRunnerPtr
-    = std::shared_ptr<tensorrt_llm::kernels::internal_cutlass_kernels::CutlassLowLatencyFp8GemmRunnerInterface>;
-
-class LowLatencyGemmPluginProfiler
-    : public GemmPluginProfiler<
-          tensorrt_llm::kernels::internal_cutlass_kernels::CutlassLowLatencyFp8GemmRunnerInterface::ConfigType,
-          LowLatencyGemmRunnerPtr, GemmIdCore, GemmIdCoreHash>
-{
-
-public:
-    using Config = tensorrt_llm::kernels::internal_cutlass_kernels::CutlassLowLatencyFp8GemmRunnerInterface::ConfigType;
-
-protected:
-    void runTactic(int m, int n, int k, Config const& tactic, char* workspace, cudaStream_t const& stream) override;
-
-    void computeTmpSize(size_t maxM, size_t n, size_t k) override;
-
-    std::vector<Config> getTactics(int m, int n, int k) const override;
-};
-
-class LowLatencyGemmPlugin : public BasePlugin
-{
-
-public:
-    using PluginProfilerPtr = std::shared_ptr<LowLatencyGemmPluginProfiler>;
-
-    LowLatencyGemmPlugin() = delete;
-
-    LowLatencyGemmPlugin(nvinfer1::DataType type, float alpha, PluginProfilerPtr const& pluginProfiler);
-
-    LowLatencyGemmPlugin(void const* data, size_t length, PluginProfilerPtr const& pluginProfiler);
-    ~LowLatencyGemmPlugin() override = default;
-
-    // IPluginV2DynamicExt Methods
-    nvinfer1::IPluginV2DynamicExt* clone() const noexcept override;
-    nvinfer1::DimsExprs getOutputDimensions(int outputIndex, nvinfer1::DimsExprs const* inputs, int nbInputs,
-        nvinfer1::IExprBuilder& exprBuilder) noexcept override;
-    bool supportsFormatCombination(
-        int pos, nvinfer1::PluginTensorDesc const* inOut, int nbInputs, int nbOutputs) noexcept override;
-    void configurePlugin(nvinfer1::DynamicPluginTensorDesc const* in, int nbInputs,
-        nvinfer1::DynamicPluginTensorDesc const* out, int nbOutputs) noexcept override;
-    size_t getWorkspaceSize(nvinfer1::PluginTensorDesc const* inputs, int nbInputs,
-        nvinfer1::PluginTensorDesc const* outputs, int nbOutputs) const noexcept override;
-    int enqueue(nvinfer1::PluginTensorDesc const* inputDesc, nvinfer1::PluginTensorDesc const* outputDesc,
-        void const* const* inputs, void* const* outputs, void* workspace, cudaStream_t stream) noexcept override;
-
-    // IPluginV2Ext Methods
-    nvinfer1::DataType getOutputDataType(
-        int index, nvinfer1::DataType const* inputTypes, int nbInputs) const noexcept override;
-
-    // IPluginV2 Methods
-    char const* getPluginType() const noexcept override;
-    char const* getPluginVersion() const noexcept override;
-    int getNbOutputs() const noexcept override;
-    int initialize() noexcept override;
-    void terminate() noexcept override;
-    size_t getSerializationSize() const noexcept override;
-    void serialize(void* buffer) const noexcept override;
-    void destroy() noexcept override;
-
-private:
-    void init(nvinfer1::DataType type);
-    void configGemm();
-
-private:
-    std::string const mLayerName;
-
-    LowLatencyGemmRunnerPtr m_lowLatencyGemmRunner;
-    size_t m_workspaceMaxSize;
-
-    GemmDims mDims{};
-    GemmIdCore mGemmId{};
-
-    PluginProfilerPtr mPluginProfiler;
-
-    nvinfer1::DataType mType;
-    float mAplha{1.0F};
-};
-
-class LowLatencyGemmPluginCreator : public BaseCreator
-{
-public:
-    LowLatencyGemmPluginCreator();
-
-    char const* getPluginName() const noexcept override;
-
-    char const* getPluginVersion() const noexcept override;
-
-    nvinfer1::PluginFieldCollection const* getFieldNames() noexcept override;
-
-    nvinfer1::IPluginV2* createPlugin(char const* name, nvinfer1::PluginFieldCollection const* fc) noexcept override;
-
-    nvinfer1::IPluginV2* deserializePlugin(
-        char const* name, void const* serialData, size_t serialLength) noexcept override;
-
-private:
-    GemmPluginProfilerManager<LowLatencyGemmPluginProfiler> gemmPluginProfileManager;
-    static nvinfer1::PluginFieldCollection mFC;
-    static std::vector<nvinfer1::PluginField> mPluginAttributes;
-};
-
-} // namespace tensorrt_llm::plugins
diff --git a/cpp/tensorrt_llm/plugins/weightOnlyGroupwiseQuantMatmulPlugin/weightOnlyGroupwiseQuantMatmulPlugin.cpp b/cpp/tensorrt_llm/plugins/weightOnlyGroupwiseQuantMatmulPlugin/weightOnlyGroupwiseQuantMatmulPlugin.cpp
index 40c8113a..abecb2b4 100644
--- a/cpp/tensorrt_llm/plugins/weightOnlyGroupwiseQuantMatmulPlugin/weightOnlyGroupwiseQuantMatmulPlugin.cpp
+++ b/cpp/tensorrt_llm/plugins/weightOnlyGroupwiseQuantMatmulPlugin/weightOnlyGroupwiseQuantMatmulPlugin.cpp
@@ -153,6 +153,7 @@ void WeightOnlyGroupwiseQuantMatmulPlugin::init(nvinfer1::DataType type, int qua
             {
                 TLLM_THROW("W4A(fp)8 kernel is unsupported on pre-Ada (sm<89) architectures!");
             }
+            #if defined(ENABLE_FP8)
             if (quant_algo & ZERO)
             {
                 // has zeros
@@ -167,6 +168,7 @@ void WeightOnlyGroupwiseQuantMatmulPlugin::init(nvinfer1::DataType type, int qua
                     = std::make_shared<tensorrt_llm::kernels::cutlass_kernels::CutlassFpAIntBGemmRunner<__nv_fp8_e4m3,
                         cutlass::uint4b_t, cutlass::WeightOnlyQuantOp::FINEGRAINED_SCALE_ONLY, half, half, half>>();
             }
+            #endif
         }
         else
         {
@@ -387,9 +389,11 @@ int WeightOnlyGroupwiseQuantMatmulPlugin::enqueue(nvinfer1::PluginTensorDesc con
         {
             if (mQuantAlgo & FP8_ALPHA)
             {
+                #if defined(ENABLE_FP8)
                 tensorrt_llm::kernels::apply_per_channel_scale_kernel_launcher<half, __nv_fp8_e4m3>(
                     reinterpret_cast<__nv_fp8_e4m3*>(workspace), reinterpret_cast<half const*>(inputs[0]),
                     reinterpret_cast<half const*>(inputs[mPreQuantScaleInputIdx]), m, k, stream);
+                #endif
             }
             else
             {
@@ -402,10 +406,12 @@ int WeightOnlyGroupwiseQuantMatmulPlugin::enqueue(nvinfer1::PluginTensorDesc con
         else if (mType == nvinfer1::DataType::kBF16)
         {
             if (mQuantAlgo & FP8_ALPHA)
-            {
+            {   
+                #if defined(ENABLE_FP8)
                 tensorrt_llm::kernels::apply_per_channel_scale_kernel_launcher<__nv_bfloat16, __nv_fp8_e4m3>(
                     reinterpret_cast<__nv_fp8_e4m3*>(workspace), reinterpret_cast<__nv_bfloat16 const*>(inputs[0]),
                     reinterpret_cast<__nv_bfloat16 const*>(inputs[mPreQuantScaleInputIdx]), m, k, stream);
+                #endif
             }
             else
             {
diff --git a/cpp/tensorrt_llm/pybind/batch_manager/llmRequest.cpp b/cpp/tensorrt_llm/pybind/batch_manager/llmRequest.cpp
index 19394008..f3544a59 100644
--- a/cpp/tensorrt_llm/pybind/batch_manager/llmRequest.cpp
+++ b/cpp/tensorrt_llm/pybind/batch_manager/llmRequest.cpp
@@ -82,7 +82,7 @@ std::shared_ptr<tb::LlmRequest> LlmRequest::toTrtLlm() const
         mDraftTokens, draftLogits, mExcludeInputFromOutput, callbackAdapter(mLogitsPostProcessor),
         mApplyLogitsPostProcessorBatched, mEncoderTokens, mReturnEncoderOutput, mClientId, mPriority,
         encoderInputFeatures, mEncoderOutputLength, tb::LlmRequestType::LLMREQUEST_TYPE_CONTEXT_AND_GENERATION,
-        mInputTokenExtraIds, mNumReturnSequences);
+        mInputTokenExtraIds);
 }
 
 void LlmRequest::initBindings(py::module_& m)
@@ -98,8 +98,7 @@ void LlmRequest::initBindings(py::module_& m)
                  std::optional<LlmRequest::VecTokens>, std::optional<LlmRequest::TensorPtr>, bool,
                  std::optional<LlmRequest::LogitsPostProcessor>, bool, std::optional<LlmRequest::VecTokens>, bool,
                  std::optional<RequestIdType>, executor::PriorityType, std::optional<LlmRequest::TensorPtr>,
-                 std::optional<LlmRequest::SizeType32>, std::optional<LlmRequest::VecTokenExtraIds>,
-                 LlmRequest::SizeType32>(),
+                 std::optional<LlmRequest::SizeType32>, std::optional<LlmRequest::VecTokenExtraIds>>(),
             py::arg("request_id"), py::arg("max_new_tokens"), py::arg("input_tokens"), py::arg("sampling_config"),
             py::arg("is_streaming"), py::arg("end_id") = std::nullopt, py::arg("pad_id") = std::nullopt,
             py::arg("embedding_bias") = std::nullopt, py::arg("bad_words_list") = std::nullopt,
@@ -114,7 +113,7 @@ void LlmRequest::initBindings(py::module_& m)
             py::arg("encoder_input_tokens") = std::nullopt, py::arg("return_encoder_output") = false,
             py::arg("client_id") = std::nullopt, py::arg("priority") = executor::Request::kDefaultPriority,
             py::arg("encoder_input_features") = std::nullopt, py::arg("encoder_output_length") = std::nullopt,
-            py::arg("input_token_extra_ids") = std::nullopt, py::arg("num_return_sequences") = 1)
+            py::arg("input_token_extra_ids") = std::nullopt)
         .def("get_num_tokens", &LlmRequest::getNumTokens, py::arg("beam"))
         .def_property_readonly("max_beam_num_tokens", &LlmRequest::getMaxBeamNumTokens)
         .def("get_token", &LlmRequest::getToken, py::arg("beam"), py::arg("pos"))
@@ -177,6 +176,5 @@ void LlmRequest::initBindings(py::module_& m)
         .def_property(
             "draft_logits", [](LlmRequest& self) { return self.getDraftLogits(); },
             [](LlmRequest& self, LlmRequest::TensorPtr& logits)
-            { self.setDraftLogits(std::make_optional<LlmRequest::TensorPtr>(logits)); })
-        .def_property("num_return_sequences", &LlmRequest::getNumReturnSequences, &LlmRequest::setNumReturnSequences);
+            { self.setDraftLogits(std::make_optional<LlmRequest::TensorPtr>(logits)); });
 }
diff --git a/cpp/tensorrt_llm/pybind/batch_manager/llmRequest.h b/cpp/tensorrt_llm/pybind/batch_manager/llmRequest.h
index 34ea424e..b27f835e 100644
--- a/cpp/tensorrt_llm/pybind/batch_manager/llmRequest.h
+++ b/cpp/tensorrt_llm/pybind/batch_manager/llmRequest.h
@@ -67,7 +67,7 @@ public:
         executor::PriorityType priority = executor::Request::kDefaultPriority,
         std::optional<TensorPtr> encoderInputFeatures = std::nullopt,
         std::optional<SizeType32> encoderOutputLength = std::nullopt,
-        std::optional<VecTokenExtraIds> inputTokenExtraIds = std::nullopt, SizeType32 numReturnSequences = 1)
+        std::optional<VecTokenExtraIds> inputTokenExtraIds = std::nullopt)
         : Base(requestId, maxNewTokens, std::make_shared<std::vector<TokenIdType>>(std::move(inputTokens)),
             samplingConfig, isStreaming, endId, padId, embeddingBias, badWordsList, stopWordsList,
             positionIds.has_value() ? std::make_shared<std::vector<SizeType32>>(std::move(positionIds.value()))
@@ -82,8 +82,7 @@ public:
             returnEncoderOutput, clientId, priority, encoderInputFeatures, encoderOutputLength,
             tb::LlmRequestType::LLMREQUEST_TYPE_CONTEXT_AND_GENERATION,
             inputTokenExtraIds ? std::make_optional(std::make_shared<VecTokenExtraIds>(std::move(*inputTokenExtraIds)))
-                               : std::optional<std::shared_ptr<VecTokenExtraIds>>(std::nullopt),
-            numReturnSequences)
+                               : std::optional<std::shared_ptr<VecTokenExtraIds>>(std::nullopt))
     {
     }
 
diff --git a/cpp/tensorrt_llm/pybind/bindings.cpp b/cpp/tensorrt_llm/pybind/bindings.cpp
index 7a6c25c2..b1b49320 100644
--- a/cpp/tensorrt_llm/pybind/bindings.cpp
+++ b/cpp/tensorrt_llm/pybind/bindings.cpp
@@ -335,7 +335,6 @@ PYBIND11_MODULE(TRTLLM_PYBIND_MODULE, m)
     tensorNames.attr("DRAFT_INPUT_IDS") = py::str(tb::inference_request::kDraftInputIdsTensorName);
     tensorNames.attr("DRAFT_LOGITS") = py::str(tb::inference_request::kDraftLogitsTensorName);
     tensorNames.attr("MAX_NEW_TOKENS") = py::str(tb::inference_request::kMaxNewTokensTensorName);
-    tensorNames.attr("NUM_RETURN_SEQUENCES") = py::str(tb::inference_request::kNumReturnSequencesTensorName);
     tensorNames.attr("BEAM_WIDTH") = py::str(tb::inference_request::kBeamWidthTensorName);
     tensorNames.attr("END_ID") = py::str(tb::inference_request::kEndIdTensorName);
     tensorNames.attr("PAD_ID") = py::str(tb::inference_request::kPadIdTensorName);
diff --git a/cpp/tensorrt_llm/pybind/executor/bindings.cpp b/cpp/tensorrt_llm/pybind/executor/bindings.cpp
index c578eb18..840ceaac 100644
--- a/cpp/tensorrt_llm/pybind/executor/bindings.cpp
+++ b/cpp/tensorrt_llm/pybind/executor/bindings.cpp
@@ -295,40 +295,40 @@ void InitBindings(pybind11::module_& m)
     request
         // A modified version of constructor to accpect deprecated args maxNewTokens
         // TODO(enweiz): use the original constructor after the deprecated args are removed
-        .def(py::init(
-                 [](tle::VecTokens inputTokenIds, std::optional<tle::SizeType32> maxTokens,
-                     std::optional<tle::SizeType32> maxNewTokens, bool streaming,
-                     tle::SamplingConfig const& samplingConfig, tle::OutputConfig const& outputConfig,
-                     std::optional<tle::SizeType32> const& endId, std::optional<tle::SizeType32> const& padId,
-                     std::optional<std::vector<SizeType32>> positionIds,
-                     std::optional<std::list<tle::VecTokens>> badWords,
-                     std::optional<std::list<tle::VecTokens>> stopWords, std::optional<tle::Tensor> embeddingBias,
-                     std::optional<tle::ExternalDraftTokensConfig> externalDraftTokensConfig,
-                     std::optional<tle::PromptTuningConfig> pTuningConfig, std::optional<tle::LoraConfig> loraConfig,
-                     std::optional<tle::LookaheadDecodingConfig> lookaheadConfig,
-                     std::optional<std::string> logitsPostProcessorName,
-                     std::optional<tle::VecTokens> encoderInputTokenIds, std::optional<tle::IdType> clientId,
-                     bool returnAllGeneratedTokens, tle::PriorityType priority, tle::RequestType type,
-                     std::optional<tle::ContextPhaseParams> contextPhaseParams,
-                     std::optional<tle::Tensor> encoderInputFeatures,
-                     std::optional<tle::SizeType32> encoderOutputLength, SizeType32 numReturnSequences)
-                 {
-                     if (maxNewTokens.has_value())
-                     {
-                         TLLM_LOG_WARNING("max_new_tokens is being deprecated; please use max_tokens instead.");
-                         if (!maxTokens.has_value())
-                         {
-                             maxTokens = maxNewTokens;
-                         }
-                     }
-                     TLLM_CHECK_WITH_INFO(maxTokens.has_value(), "missing required argument max_tokens");
-
-                     return std::make_unique<tle::Request>(inputTokenIds, maxTokens.value(), streaming, samplingConfig,
-                         outputConfig, endId, padId, positionIds, badWords, stopWords, embeddingBias,
-                         externalDraftTokensConfig, pTuningConfig, loraConfig, lookaheadConfig, logitsPostProcessorName,
-                         encoderInputTokenIds, clientId, returnAllGeneratedTokens, priority, type, contextPhaseParams,
-                         encoderInputFeatures, encoderOutputLength, numReturnSequences);
-                 }),
+        .def(
+            py::init(
+                [](tle::VecTokens inputTokenIds, std::optional<tle::SizeType32> maxTokens,
+                    std::optional<tle::SizeType32> maxNewTokens, bool streaming,
+                    tle::SamplingConfig const& samplingConfig, tle::OutputConfig const& outputConfig,
+                    std::optional<tle::SizeType32> const& endId, std::optional<tle::SizeType32> const& padId,
+                    std::optional<std::vector<SizeType32>> positionIds,
+                    std::optional<std::list<tle::VecTokens>> badWords,
+                    std::optional<std::list<tle::VecTokens>> stopWords, std::optional<tle::Tensor> embeddingBias,
+                    std::optional<tle::ExternalDraftTokensConfig> externalDraftTokensConfig,
+                    std::optional<tle::PromptTuningConfig> pTuningConfig, std::optional<tle::LoraConfig> loraConfig,
+                    std::optional<tle::LookaheadDecodingConfig> lookaheadConfig,
+                    std::optional<std::string> logitsPostProcessorName,
+                    std::optional<tle::VecTokens> encoderInputTokenIds, std::optional<tle::IdType> clientId,
+                    bool returnAllGeneratedTokens, tle::PriorityType priority, tle::RequestType type,
+                    std::optional<tle::ContextPhaseParams> contextPhaseParams,
+                    std::optional<tle::Tensor> encoderInputFeatures, std::optional<tle::SizeType32> encoderOutputLength)
+                {
+                    if (maxNewTokens.has_value())
+                    {
+                        TLLM_LOG_WARNING("max_new_tokens is being deprecated; please use max_tokens instead.");
+                        if (!maxTokens.has_value())
+                        {
+                            maxTokens = maxNewTokens;
+                        }
+                    }
+                    TLLM_CHECK_WITH_INFO(maxTokens.has_value(), "missing required argument max_tokens");
+
+                    return std::make_unique<tle::Request>(inputTokenIds, maxTokens.value(), streaming, samplingConfig,
+                        outputConfig, endId, padId, positionIds, badWords, stopWords, embeddingBias,
+                        externalDraftTokensConfig, pTuningConfig, loraConfig, lookaheadConfig, logitsPostProcessorName,
+                        encoderInputTokenIds, clientId, returnAllGeneratedTokens, priority, type, contextPhaseParams,
+                        encoderInputFeatures, encoderOutputLength);
+                }),
             py::arg("input_token_ids"), py::kw_only(), py::arg("max_tokens") = py::none(),
             py::arg("max_new_tokens") = py::none(), py::arg("streaming") = false,
             py::arg_v("sampling_config", tle::SamplingConfig(), "SamplingConfig()"),
@@ -343,7 +343,7 @@ void InitBindings(pybind11::module_& m)
             py::arg_v("type", tle::RequestType::REQUEST_TYPE_CONTEXT_AND_GENERATION,
                 "RequestType.REQUEST_TYPE_CONTEXT_AND_GENERATION"),
             py::arg("context_phase_params") = py::none(), py::arg("encoder_input_features") = py::none(),
-            py::arg("encoder_output_length") = py::none(), py::arg("num_return_sequences") = 1)
+            py::arg("encoder_output_length") = py::none())
         .def_property_readonly("input_token_ids", &tle::Request::getInputTokenIds)
         .def_property_readonly("max_tokens", &tle::Request::getMaxTokens)
         .def_property_readonly("max_new_tokens", &tle::Request::getMaxNewTokens)
@@ -371,9 +371,7 @@ void InitBindings(pybind11::module_& m)
             &tle::Request::setReturnAllGeneratedTokens)
         .def_property("request_type", &tle::Request::getRequestType, &tle::Request::setRequestType)
         .def_property(
-            "encoder_input_features", &tle::Request::getEncoderInputFeatures, &tle::Request::setEncoderInputFeatures)
-        .def_property(
-            "num_return_sequences", &tle::Request::getNumReturnSequences, &tle::Request::setNumReturnSequences);
+            "encoder_input_features", &tle::Request::getEncoderInputFeatures, &tle::Request::setEncoderInputFeatures);
     request.attr("BATCHED_POST_PROCESSOR_NAME") = tle::Request::kBatchedPostProcessorName;
 
     py::enum_<tle::FinishReason>(m, "FinishReason")
@@ -391,9 +389,7 @@ void InitBindings(pybind11::module_& m)
         .def_readwrite("context_logits", &tle::Result::contextLogits)
         .def_readwrite("generation_logits", &tle::Result::generationLogits)
         .def_readwrite("encoder_output", &tle::Result::encoderOutput)
-        .def_readwrite("finish_reasons", &tle::Result::finishReasons)
-        .def_readwrite("sequence_index", &tle::Result::sequenceIndex)
-        .def_readwrite("is_sequence_final", &tle::Result::isSequenceFinal);
+        .def_readwrite("finish_reasons", &tle::Result::finishReasons);
 
     py::class_<tle::Response>(m, "Response")
         .def(py::init<IdType, std::string>(), py::arg("request_id"), py::arg("error_msg"))
@@ -624,8 +620,8 @@ void InitBindings(pybind11::module_& m)
     auto extendedRuntimePerfKnobConfigGetstate = [](tle::ExtendedRuntimePerfKnobConfig const& self)
     { return py::make_tuple(self.getMultiBlockMode(), self.getEnableContextFMHAFP32Acc()); };
     py::class_<tle::ExtendedRuntimePerfKnobConfig>(m, "ExtendedRuntimePerfKnobConfig")
-        .def(
-            py::init<bool, bool>(), py::arg("multi_block_mode") = true, py::arg("enable_context_fmha_fp32_acc") = false)
+        .def(py::init<bool, bool>(), py::arg("multi_block_mode") = false,
+            py::arg("enable_context_fmha_fp32_acc") = false)
         .def_property("multi_block_mode", &tle::ExtendedRuntimePerfKnobConfig::getMultiBlockMode,
             &tle::ExtendedRuntimePerfKnobConfig::setMultiBlockMode)
         .def_property("enable_context_fmha_fp32_acc", &tle::ExtendedRuntimePerfKnobConfig::getEnableContextFMHAFP32Acc,
diff --git a/cpp/tensorrt_llm/pybind/executor/executor.cpp b/cpp/tensorrt_llm/pybind/executor/executor.cpp
index 229edfc3..2dfe2974 100644
--- a/cpp/tensorrt_llm/pybind/executor/executor.cpp
+++ b/cpp/tensorrt_llm/pybind/executor/executor.cpp
@@ -18,64 +18,15 @@
 #include "executor.h"
 #include "tensorrt_llm/common/assert.h"
 #include "tensorrt_llm/common/logger.h"
-#include "tensorrt_llm/executor/tensor.h"
 #include "tensorrt_llm/pybind/utils/pathCaster.h"
 
 #include <pybind11/chrono.h>
-#include <pybind11/numpy.h>
 #include <pybind11/pybind11.h>
 #include <pybind11/stl.h>
 
 namespace py = pybind11;
 namespace tle = tensorrt_llm::executor;
 
-namespace
-{
-tle::Tensor numpyToTensor(py::array const& array)
-{
-    auto npDtype = array.dtype();
-    tle::DataType dtype;
-    if (npDtype.is(py::dtype("float16")))
-    {
-        dtype = tle::DataType::kFP16;
-    }
-    else if (npDtype.is(py::dtype("float32")))
-    {
-        dtype = tle::DataType::kFP32;
-    }
-    else if (npDtype.is(py::dtype("int8")))
-    {
-        dtype = tle::DataType::kINT8;
-    }
-    else if (npDtype.is(py::dtype("int32")))
-    {
-        dtype = tle::DataType::kINT32;
-    }
-    else if (npDtype.is(py::dtype("int64")))
-    {
-        dtype = tle::DataType::kINT64;
-    }
-    else if (npDtype.attr("kind").cast<std::string>() == "V" && npDtype.attr("itemsize").cast<int>() == 1
-        && npDtype.attr("metadata")["dtype"].cast<std::string>() == "float8")
-    {
-        dtype = tle::DataType::kFP8;
-    }
-    else if (npDtype.attr("kind").cast<std::string>() == "V" && npDtype.attr("itemsize").cast<int>() == 2
-        && npDtype.attr("metadata")["dtype"].cast<std::string>() == "bfloat16")
-    {
-        dtype = tle::DataType::kBF16;
-    }
-    else
-    {
-        TLLM_THROW("Unsupported numpy dtype: " + npDtype.attr("name").cast<std::string>());
-    }
-
-    tle::Shape shape(array.shape(), array.ndim());
-
-    return tle::Tensor::of(dtype, const_cast<void*>(array.data()), shape);
-}
-} // namespace
-
 namespace tensorrt_llm::pybind::executor
 {
 
@@ -92,24 +43,12 @@ Executor::Executor(std::filesystem::path const& encoderModelPath, std::filesyste
 }
 
 Executor::Executor(pybind11::buffer engineBuffer, std::string const& jsonConfigStr, tle::ModelType modelType,
-    tle::ExecutorConfig const& executorConfig, std::optional<pybind11::dict> managedWeights)
+    tle::ExecutorConfig const& executorConfig)
 {
     py::buffer_info info = engineBuffer.request();
     uint8_t const* data = reinterpret_cast<uint8_t const*>(info.ptr);
     size_t size = info.size;
-    std::optional<std::map<std::string, tle::Tensor>> managedWeightsMap = std::nullopt;
-    if (managedWeights.has_value() && !managedWeights.value().empty())
-    {
-        managedWeightsMap = std::map<std::string, tle::Tensor>();
-        for (auto const& item : managedWeights.value())
-        {
-            std::string name = item.first.cast<std::string>();
-            py::array array = item.second.cast<py::array>();
-            managedWeightsMap->emplace(name, numpyToTensor(array));
-        }
-    }
-    mExecutor = std::make_unique<tle::Executor>(
-        tle::BufferView(data, size), jsonConfigStr, modelType, executorConfig, managedWeightsMap);
+    mExecutor = std::make_unique<tle::Executor>(tle::BufferView(data, size), jsonConfigStr, modelType, executorConfig);
 }
 
 Executor::Executor(std::string const& encoderEngineBuffer, std::string const& encoderJsonConfigStr,
@@ -157,9 +96,8 @@ void Executor::initBindings(py::module_& m)
                  tle::ExecutorConfig const&>(),
             py::arg("encoder_model_path"), py::arg("decoder_model_path"), py::arg("model_type"),
             py::arg("executor_config"))
-        .def(py::init<py::buffer, std::string const&, tle::ModelType, tle::ExecutorConfig const&, py::dict>(),
-            py::arg("engine_buffer"), py::arg("json_config_str"), py::arg("model_type"), py::arg("executor_config"),
-            py::arg("managed_weights") = py::dict())
+        .def(py::init<py::buffer, std::string const&, tle::ModelType, tle::ExecutorConfig const&>(),
+            py::arg("engine_buffer"), py::arg("json_config_str"), py::arg("model_type"), py::arg("executor_config"))
         .def(py::init<std::string const&, std::string const&, std::string const&, std::string const&, tle::ModelType,
                  tle::ExecutorConfig const&>(),
             py::arg("encoder_engine_buffer"), py::arg("encoder_json_config_str"), py::arg("decoder_engine_buffer"),
diff --git a/cpp/tensorrt_llm/pybind/executor/executor.h b/cpp/tensorrt_llm/pybind/executor/executor.h
index 921988f2..e19cfcc7 100644
--- a/cpp/tensorrt_llm/pybind/executor/executor.h
+++ b/cpp/tensorrt_llm/pybind/executor/executor.h
@@ -35,7 +35,7 @@ public:
         tle::ModelType modelType, tle::ExecutorConfig const& executorConfig);
 
     Executor(pybind11::buffer engineBuffer, std::string const& jsonConfigStr, tle::ModelType modelType,
-        tle::ExecutorConfig const& executorConfig, std::optional<pybind11::dict> managedWeights);
+        tle::ExecutorConfig const& executorConfig);
 
     Executor(std::string const& encoderEngineBuffer, std::string const& encoderJsonConfigStr,
         std::string const& decoderEngineBuffer, std::string const& decoderJsonConfigStr, tle::ModelType modelType,
diff --git a/cpp/tensorrt_llm/runtime/gptSession.cpp b/cpp/tensorrt_llm/runtime/gptSession.cpp
index c5d4dda5..56b85382 100644
--- a/cpp/tensorrt_llm/runtime/gptSession.cpp
+++ b/cpp/tensorrt_llm/runtime/gptSession.cpp
@@ -1312,7 +1312,7 @@ void GptSession::CudaGraphExecutor::launch(CudaStream const& stream)
 bool GptSession::CudaGraphExecutor::update(cudaGraph_t const& graph)
 {
     TLLM_LOG_TRACE("%s start", __PRETTY_FUNCTION__);
-    return cudaGraphExecUpdate(mInstance, graph, nullptr) != cudaSuccess;
+    // return cudaGraphExecUpdate(mInstance, graph, nullptr) != cudaSuccess;
 }
 
 void GptSession::CudaGraphExecutor::clear()
diff --git a/cpp/tensorrt_llm/runtime/iBuffer.cpp b/cpp/tensorrt_llm/runtime/iBuffer.cpp
index d3fb05ab..b9bd81bd 100644
--- a/cpp/tensorrt_llm/runtime/iBuffer.cpp
+++ b/cpp/tensorrt_llm/runtime/iBuffer.cpp
@@ -99,7 +99,7 @@ char const* IBuffer::getDataTypeName() const
     case nvinfer1::DataType::kBOOL: return DataTypeTraits<nvinfer1::DataType::kBOOL>::name;
     case nvinfer1::DataType::kUINT8: return DataTypeTraits<nvinfer1::DataType::kUINT8>::name;
     case nvinfer1::DataType::kINT8: return DataTypeTraits<nvinfer1::DataType::kINT8>::name;
-    case nvinfer1::DataType::kFP8: return DataTypeTraits<nvinfer1::DataType::kFP8>::name;
+    // case nvinfer1::DataType::kFP8: return DataTypeTraits<nvinfer1::DataType::kFP8>::name;
     case nvinfer1::DataType::kINT4: /* do nothing */;
     }
     TLLM_THROW("Unknown data type");
diff --git a/cpp/tensorrt_llm/runtime/torchUtils.h b/cpp/tensorrt_llm/runtime/torchUtils.h
index 28c30f23..bf4edab9 100644
--- a/cpp/tensorrt_llm/runtime/torchUtils.h
+++ b/cpp/tensorrt_llm/runtime/torchUtils.h
@@ -89,7 +89,9 @@ public:
         case IBuffer::DataType::kINT32: return torch::kInt32;
         case IBuffer::DataType::kINT64: return torch::kInt64;
         case IBuffer::DataType::kBOOL: return at::ScalarType::Bool;
+#ifdef ENABLE_FP8
         case IBuffer::DataType::kFP8: return at::ScalarType::Bits8;
+#endif
         case IBuffer::DataType::kBF16: return at::ScalarType::BFloat16;
         default: TLLM_THROW("unsupported data type");
         }
@@ -106,7 +108,9 @@ public:
         case torch::kInt32: return IBuffer::DataType::kINT32;
         case torch::kInt64: return IBuffer::DataType::kINT64;
         case at::ScalarType::Bool: return IBuffer::DataType::kBOOL;
+#ifdef ENABLE_FP8
         case at::ScalarType::Bits8: return IBuffer::DataType::kFP8;
+#endif
         case at::ScalarType::BFloat16: return IBuffer::DataType::kBF16;
         default: TLLM_THROW("unsupported data type");
         }
diff --git a/cpp/tensorrt_llm/runtime/utils/debugUtils.cu b/cpp/tensorrt_llm/runtime/utils/debugUtils.cu
index a665a2fa..e53aff81 100644
--- a/cpp/tensorrt_llm/runtime/utils/debugUtils.cu
+++ b/cpp/tensorrt_llm/runtime/utils/debugUtils.cu
@@ -59,9 +59,10 @@ template void invokeCheckTensorNanKernel(float const* data, std::size_t size, in
 template void invokeCheckTensorNanKernel(half const* data, std::size_t size, int* foundNan, cudaStream_t stream);
 template void invokeCheckTensorNanKernel(
     __nv_bfloat16 const* data, std::size_t size, int* foundNan, cudaStream_t stream);
+#ifdef ENABLE_FP8
 template void invokeCheckTensorNanKernel(
     __nv_fp8_e4m3 const* data, std::size_t size, int* foundNan, cudaStream_t stream);
-
+#endif
 template <typename T>
 void printLogitsKeyInfo(ITensor const& tensor, std::string const& infoStr)
 {
@@ -126,8 +127,9 @@ void printLogitsKeyInfo(ITensor const& tensor, std::string const& infoStr)
 template void printLogitsKeyInfo<float>(ITensor const& tensor, std::string const& infoStr);
 template void printLogitsKeyInfo<half>(ITensor const& tensor, std::string const& infoStr);
 template void printLogitsKeyInfo<__nv_bfloat16>(ITensor const& tensor, std::string const& infoStr);
+#ifdef ENABLE_FP8
 template void printLogitsKeyInfo<__nv_fp8_e4m3>(ITensor const& tensor, std::string const& infoStr);
-
+#endif
 template <typename T>
 bool tensorHasNan(ITensor const& tensor, BufferManager const& manager, std::string const& infoStr)
 {
@@ -145,9 +147,10 @@ template bool tensorHasNan<float>(ITensor const& tensor, BufferManager const& ma
 template bool tensorHasNan<half>(ITensor const& tensor, BufferManager const& manager, std::string const& infoStr);
 template bool tensorHasNan<__nv_bfloat16>(
     ITensor const& tensor, BufferManager const& manager, std::string const& infoStr);
+#ifdef ENABLE_FP8
 template bool tensorHasNan<__nv_fp8_e4m3>(
     ITensor const& tensor, BufferManager const& manager, std::string const& infoStr);
-
+#endif
 bool tensorHasNan(
     size_t M, size_t K, nvinfer1::DataType type, void const* data, cudaStream_t stream, std::string const& infoStr)
 {
@@ -166,10 +169,12 @@ bool tensorHasNan(
     {
         return tensorHasNan<__nv_bfloat16>(*tensorView, manager, infoStr);
     }
+#ifdef ENABLE_FP8
     else if (type == nvinfer1::DataType::kFP8)
     {
         return tensorHasNan<__nv_fp8_e4m3>(*tensorView, manager, infoStr);
     }
+#endif
     else
     {
         TLLM_THROW("Not supported type for Nan check");
diff --git a/cpp/tensorrt_llm/thop/fp8Op.cpp b/cpp/tensorrt_llm/thop/fp8Op.cpp
index e13bb78c..7af8e6a0 100644
--- a/cpp/tensorrt_llm/thop/fp8Op.cpp
+++ b/cpp/tensorrt_llm/thop/fp8Op.cpp
@@ -22,7 +22,7 @@
     && ((TORCH_VERSION_MAJOR > 1) || ((TORCH_VERSION_MAJOR == 1) && (TORCH_VERSION_MINOR >= 9)))
 #define TORCH_IS_AT_LEAST_v190
 #endif
-
+#ifdef ENABLE_FP8
 namespace torch_ext
 {
 using torch::Tensor;
@@ -225,3 +225,4 @@ static auto symmetric_dequantize_activation
 
 static auto symmetric_dequantize_per_tensor
     = torch::RegisterOperators("tensorrt_llm::dequantize_e4m3_per_tensor", &torch_ext::symmetric_dequantize_per_tensor);
+#endif // ENABLE_FP8
diff --git a/cpp/tensorrt_llm/thop/weightOnlyQuantOp.cpp b/cpp/tensorrt_llm/thop/weightOnlyQuantOp.cpp
index 02156e4e..a4ef2a51 100644
--- a/cpp/tensorrt_llm/thop/weightOnlyQuantOp.cpp
+++ b/cpp/tensorrt_llm/thop/weightOnlyQuantOp.cpp
@@ -40,12 +40,14 @@ void check_quant_type_allowed(torch::ScalarType quant_type)
 
 QuantType get_ft_quant_type(torch::ScalarType quant_type, torch::ScalarType activation_type = torch::kFloat16)
 {
+#ifdef ENABLE_FP8
     // Actually we need FP8 here, but current torch version does not support FP8. That's why INT8 is employed here
     if (activation_type == torch::kFloat8_e4m3fn)
     {
         return QuantType::W4_AFP8;
-    }
-    else if (quant_type == torch::kInt8)
+    } else
+#endif
+    if (quant_type == torch::kInt8)
     {
         return QuantType::W8_A16;
     }
diff --git a/examples/run.py b/examples/run.py
index 0baa00b3..029adc2b 100644
--- a/examples/run.py
+++ b/examples/run.py
@@ -24,7 +24,7 @@ import numpy as np
 import torch
 from utils import (DEFAULT_HF_MODEL_DIRS, DEFAULT_PROMPT_TEMPLATES,
                    add_common_args, load_tokenizer, prepare_enc_dec_inputs,
-                   read_model_name, supports_inflight_batching,
+                   read_model_name, #supports_inflight_batching,
                    throttle_generator)
 
 import tensorrt_llm
@@ -328,13 +328,13 @@ def main(args):
         x.size(0) for x in (encoder_input_features or encoder_input_ids)
     ] if is_enc_dec else None
 
-    if not args.use_py_session and not supports_inflight_batching(
-            os.path.join(args.engine_dir, "decoder") if is_enc_dec else args.
-            engine_dir):
-        logger.warning(
-            "The given engine does not support in-flight batching, fallback to python session"
-        )
-        args.use_py_session = True
+    #if not args.use_py_session and not supports_inflight_batching(
+    #        os.path.join(args.engine_dir, "decoder") if is_enc_dec else args.
+    #        engine_dir):
+    #    logger.warning(
+    #        "The given engine does not support in-flight batching, fallback to python session"
+    #    )
+    #   args.use_py_session = True
 
     if not PYTHON_BINDINGS and not args.use_py_session:
         logger.warning(
@@ -394,7 +394,10 @@ def main(args):
             kv_cache_free_gpu_memory_fraction=args.
             kv_cache_free_gpu_memory_fraction,
             enable_chunked_context=args.enable_chunked_context,
-            multi_block_mode=args.multi_block_mode)
+            multi_block_mode=args.multi_block_mode,
+            medusa_choices=args.medusa_choices,
+            lookahead_config=args.lookahead_config
+            )
     runner_kwargs.update(
         enable_context_fmha_fp32_acc=args.enable_context_fmha_fp32_acc)
     runner = runner_cls.from_dir(**runner_kwargs)
@@ -417,7 +420,7 @@ def main(args):
             top_k=args.top_k,
             top_p=args.top_p,
             num_beams=args.num_beams,
-            num_return_sequences=args.num_return_sequences,
+            #num_return_sequences=args.num_return_sequences,
             length_penalty=args.length_penalty,
             early_stopping=args.early_stopping,
             repetition_penalty=args.repetition_penalty,
@@ -436,7 +439,11 @@ def main(args):
             no_repeat_ngram_size=args.no_repeat_ngram_size,
             return_dict=True,
             medusa_choices=args.medusa_choices,
-            return_all_generated_tokens=args.return_all_generated_tokens)
+            return_all_generated_tokens=args.return_all_generated_tokens,
+            sampling_config=None,
+            stopping_criteria=None,
+            logits_processor=None
+            )
         torch.cuda.synchronize()
 
     if args.streaming:
diff --git a/requirements.txt b/requirements.txt
index 69866216..f31a42e7 100644
--- a/requirements.txt
+++ b/requirements.txt
@@ -8,19 +8,19 @@ lark
 mpi4py
 numpy<2
 onnx>=1.12.0
-openai==1.39.0
+#openai==1.39.0
 polygraphy
 psutil
-pynvml>=11.5.0
+#pynvml>=11.5.0
 pulp
 pandas
 h5py==3.10.0
 StrEnum
 sentencepiece>=0.1.99
-tensorrt~=10.4.0
+#tensorrt~=10.4.0
 # https://github.com/pytorch/pytorch/blob/v2.4.0/version.txt uses 2.4.0a0.
 # https://docs.nvidia.com/deeplearning/frameworks/pytorch-release-notes/rel-24-07.html#rel-24-07 uses 2.4.0a0.
-torch>=2.4.0a0,<=2.4.0
+#torch>=2.4.0a0,<=2.4.0
 nvidia-modelopt~=0.15.0
 transformers>=4.38.2,<=4.42.4
 pillow==10.3.0
diff --git a/scripts/build_wheel.py b/scripts/build_wheel.py
index 0f7e4c75..d85ca493 100755
--- a/scripts/build_wheel.py
+++ b/scripts/build_wheel.py
@@ -181,7 +181,7 @@ def main(*,
     build_micro_benchmarks = "ON" if micro_benchmarks else "OFF"
     micro_benchmarks_lib = "micro_benchmarks" if micro_benchmarks else ""
     disable_nvtx = "OFF" if nvtx else "ON"
-    executor_worker = "" if on_windows else "executorWorker "
+    executor_worker = "" #if on_windows else "executorWorker "
 
     source_dir = get_source_dir()
     with working_directory(build_dir):
@@ -243,9 +243,9 @@ def main(*,
         clear_folder(bin_dir)
     bin_dir.mkdir(parents=True, exist_ok=True)
 
-    if not on_windows:
-        copy(build_dir / "tensorrt_llm/executor_worker/executorWorker",
-             bin_dir / "executorWorker")
+    #if not on_windows:
+    #    copy(build_dir / "tensorrt_llm/executor_worker/executorWorker",
+    #         bin_dir / "executorWorker")
 
     if not cpp_only:
 
diff --git a/scripts/gen_cuda_headers_for_xqa.py b/scripts/gen_cuda_headers_for_xqa.py
index 43ce8505..f296fb10 100755
--- a/scripts/gen_cuda_headers_for_xqa.py
+++ b/scripts/gen_cuda_headers_for_xqa.py
@@ -25,7 +25,7 @@ parser.add_argument(
     '--input_files',
     help='Input CUDA header file name list, separated by ","',
     default=
-    'cuda_bf16.h,cuda_bf16.hpp,cuda_fp16.h,cuda_fp16.hpp,cuda_fp8.h,cuda_fp8.hpp,vector_types.h,vector_functions.h'
+    'cuda_bf16.h,cuda_bf16.hpp,cuda_fp16.h,cuda_fp16.hpp,vector_types.h,vector_functions.h'
 )
 parser.add_argument('--cuda_root',
                     help='CUDA Toolkit path',
diff --git a/tensorrt_llm/_utils.py b/tensorrt_llm/_utils.py
index 3e502924..6d46c408 100644
--- a/tensorrt_llm/_utils.py
+++ b/tensorrt_llm/_utils.py
@@ -46,8 +46,8 @@ def torch_to_numpy(x: torch.Tensor):
         f'x must be a torch.Tensor object, but got {type(x)}.'
     if x.dtype == torch.bfloat16:
         return x.view(torch.int16).detach().cpu().numpy().view(np_bfloat16)
-    elif x.dtype == torch.float8_e4m3fn:
-        return x.view(torch.int8).detach().cpu().numpy().view(np_float8)
+    #elif x.dtype == torch.float8_e4m3fn:
+    #    return x.view(torch.int8).detach().cpu().numpy().view(np_float8)
     else:
         return x.detach().cpu().numpy()
 
@@ -64,8 +64,7 @@ def numpy_to_torch(x):
 def numpy_to_dtype(x, dtype: str):
     if str_dtype_to_np(dtype) == x.dtype:
         return x
-    if x.dtype not in [np_bfloat16, np_float8
-                       ] and dtype not in ['bfloat16', 'fp8']:
+    if x.dtype not in [np_bfloat16] and dtype not in ['bfloat16']:
         return x.astype(str_dtype_to_np(dtype))
     else:
         return torch_to_numpy(numpy_to_torch(x).to(str_dtype_to_torch(dtype)))
@@ -101,12 +100,12 @@ def numpy_array(data, trt_dtype):
 
 
 def copy_torch_to_numpy(x: torch.Tensor, ndarray: np.array):
-    if x.dtype == torch.bfloat16:
-        torch.from_numpy(ndarray.view(np.int16)).copy_(x.view(torch.int16))
-    elif x.dtype == torch.float8_e4m3fn:
-        torch.from_numpy(ndarray.view(np.int8)).copy_(x.view(torch.int8))
-    else:
-        torch.from_numpy(ndarray).copy_(x)
+    #if x.dtype == torch.bfloat16:
+    #    torch.from_numpy(ndarray.view(np.int16)).copy_(x.view(torch.int16))
+    #elif x.dtype == torch.float8_e4m3fn:
+    #    torch.from_numpy(ndarray.view(np.int8)).copy_(x.view(torch.int8))
+    #else:
+    #    torch.from_numpy(ndarray).copy_(x)
     return ndarray
 
 
@@ -134,7 +133,7 @@ _str_to_np_dict = dict(
     int8=np.int8,
     bool=np.bool_,
     bfloat16=np_bfloat16,
-    fp8=np_float8,
+    #fp8=np_float8,
 )
 
 
@@ -152,7 +151,7 @@ _str_to_torch_dtype_dict = dict(
     int32=torch.int32,
     int8=torch.int8,
     bool=torch.bool,
-    fp8=torch.float8_e4m3fn,
+    #fp8=torch.float8_e4m3fn,
 )
 
 
@@ -175,8 +174,8 @@ _str_to_trt_dtype_dict = dict(float16=trt.float16,
                               int32=trt.int32,
                               int8=trt.int8,
                               bool=trt.bool,
-                              bfloat16=trt.bfloat16,
-                              fp8=trt.fp8)
+                              bfloat16=trt.bfloat16,)
+                              #fp8=trt.fp8)
 
 
 def str_dtype_to_trt(dtype):
@@ -209,7 +208,7 @@ _np_to_trt_dtype_dict = {
     np.dtype('float32'): trt.float32,
     np.dtype('bool'): trt.bool,
     np_bfloat16: trt.bfloat16,
-    np_float8: trt.fp8,
+    #np_float8: trt.fp8,
 }
 
 
@@ -227,7 +226,7 @@ _trt_to_np_dtype_dict = {
     trt.float32: np.float32,
     trt.bool: np.bool_,
     trt.bfloat16: np_bfloat16,
-    trt.fp8: np_float8,
+    #trt.fp8: np_float8,
 }
 
 
@@ -246,7 +245,7 @@ _torch_to_np_dtype_dict = {
     torch.int64: np.int64,
     torch.float16: np.float16,
     torch.bfloat16: np_bfloat16,
-    torch.float8_e4m3fn: np_float8,
+    #torch.float8_e4m3fn: np_float8,
     torch.float32: np.float32,
     torch.float64: np.float64,
     torch.complex64: np.complex64,
@@ -268,7 +267,7 @@ _trt_to_torch_dtype_dict = {
     trt.int8: torch.int8,
     trt.bool: torch.bool,
     trt.bfloat16: torch.bfloat16,
-    trt.fp8: torch.float8_e4m3fn,
+    #trt.fp8: torch.float8_e4m3fn,
 }
 
 
@@ -295,7 +294,7 @@ _torch_to_trt_dtype_dict = {
     torch.int64: trt.int64,
     torch.int32: trt.int32,
     torch.int8: trt.int8,
-    torch.float8_e4m3fn: trt.fp8,
+    #torch.float8_e4m3fn: trt.fp8,
     torch.qint8: trt.int8,
     torch.bool: trt.bool,
     torch.bfloat16: trt.bfloat16
diff --git a/tensorrt_llm/auto_parallel/cluster_info.py b/tensorrt_llm/auto_parallel/cluster_info.py
index 57d63706..f8b216f2 100644
--- a/tensorrt_llm/auto_parallel/cluster_info.py
+++ b/tensorrt_llm/auto_parallel/cluster_info.py
@@ -3,13 +3,15 @@ import re
 from dataclasses import dataclass, field
 from typing import Dict, Tuple, Union
 
-import pynvml
+
 import torch
 from cuda import cudart
+pynvml = None
 
 from tensorrt_llm._utils import DictConversion
 from tensorrt_llm.logger import logger
-from tensorrt_llm.profiler import PyNVMLContext, _device_get_memory_info_fn
+if pynvml is not None:
+    from tensorrt_llm.profiler import PyNVMLContext, _device_get_memory_info_fn
 
 
 @dataclass
@@ -451,79 +453,88 @@ def nvlink_bandwidth(nvlink_version: int) -> int:
 def infer_cluster_info() -> ClusterInfo:
     device = torch.cuda.current_device()
     index = device.index if isinstance(device, torch.device) else device
-    with PyNVMLContext():
-        handle = pynvml.nvmlDeviceGetHandleByIndex(index)
-        compute_cap = pynvml.nvmlDeviceGetCudaComputeCapability(handle)
-        logger.info(f"Compute capability: {compute_cap}")
-        err, properties = cudart.cudaGetDeviceProperties(index)
-        sm_count = properties.multiProcessorCount
-        logger.info(f"SM count: {sm_count}")
-        sm_clock = pynvml.nvmlDeviceGetMaxClockInfo(
-            handle,
-            pynvml.NVML_CLOCK_SM,
-        )
-        logger.info(f"SM clock: {sm_clock} MHz")
-        math_throughput = MathThroughput.to_tflops(
-            ipc_per_sm(compute_cap),
-            sm_count,
-            sm_clock,
-        )
-        for name in math_throughput.__dataclass_fields__:
-            tflops = getattr(math_throughput, name)
-            logger.info(f"{name} TFLOPS: {tflops}")
-
-        mem_info = _device_get_memory_info_fn(handle)
-        memory_budget = mem_info.total // (1024**3)
-        logger.info(f"Total Memory: {memory_budget} GiB")
-
-        mem_clock = pynvml.nvmlDeviceGetMaxClockInfo(
-            handle,
-            pynvml.NVML_CLOCK_MEM,
-        )
-        logger.info(f"Memory clock: {mem_clock} MHz")
-        if pynvml.__version__ < '11.5.0':
-            mem_bus_width = properties.memoryBusWidth
-        else:
-            mem_bus_width = pynvml.nvmlDeviceGetMemoryBusWidth(handle)
-        logger.info(f"Memory bus width: {mem_bus_width}")
-        memory_bw = mem_bus_width * mem_clock * 2 // int(8e3)
-        logger.info(f"Memory bandwidth: {memory_bw} GB/s")
-
-        try:
-            is_nvl_active = bool(pynvml.nvmlDeviceGetNvLinkState(handle, 0))
-            logger.info(f"NVLink is active: {is_nvl_active}")
-        except pynvml.NVMLError:
-            is_nvl_active = False
-
-        intra_node_sharp = False
-        if is_nvl_active:
-            nvl_version_enum = pynvml.nvmlDeviceGetNvLinkVersion(handle, 0)
-            nvl_version = nvlink_version(nvl_version_enum)
-            logger.info(f"NVLink version: {nvl_version}")
-            nvl_bw = nvlink_bandwidth(nvl_version)
-            logger.info(f"NVLink bandwidth: {nvl_bw} GB/s")
-            intra_node_bw = nvl_bw
-            if nvl_version >= 4:
-                intra_node_sharp = True
-        else:
+    if pynvml is not None:
+        with PyNVMLContext():
+            handle = pynvml.nvmlDeviceGetHandleByIndex(index)
+            compute_cap = pynvml.nvmlDeviceGetCudaComputeCapability(handle)
+            logger.info(f"Compute capability: {compute_cap}")
+            err, properties = cudart.cudaGetDeviceProperties(index)
+            sm_count = properties.multiProcessorCount
+            logger.info(f"SM count: {sm_count}")
+            sm_clock = pynvml.nvmlDeviceGetMaxClockInfo(
+                handle,
+                pynvml.NVML_CLOCK_SM,
+            )
+            logger.info(f"SM clock: {sm_clock} MHz")
+            math_throughput = MathThroughput.to_tflops(
+                ipc_per_sm(compute_cap),
+                sm_count,
+                sm_clock,
+            )
+            for name in math_throughput.__dataclass_fields__:
+                tflops = getattr(math_throughput, name)
+                logger.info(f"{name} TFLOPS: {tflops}")
+
+            mem_info = _device_get_memory_info_fn(handle)
+            memory_budget = mem_info.total // (1024**3)
+            logger.info(f"Total Memory: {memory_budget} GiB")
+
+            mem_clock = pynvml.nvmlDeviceGetMaxClockInfo(
+                handle,
+                pynvml.NVML_CLOCK_MEM,
+            )
+            logger.info(f"Memory clock: {mem_clock} MHz")
             if pynvml.__version__ < '11.5.0':
-                pcie_gen = pynvml.nvmlDeviceGetCurrPcieLinkGeneration(handle)
-                pcie_speed = (2**pcie_gen) * 1000
+                mem_bus_width = properties.memoryBusWidth
             else:
-                pcie_speed = pynvml.nvmlDeviceGetPcieSpeed(handle)
-            logger.info(f"PCIe speed: {pcie_speed} Mbps")
-            pcie_link_width = pynvml.nvmlDeviceGetCurrPcieLinkWidth(handle)
-            logger.info(f"PCIe link width: {pcie_link_width}")
-            pcie_bw = pcie_speed * pcie_link_width // int(8e3)
-            logger.info(f"PCIe bandwidth: {pcie_bw} GB/s")
-            intra_node_bw = pcie_bw
-
+                mem_bus_width = pynvml.nvmlDeviceGetMemoryBusWidth(handle)
+            logger.info(f"Memory bus width: {mem_bus_width}")
+            memory_bw = mem_bus_width * mem_clock * 2 // int(8e3)
+            logger.info(f"Memory bandwidth: {memory_bw} GB/s")
+
+            try:
+                is_nvl_active = bool(pynvml.nvmlDeviceGetNvLinkState(handle, 0))
+                logger.info(f"NVLink is active: {is_nvl_active}")
+            except pynvml.NVMLError:
+                is_nvl_active = False
+
+            intra_node_sharp = False
+            if is_nvl_active:
+                nvl_version_enum = pynvml.nvmlDeviceGetNvLinkVersion(handle, 0)
+                nvl_version = nvlink_version(nvl_version_enum)
+                logger.info(f"NVLink version: {nvl_version}")
+                nvl_bw = nvlink_bandwidth(nvl_version)
+                logger.info(f"NVLink bandwidth: {nvl_bw} GB/s")
+                intra_node_bw = nvl_bw
+                if nvl_version >= 4:
+                    intra_node_sharp = True
+            else:
+                if pynvml.__version__ < '11.5.0':
+                    pcie_gen = pynvml.nvmlDeviceGetCurrPcieLinkGeneration(handle)
+                    pcie_speed = (2**pcie_gen) * 1000
+                else:
+                    pcie_speed = pynvml.nvmlDeviceGetPcieSpeed(handle)
+                logger.info(f"PCIe speed: {pcie_speed} Mbps")
+                pcie_link_width = pynvml.nvmlDeviceGetCurrPcieLinkWidth(handle)
+                logger.info(f"PCIe link width: {pcie_link_width}")
+                pcie_bw = pcie_speed * pcie_link_width // int(8e3)
+                logger.info(f"PCIe bandwidth: {pcie_bw} GB/s")
+                intra_node_bw = pcie_bw
+
+            cluster_info = ClusterInfo(
+                math_throughput=math_throughput,
+                memory_bw=memory_bw,
+                memory_budget_per_device=memory_budget,
+                intra_node_bw_per_device=intra_node_bw,
+                intra_node_sharp=intra_node_sharp,
+            )
+    else:
         cluster_info = ClusterInfo(
-            math_throughput=math_throughput,
-            memory_bw=memory_bw,
-            memory_budget_per_device=memory_budget,
-            intra_node_bw_per_device=intra_node_bw,
-            intra_node_sharp=intra_node_sharp,
+            math_throughput=-1,
+            memory_bw=-1,
+            memory_budget_per_device=-1,
+            intra_node_bw_per_device=-1,
+            intra_node_sharp=-1,
         )
     return cluster_info
 
@@ -533,6 +544,10 @@ def infer_cluster_config() -> Dict[str, Union[str, ClusterInfo]]:
     cluster_key = infer_cluster_key()
     if cluster_key is not None:
         return dict(cluster_key=cluster_key)
+    elif pynvml is None:
+        cluster_info = infer_cluster_info()
+        return dict(cluster_key=device_name.replace(" ", "-"),
+                    cluster_info=cluster_info)
     else:
         try:
             cluster_info = infer_cluster_info()
diff --git a/tensorrt_llm/builder.py b/tensorrt_llm/builder.py
index e8dfb903..a4754b25 100644
--- a/tensorrt_llm/builder.py
+++ b/tensorrt_llm/builder.py
@@ -368,7 +368,7 @@ class Builder():
     def build_engine(self,
                      network: Network,
                      builder_config: BuilderConfig,
-                     managed_weights: dict = None) -> trt.IHostMemory:
+                     managed_weights = None) -> trt.IHostMemory:
         '''
             @brief: Build one TensorRT engine from the network.
             @param network: Network object.
@@ -689,7 +689,7 @@ class Engine:
         self,
         config: EngineConfig,
         engine: Union[trt.IHostMemory, None],
-        managed_weights: dict[str, np.ndarray] = None,
+        managed_weights = None #: dict[str, np.ndarray] = None,
     ):
         self.config = config
         self.engine = engine
@@ -897,8 +897,8 @@ def _init_max_seq_len(model_config, build_config):
             assert build_config.max_input_len <= build_config.max_seq_len, 'max_input_len should not be larger than max_seq_len'
 
 
-def serialize_managed_weights(managed_weights: dict[str, np.ndarray],
-                              path: str | Path,
+def serialize_managed_weights(managed_weights, #: dict[str, np.ndarray],
+                              path, #: str | Path,
                               metadata=None) -> None:
     header = {}
     if metadata is not None:
@@ -940,7 +940,7 @@ def serialize_managed_weights(managed_weights: dict[str, np.ndarray],
             f.write(buf)
 
 
-def deserialize_managed_weights(path: str | Path) -> dict[str, np.ndarray]:
+def deserialize_managed_weights(path): #-> dict[str, np.ndarray]:
     with open(path, "rb") as f:
         header_json_len = int.from_bytes(f.read(8), byteorder="little")
         header_json = f.read(header_json_len).decode()
@@ -974,7 +974,7 @@ def deserialize_managed_weights(path: str | Path) -> dict[str, np.ndarray]:
     return managed_weights
 
 
-def build(model: PretrainedModel, build_config: BuildConfig) -> Engine:
+def build(model: PretrainedModel, build_config: BuildConfig): # -> Engine:
     '''Build engine from given model and optimization options specified in the build_config
        WARNING: this function may change the given \p model object state in some optimization passes
        to avoid cloning a model since normally the LLM models consumes large memory.
diff --git a/tensorrt_llm/executor.py b/tensorrt_llm/executor.py
index af0d7138..0b638783 100644
--- a/tensorrt_llm/executor.py
+++ b/tensorrt_llm/executor.py
@@ -39,7 +39,7 @@ def has_event_loop() -> bool:
     return True
 
 
-@dataclass(slots=True)
+@dataclass #(slots=True)
 class LoRARequest:
     lora_name: str
     lora_int_id: int
@@ -89,7 +89,7 @@ class GenerationRequest:
         return self
 
 
-@dataclass(slots=True)
+@dataclass #(slots=True)
 class CompletionOutput:
     """The output data of one completion output of a request.
 
@@ -391,9 +391,9 @@ class GenerationExecutor(ABC):
     class ResponseTensors(NamedTuple):
         output_token_ids: list
         # context_logits is a tensor or a string denoting the path to the shared memory.
-        context_logits: Optional[torch.Tensor | str]
+        context_logits: Optional[Union[torch.Tensor, str]]
         # generation_logits is a tensor or a string denoting the path to the shared memory.
-        generation_logits: Optional[torch.Tensor | str]
+        generation_logits: Optional[Union[torch.Tensor, str]]
         log_probs: Optional[list]
         cum_log_probs: Optional[list]
 
@@ -403,9 +403,9 @@ class GenerationExecutor(ABC):
         tensors: Optional["GenerationExecutor.ResponseTensors"]
         is_final: Optional[bool]
         # error is either str from cpp-executor or a Exception from Python threads/processes
-        error: Optional[str | Exception]
+        error: Optional[Union[str, Exception]]
 
-    @dataclass(slots=True)
+    @dataclass #(slots=True)
     class PendingResponse:
         response: "GenerationExecutor.Response"
         start_time: float  # this is used to track the latency before the response is dispatched.
diff --git a/tensorrt_llm/hlapi/build_cache.py b/tensorrt_llm/hlapi/build_cache.py
index 30fcfffb..a2fc51b8 100644
--- a/tensorrt_llm/hlapi/build_cache.py
+++ b/tensorrt_llm/hlapi/build_cache.py
@@ -16,7 +16,7 @@ from tensorrt_llm import BuildConfig
 from tensorrt_llm.logger import logger
 
 
-def get_build_cache_config_from_env() -> tuple[bool, str]:
+def get_build_cache_config_from_env(): # -> tuple[bool, str]:
     """
     Get the build cache configuration from the environment variables
     """
diff --git a/tensorrt_llm/hlapi/llm_utils.py b/tensorrt_llm/hlapi/llm_utils.py
index a877667d..5b946dc7 100644
--- a/tensorrt_llm/hlapi/llm_utils.py
+++ b/tensorrt_llm/hlapi/llm_utils.py
@@ -800,7 +800,7 @@ class ModelLoader:
 
     def __init__(self,
                  llm_args: LlmArgs,
-                 workspace: Optional[str | tempfile.TemporaryDirectory] = None,
+                 workspace, #: Optional[str | tempfile.TemporaryDirectory] = None,
                  llm_build_stats: Optional["LlmBuildStats"] = None):
         self.llm_args = llm_args
         self._workspace = workspace or tempfile.TemporaryDirectory()
@@ -1376,7 +1376,7 @@ class CachedModelLoader:
     @staticmethod
     def _node_build_task(
         llm_args: LlmArgs,
-        workspace: Optional[str | tempfile.TemporaryDirectory] = None,
+        workspace, #: Optional[str | tempfile.TemporaryDirectory] = None,
         llm_build_stats: Optional['LlmBuildStats'] = None,
         engine_dir: Optional[Path] = None,
     ):
diff --git a/tensorrt_llm/hlapi/utils.py b/tensorrt_llm/hlapi/utils.py
index 9bf2cbe4..a16f2edc 100644
--- a/tensorrt_llm/hlapi/utils.py
+++ b/tensorrt_llm/hlapi/utils.py
@@ -35,7 +35,7 @@ def print_traceback_on_error(func):
     return wrapper
 
 
-@dataclass(slots=True, kw_only=True)
+@dataclass #(slots=True, kw_only=True)
 class SamplingParams:
     """
     Sampling parameters for text generation.
@@ -96,15 +96,15 @@ class SamplingParams:
     max_tokens: int = 32
     max_new_tokens: Optional[int] = None
 
-    bad: Optional[Union[str, List[str]]] = None
-    bad_token_ids: Optional[List[int]] = None
-    _bad_word_ids: Optional[List[List[int]]] = field(default=None,
+    bad: Optional[Union[str, List]] = None
+    bad_token_ids: Optional[List] = None
+    _bad_word_ids: Optional[List] = field(default=None,
                                                      init=False,
                                                      repr=False)
-    stop: Optional[Union[str, List[str]]] = None
-    stop_token_ids: Optional[List[int]] = None
+    stop: Optional[Union[str, List]] = None
+    stop_token_ids: Optional[List] = None
     include_stop_str_in_output: bool = False
-    _stop_word_ids: Optional[List[List[int]]] = field(default=None,
+    _stop_word_ids: Optional[List] = field(default=None,
                                                       init=False,
                                                       repr=False)
 
diff --git a/tensorrt_llm/models/deci/layer_config.py b/tensorrt_llm/models/deci/layer_config.py
index 84ed3487..cc24f960 100644
--- a/tensorrt_llm/models/deci/layer_config.py
+++ b/tensorrt_llm/models/deci/layer_config.py
@@ -30,7 +30,7 @@ class FFNImplementation(str, enum.Enum):
     NO_OP = "no_op"
 
 
-@dataclass(frozen=True, kw_only=True)
+@dataclass #(frozen=True, kw_only=True)
 class AttentionConfig:
     impl: AttentionImplementation = AttentionImplementation.ATTENTION
     num_key_value_heads: Optional[int] = None
@@ -40,13 +40,13 @@ class AttentionConfig:
         return self.impl == AttentionImplementation.ATTENTION
 
 
-@dataclass(frozen=True, kw_only=True)
+@dataclass #(frozen=True, kw_only=True)
 class FFNConfig:
     impl: FFNImplementation = FFNImplementation.MLP
     intermediate_size: Optional[int] = None
 
 
-@dataclass(frozen=True, kw_only=True)
+@dataclass #(frozen=True, kw_only=True)
 class DeciLayerConfig:
     attention: AttentionConfig = field(default_factory=AttentionConfig)
     ffn: FFNConfig = field(default_factory=FFNConfig)
diff --git a/tensorrt_llm/models/gemma/convert.py b/tensorrt_llm/models/gemma/convert.py
index ce70eafc..ab8e8765 100644
--- a/tensorrt_llm/models/gemma/convert.py
+++ b/tensorrt_llm/models/gemma/convert.py
@@ -810,7 +810,7 @@ def load_gemma_weights(
     return weights
 
 
-@dataclass(frozen=True, kw_only=True)
+@dataclass #(frozen=True, kw_only=True)
 class QuantizeModifiers:
     """
     Bag of additional conversion parameters, sourced from argparse or from defaults.
diff --git a/tensorrt_llm/models/gemma/utils/params.py b/tensorrt_llm/models/gemma/utils/params.py
index f0445f91..ed40577b 100644
--- a/tensorrt_llm/models/gemma/utils/params.py
+++ b/tensorrt_llm/models/gemma/utils/params.py
@@ -22,10 +22,10 @@ open sourcing.
 import functools
 from typing import Any
 
-Params = dict[str, Any]
+Params = dict #[str, Any]
 
 
-@functools.cache
+#@functools.cache
 def load_params(path: str) -> Params:
     import orbax.checkpoint
     """Loads parameters from a checkpoint path."""
diff --git a/tensorrt_llm/models/llama/convert.py b/tensorrt_llm/models/llama/convert.py
index cd9bbc63..94800a14 100644
--- a/tensorrt_llm/models/llama/convert.py
+++ b/tensorrt_llm/models/llama/convert.py
@@ -1590,6 +1590,8 @@ def load_weights_from_hf_safetensors(model_dir: str, config: LLaMAConfig):
     if mapping.is_last_pp_rank():
         v = load(param_name_map["lm_head"], -1, 1) if pad_vocab else load(
             param_name_map["lm_head"], 0, 1)  # lm_head
+        if v == None:
+            v = load(param_name_map["vocab_embedding"], -1 if pad_vocab else 0)
         if pad_vocab:
             v = torch.nn.functional.pad(
                 v, (0, 0, 0, vocab_size_padded - vocab_size), 'constant', 0)
diff --git a/tensorrt_llm/models/model_weights_loader.py b/tensorrt_llm/models/model_weights_loader.py
index e30f437e..896b5ada 100644
--- a/tensorrt_llm/models/model_weights_loader.py
+++ b/tensorrt_llm/models/model_weights_loader.py
@@ -65,7 +65,7 @@ class ModelWeightsLoader:
 
     def translate_to_external_key(
             self, tllm_key: str,
-            tllm_to_externel_key_dict: dict) -> str | List[str]:
+            tllm_to_externel_key_dict: dict): # -> str | List[str]:
         """Translate TRT-LLM key into HF key or HF key list (e.g. QKV/MoE/GPTQ)
 
         tllm_key will get translated into HF format section by section.
diff --git a/tensorrt_llm/models/modeling_utils.py b/tensorrt_llm/models/modeling_utils.py
index 5a03adc9..6e13f78c 100644
--- a/tensorrt_llm/models/modeling_utils.py
+++ b/tensorrt_llm/models/modeling_utils.py
@@ -42,7 +42,7 @@ from .convert_utils import weight_only_quantize_dict
 from .generation_mixin import GenerationMixin
 
 
-@dataclasses.dataclass(kw_only=True, frozen=True)
+@dataclasses.dataclass #(kw_only=True, frozen=True)
 class Gemma2ConfigGroup:
     query_pre_attn_scalar: int
     final_logit_softcapping: Optional[float]
diff --git a/tensorrt_llm/parameter.py b/tensorrt_llm/parameter.py
index f405387a..2d062983 100644
--- a/tensorrt_llm/parameter.py
+++ b/tensorrt_llm/parameter.py
@@ -222,7 +222,7 @@ class Parameter:
             return network.trt_network.set_weights_name(
                 self._get_weights(network), name)
 
-    def _get_weights(self, network) -> trt.Weights | Tensor | None:
+    def _get_weights(self, network): # -> trt.Weights | Tensor | None:
         tensor = network.get_parameter_tensor(self)
         if self.is_managed(network):
             return tensor
diff --git a/tensorrt_llm/plugin/plugin.py b/tensorrt_llm/plugin/plugin.py
index 6b0a8681..b7c20760 100644
--- a/tensorrt_llm/plugin/plugin.py
+++ b/tensorrt_llm/plugin/plugin.py
@@ -130,7 +130,7 @@ class PluginConfigMeta(type):
         return super().__new__(cls, name, bases, attrs)
 
 
-@dataclass(slots=True)
+@dataclass
 class PluginConfig(metaclass=PluginConfigMeta):
     """The config that manages plugin-related options.
 
diff --git a/tensorrt_llm/profiler.py b/tensorrt_llm/profiler.py
index 7a0874ea..b6998f6a 100644
--- a/tensorrt_llm/profiler.py
+++ b/tensorrt_llm/profiler.py
@@ -25,10 +25,11 @@ try:
     import psutil
 except ImportError:
     psutil = None
-try:
-    import pynvml
-except ImportError:
-    pynvml = None
+#try:
+#    import pynvml
+#except ImportError:
+#    pynvml = None
+pynvml = None
 import traceback
 
 from tensorrt_llm.logger import logger
diff --git a/tensorrt_llm/runtime/model_runner_cpp.py b/tensorrt_llm/runtime/model_runner_cpp.py
index d715c2da..896f8287 100644
--- a/tensorrt_llm/runtime/model_runner_cpp.py
+++ b/tensorrt_llm/runtime/model_runner_cpp.py
@@ -84,20 +84,20 @@ class ModelRunnerCpp(ModelRunnerMixin):
         max_input_len: Optional[int] = None,
         max_output_len: Optional[int] = None,
         max_beam_width: Optional[int] = None,
-        max_attention_window_size: Optional[list[int]] = None,
+        max_attention_window_size = None, #: Optional[list[int]] = None,
         sink_token_length: Optional[int] = None,
         kv_cache_free_gpu_memory_fraction: Optional[float] = None,
-        medusa_choices: list[list[int]] | None = None,
-        lookahead_config: list[int] | None = None,
+        medusa_choices = None, #: list[list[int]] | None = None,
+        lookahead_config = None, # : list[int] | None = None,
         debug_mode: bool = False,
         lora_ckpt_source: str = "hf",
         gpu_weights_percent: float = 1,
-        max_tokens_in_paged_kv_cache: int | None = None,
+        max_tokens_in_paged_kv_cache = None, #: int | None = None,
         kv_cache_enable_block_reuse: bool = False,
         enable_chunked_context: bool = False,
         is_enc_dec: bool = False,
-        multi_block_mode: bool = True,
-        enable_context_fmha_fp32_acc: Optional[bool] = None
+        multi_block_mode, #: bool = True,
+        enable_context_fmha_fp32_acc = None, #: Optional[bool] = None
     ) -> 'ModelRunnerCpp':
         """
         Create a ModelRunnerCpp instance from an engine directory.
@@ -422,33 +422,33 @@ class ModelRunnerCpp(ModelRunnerMixin):
 
     def generate(
             self,
-            batch_input_ids: List[torch.Tensor],
+            batch_input_ids, #: List[torch.Tensor],
             *,
-            position_ids: List[torch.Tensor] = None,
-            encoder_input_ids: List[torch.Tensor] = None,
-            encoder_input_features: List[
-                torch.Tensor] = None,  # TODO: add to doc string
-            encoder_output_lengths: List[int] = None,
-            sampling_config: Optional[SamplingConfig] = None,
-            lora_uids: Optional[list] = None,
-            lookahead_config: list[int] | None = None,
+            position_ids = None, #: List[torch.Tensor] = None,
+            encoder_input_ids = None, #: List[torch.Tensor] = None,
+            encoder_input_features = None, #: List[
+                # torch.Tensor] = None,  # TODO: add to doc string
+            encoder_output_lengths = None, #: List[int] = None,
+            sampling_config = None, #: Optional[SamplingConfig] = None,
+            lora_uids = None, #: Optional[list] = None,
+            lookahead_config = None, #: list[int] | None = None,
             streaming: bool = False,
-            stopping_criteria: Optional[StoppingCriteria] = None,
-            logits_processor: Optional[LogitsProcessor] = None,
+            stopping_criteria = None, #: Optional[StoppingCriteria] = None,
+            logits_processor = None, #: Optional[LogitsProcessor] = None,
             max_new_tokens: int = 1,
             num_return_sequences: int = 1,
-            end_id: int | None = None,
-            pad_id: int | None = None,
-            bad_words_list: list[list[int]] | None = None,
-            stop_words_list: list[list[int]] | None = None,
+            end_id = None, #: int | None = None,
+            pad_id = None, #: int | None = None,
+            bad_words_list = None, # : list[list[int]] | None = None,
+            stop_words_list = None, #: list[list[int]] | None = None,
             return_dict: bool = False,
             output_sequence_lengths: bool = False,
             output_log_probs: bool = False,
             output_cum_log_probs: bool = False,
-            prompt_table: Optional[Union[str, torch.Tensor]] = None,
-            prompt_tasks: Optional[str] = None,
+            prompt_table = None, #: Optional[Union[str, torch.Tensor]] = None,
+            prompt_tasks = None, #: Optional[str] = None,
             return_all_generated_tokens: bool = False,
-            **kwargs) -> Union[torch.Tensor, dict]:
+            **kwargs): # -> Union[torch.Tensor, dict]:
         """
         Generates sequences of token ids.
         The generation-controlling parameters are set in the sampling_config; it will be set to a default one if not passed.
@@ -580,7 +580,7 @@ class ModelRunnerCpp(ModelRunnerMixin):
                 position_ids=position_ids[i].tolist()
                 if position_ids is not None else None,
                 max_tokens=max_new_tokens,
-                num_return_sequences=num_return_sequences,
+                #num_return_sequences=num_return_sequences,
                 pad_id=pad_id,
                 end_id=end_id,
                 stop_words=stop_words,
@@ -730,8 +730,8 @@ class ModelRunnerCpp(ModelRunnerMixin):
 
         def req_idx(response: trtllm.Response):
             batch_idx = request_ids.index(response.request_id)
-            seq_idx = response.result.sequence_index
-            return batch_idx * num_return_sequences + seq_idx
+            #seq_idx = response.result.sequence_index
+            return batch_idx * num_return_sequences #+ seq_idx
 
         for response in responses:
             if response.has_error():
@@ -812,8 +812,8 @@ class ModelRunnerCpp(ModelRunnerMixin):
                             dtype=logits.dtype,
                             device=cuda_device)
                     batch_idx = request_ids.index(response.request_id)
-                    seq_idx = response.result.sequence_index
-                    reqid_pos = batch_idx * num_return_sequences + seq_idx
+                    #seq_idx = response.result.sequence_index
+                    reqid_pos = batch_idx * num_return_sequences #+ seq_idx
                     if streaming:
                         gen_logits[reqid_pos, :seq_len, ...] = logits[0]
                     else:
