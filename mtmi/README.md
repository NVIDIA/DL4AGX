# MTMI-Inference
## Introduction
This application is to demostrate the deployment of a multi-task network on orin platform. 

## Environment

To be added.
```bash
$ pip install -r requirements.txt
```

Prepare Tensorrt Environment:
```bash
$ TRTDIR=PATH_TO_TRT_8.6.12.3
$ export PATH=$TRTDIR/bin:$PATH
$ export LD_LIBRARY_PATH=$TRTDIR/lib:$LD_LIBRARY_PATH
```

## Onnx scripts

The original onnx model has been exported by running `python tools/pytorch2onnx_seg.py configs/mtformer/mtformer_export.py --checkpoint latest.pth` under [MTMI:mtmi_inference](https://gitlab-master.nvidia.com/boyinz/mtmi/-/tree/mtmi_inference). We use lfs to store the onnx file under [onnx_files/mtmi.onnx](https://gitlab-master.nvidia.com/boyinz/mtmi-inference/-/tree/main/onnx_files).

Step1: Simplify the onnx file:
```bash
$ python tools/onnx_slim.py
```

Step2: Split the onnx model into encoder, depth decoder and semantic segmentation decoder:
```bash
$ python tools/onnx_split.py
```

## Post-training quantization

To better utilize DLA, we should use Int8. In order to obtain scale information, we utilize tensorrt calibration apis.

### Prepare intermediate features for calibration
We first use onnx-runtime to produce intermediate features shared by the two decoders.
```bash
$ python tools/batch_preprocessing_onnx.py --onnx=PATH_TO_ONNX --image_path=PATH_TO_IMAGE_FILES --output_path=PATH_TO_SAVE_INTERMEDIATE_FEATURES
```

### Perform Post-training quantization
Then we can load from the cached feature map as input, and feed them into calibators.
With default arguments, you will get the calibration file under `calibration/` and tmp engine(will not be used) under `engines/`

```bash
$ python tools/create_calibration_cache.py --onnx=PATH_TO_HEAD_ONNX --image-path=PATH_TO_IMAGE_FILES --output-path=PATH_TO_SAVE_ENGINES --cache-path=PATH_TO_SAVE_CALIBRATION_FILES
```

Note that for now you need to manually modify the last two lines in the cache file for segmentation due to an unknown bug in tensorrt, ask Le for more information and tracking for the nvbugs:
```
input.408: 3e99a6d6
onnx::ArgMax_1963: 3e98d50a
```

## Build TensorRT engine and DLA loadables using the calibration caches
On orin platform:

Build engine with "outputIOFormats=fp16:chw32" for MIT-b0 encoder:
```bash
$ trtexec --onnx=onnx_files/mtmi_encoder.onnx --fp16 --saveEngine=engines/mtmi_encoder_fp16.engine --outputIOFormats=fp16:chw32 --verbose
```

Build DLA loadable with "inputIOFormats=int8:chw32" and "outputIOFormats=int8:dla_linear" for depth decoder and semantic segmentation decoder onnx model.
```bash
$ trtexec --onnx=onnx_files/mtmi_depth_head.onnx --int8 --saveEngine=engines/mtmi_depth_i8_dla.bin --useDLACore=0 --inputIOFormats=int8:chw32 --outputIOFormats=int8:dla_linear --buildOnly --verbose --buildDLAStandalone --calib=calibration/calibration_cache_depth.bin
$ trtexec --onnx=onnx_files/mtmi_seg_head.onnx --int8 --saveEngine=engines/mtmi_seg_i8_dla.bin --useDLACore=1 --inputIOFormats=int8:chw32 --outputIOFormats=int8:dla_linear --buildOnly --verbose --buildDLAStandalone --calib=calibration/calibration_cache_seg_mod.bin
```

## Build and run inference app
The inference app is organized as initialization then iterate and inference on images in the tests folder.
For the inference, we will 

### Build the app
In this section, we will cross compile the application on x86 for orin platform.
Docker: 
Launch docker with 

```bash
$ cd inference_app
$ sh build.sh orin
```

### Run the app
Please copy these files to orin.
```yaml
data/
  engines/
    mtmi_depth_i8_dla.loadable
    mtmi_encoder_fp16.engine
    mtmi_seg_i8_dla.loadable
  img/
    aachen_000038_000019_leftImg8bit.png        # input images, must crop to 1024x1024
    ...
  results/
    aachen_000038_000019_leftImg8bit_depth.bin  #  binary result file generated by demo app
    aachen_000038_000019_leftImg8bit_seg.bin    #  binary result file generated by demo app
    aachen_000038_000019_leftImg8bit_vis.png    #  visualization generated by visualize.py
    ...
```
Then run the demo.
```bash
$ ./mtmiapp
```

Sample logs:
```
...
loop: 1 bb_elapsed: 29.799711 chrono_elapsed: 29.965000
...
```

### Visualize results
Run the following python scripts to obtain visualization results from dumped binary result data.
```
python tools/visualize.py
```

Then you will get image results at `results/`
